ФИЛОСОФСКОЕ ОСМЫСЛЕНИЕ ОСНОВАНИЙ МАТЕМАТИКИ: ТЕОРИЯ МНОЖЕСТВ, ТЕОРЕМЫ ГЁДЕЛЯ И ФОРМАЛИЗАЦИИ АЛГОРИТМА. МОДЕЛЬ «РВАНОГО» КОНТИНУУМА

ВВЕДЕНИЕ

Наше исследование проведено в контексте философии математики. Однако в противовес многим классическим подходам, которые корректнее было бы совокупно определить не как собственно философию, а как эпистемологию математики, мы придерживаемся несколько иной позиции. Её, возможно, правильнее было бы охарактеризовать как математическую философию, то есть переосмысление фундаментальных проблем и понятий с опорой на возможности, которые предоставляют некоторые математические структуры, системы аксиом, методы фундаментальных доказательств, теоремы и прочее.
Иногда полагается, что у философов нет никаких однозначно свойственных им профессиональных навыков за исключением более или менее ярко выраженной способности «разделить волос на 4 части», то есть навыка анализировать «фрагменты бытия» с удивительной дотошностью. Мы бы к этому также добавили обратную способность, которую можно охарактеризовать как «функцию комбинирования нестандартных множеств», что подразумевает возможность видеть закономерности там, где для других наличествует лишь недифференцированная масса разрозненных данных, объективировать эти закономерности и далее рассматривать их феноменологически. Казалось бы, что у самих математиков эти самые способности уж точно не хуже, чем у философов, и с высокой вероятностью так оно и есть на самом деле. Однако ввиду разных «уровней притязаний» на точное описание своих объекта и предмета (математика обычно считается наиболее «точной» наукой из всех наук) математикам, в отличие от философов, иногда приходится в некотором смысле идти на компромиссы с такими факторами как интуиция, логика и порой даже «здравый смысл». Иначе перестаёт работать то, что ранее неплохо функционировало. Бывает, что подобные компромиссы происходят незаметно и, возможно, неосознанно, остаются скрытыми и продолжают долгое время «жить» в контексте теорий. И это никому особенно не мешает до тех пор, пока кто-нибудь, до известной степени случайно, не обратит на это внимание. Стоит заметить, что порой такие «компромиссы» служат краеугольными камнями для всего здания концепции.
Собственно говоря, именно такое «внимание» мы и собираемся обратить в данной работе на фундаментальные аспекты теоретико-множественного подхода в математике – подхода, который является несущей конструкцией для самих оснований математики. «Рай Кантора» – в действительности очень привлекательная и приятная концептуальная «локация» [ссылка на Гильберта, где он упоминает «рай Кантора»]. Carlos М. Madrid Casado Наука. Величайшие теории: выпуск 34: Вначале была аксиома. Гильберт. Основания математики. Пер. с исп. — М.: Де Агостини, 2015. — 176 с.
Также мы считаем, что как сам метод доказательства теоремы Курта Гёделя о неполноте, так и выводы из этой теоремы обладают огромным потенциалом для философского осмысления. Так что сказать об этом нам представляется весьма целесообразным. Хоть на эту тему уже было представлено достаточно большое количество различных мнений, взглядов и осмыслений, мы всё же считаем, что наше авторское видение способно внести новые смысловые оттенки. Как и про попытки формализации сущности алгоритма – фундаментального феномена, принадлежащего не только математике, физике, статистике, экономике и многим другим дисциплинам, но и, в первую очередь, самой философии. Проблема формализации алгоритма является одной из нерешённых в математике. На данный момент отсутствует однозначное мнение на этот счёт в контексте самой математики. Нам представляется, что и здесь можно, за счёт некоторого философского осмысления специфики наличествующей проблематики, очертить контур нового способа понимания алгоритма и заложить основу для осуществления дальнейшей строгой формализации.
Будет целесообразным заметить, что, как и философы, «философии» и философские подходы также являются порой весьма заметно различающимися. И не только в плане «интерпретационного базиса», но и в контексте «интенций осмысления». То есть, можно было бы обратить внимание на вековой давности внутренние пертурбации в математике и за счёт этого продемонстрировать специфику дисциплинарных противоречий, редуцировав как сами противоречия, так и выходы из них к некой социокультурной составляющей и к определению дальнейшей «конвенциональности». Подобный подход прост, «безопасен» с точки зрения корректности формулировок и, собственно, делает необязательным даже глубокое проникновение в саму математику – вполне достаточно просто истории математики как научной дисциплины. Можно было бы пойти проторенной дорожкой эпистемологии и, например, выявлять некую корреляцию между спецификой построения математических концепций и их дальнейшим развитием; или заниматься зависимостью особенностей математического мышления от общей когнитивной составляющей исследователя; или и вовсе поступить как «студент с блохами», сразу же редуцировав основания математики к лежащим в их основе философским понятиям и далее рассматривать уже непосредственно их, делая вид, что говорим о математике.
Однако мы здесь придерживаемся совершенно иного взгляда на проблематику. Рассмотрение величайших достижений математики и её самой непосредственно в том виде, в котором она пребывала в кризисе оснований и выходила из него, то есть осмысление особенностей «победы» над противоречиями, резюмированной в доказательствах и аксиомах, – вот что представляется нам поистине захватывающим и интересным. И это потому, что подобная специфика исследования позволяет на основе некоторых эксплицированных смыслов даже попытаться сделать осторожные выводы о природе, как самих математических объектов, так и наметить пути решения фундаментальных философских проблем.
В целом, одним из ключевых отличий математики от таких естественнонаучных дисциплин, как физика, химия, биология и прочие, является то что, математика, в сущности, не претендует на описание того мира, который мы привыкли считать нашей объективной реальностью. То есть, даже самый закоренелый «платонист» от математики не станет всерьёз утверждать, что где-нибудь в нашем мире существует хоть какой-нибудь объект из «мира», к примеру, евклидовой геометрии; что в мире существует натуральное число; или хотя бы что иррациональные числа являются изоморфными каким-либо объектам и предметам реальности. Даже сам Давид Гильберт, отец-основатель формализма, утверждая целесообразность принятия актуальной бесконечности в качестве валидного математического понятия, говорил, что «актуальной бесконечности в реальном мире существовать не может» [ссылка на доклад Гильберта Гильберт Д. Основания геометрии // Перевод А.В. Васильева. ‒ Петроград: Сеятель, 1923. ‒ 152 с.]. Яркий пример противоречия математической целесообразности и здравого смысла в объективном мире. А всё потому, что без валидного понятия актуальной бесконечности перестанет должным образом «функционировать», к примеру, вся иерархия алефов, станет ненужной трансфинитная индукция и понятие пустого множества – и в «рай Кантора» войдёт «демон Лапласа» [ссылка на демона Лапласа Пригожин И., Стенгерс И. Порядок из хаоса: Новый диалог человека с при¬родой: Пер. с англ./ Общ. ред. В. И. Аршинова, Ю. Л. Климонтовича  и  Ю. В. Сачкова. — М.: Прогресс, 1986.—432 с. ссылка на иерархию алефов Куратовский К., Мостовский А. Теория множеств / Перевод с английского М. И. Кратко под редакцией А. Д. Тайманова. — М.: Мир, 1970. — 416 с.]. Сами подобные факты представляют собой качественное доказательство истинности высказывания о том, что единственное, на что претендует математика, – это точность, и – одно из двух – непротиворечивость или полнота. Ведь, как доказал Гёдель, сразу и непротиворечивость, и полнота в любой достаточно богатой формальной системе, богаче логики исчисления предикатов, невозможны [ссылка на Гёделя  Гёдель 1931, Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme, I. Monatshefte für Mathematik und Physik 38: 173—198.  Пиньейро Г. Э. У интуиции есть своя логика. Гёдель. Теоремы о неполноте // Наука. Величайшие теории. — М.: Де Агостини, 2015. — Вып. 17.    Хотя на этот неприглядный факт стараются всё же не особо обращать внимание.
Математическая точность порой может сыграть злую шутку с самой математикой за счёт её обращения к правилам логического вывода. И именно по этой причине Л. Э. Я. Брауэр, основатель альтернативного по отношению к Гильбертовскому формализму течения – интуиционизма, отвергал валидность логического закона исключённого третьего [ссылка на Брауэра  Новая философская энциклопедия. В четырех томах. / Ин-т философии РАН. Научно-ред. совет: В.С. Степин, А.А. Гусейнов, Г.Ю. Семигин. М., Мысль, 2010, т. I, А - Д, с. 307-308.]. Ведь, как указывал сам Брауэр, экстраполяция валидности закона исключённого третьего, доказанная для конечных множеств, неправомерно экстраполируется на множества бесконечные, где этот логический закон уже не может являться однозначно истинным. Сам Брауэр в своих рассуждения о природе этой неправомерности не пошёл дальше – он был занят переписыванием математики с сохранением только тех её результатов, которые не использовали доказательство от противного. В данном контексте самого Брауэра можно метафорически представить как нечто среднее между Сизифом с его «Сизифовым трудом» и Гераклом с его «Авгиевыми конюшнями» [2 ссылки]   Е. Г. Рабинович. Мерное бремя Архивная копия от 4 мая 2012 на Wayback Machine // Ноосфера и художественное творчество. — М.: Наука, 1991. — С. 139—153.     Словарь иностранных слов. — М.: «Русский язык», 1989. — 624 с..
Однако, если, взяв за основу критику Брауэра, пойти дальше «за неё», то можно заметить некоторые скрытые, но очень интересные аспекты математики. А именно то, что наиболее корректно можно определить как «автономную» (не в прямом смысле, конечно) динамику математических «сущностей». Математические модели здесь выступают как стройматериал, а в качестве бригады строителей – правила логического вывода, то есть законы логики. И однажды, будучи сформированными, эти модели начинают жить своей, до известной степени самоорганизующейся, жизнью. Они изменяются, совершенствуются, пересекаются друг с другом и приобретают новые свойства и так далее – и всё это «логично». И только спустя некоторое время кто-то внезапно обнаруживает, что специфические характеристики той или иной абстрактной модели изменились настолько, что на свой изначальный вид и, в особенности, на те аспекты реальности, для которых она изначально служила аналогией, модель уже давно даже близко не похожа. И это всё происходит как бы скрытым от наблюдателя образом, а основой для этого служит логическая «динамика смыслов». Если ей «дать волю» и «разрешить» неограниченно использовать рекурсию, то апории Зенона о движении, парадоксы лжеца, брадобрея, сам парадокс Рассела и прочие «самореферентности» – будут расти как грибы после дождя.
В то же время и «рафинировать» логику, жёстко и директивно задав ей пределы возможностей, тоже оказалось не слишком удачной идеей. И логицизм во главе с Бертраном Расселом и его теория типов хорошо продемонстрировали, что убрав некоторые неявные связи между элементами формальной логической системы и полностью «запретив» парадоксы, мы не получим не только «живого» динамического аппарата, но даже сколь-нибудь жизнеспособного «кадавра» тоже не выйдет. И, соответственно, придётся ограничиваться холодной «мумией» некогда «живо» функционировавшей системы.
И этот момент, касаемо пределов юрисдикции внутренней динамики математических объектов, будет нами рассмотрен с большим интересом и многообещающими результатами.
Несмотря на вскользь упомянутые сложности, кризис оснований математики был преодолён, так или иначе. Его результатом стала некоторая сепарация дисциплины на течения и подходы, основные из которых были нами уже приведены. Дополним лишь, что интуиционизм затем плавно перетёк в конструктивизм [ссылка на конструктивизм  Марков А. А. Избранные труды. — М.: Изд-во МЦНМО, 2003. — Т. II. Теория алгоритмов и конструктивная математика, математическая логика, информатика и смежные вопросы. — 626 с.     Акимов О. Е. Дискретная математика: логика, группы, графы. — 2-е изд. — М.: «Лаборатория Базовых Знаний», 2003. — 376 с.]. На данный момент в математике центральное положение занимает теория категорий, которая со своей стороны претендует на объединение математики и на становление математической «теорией всего» [теория категорий   Родин А. В. Теория категорий и поиски новых математических оснований физики // Вопросы философии. — 2010. — № 7. — С. 67.   Р. Голдблатт. Топосы. Категорный анализ логики = Topoi. The categorial analysis of logic. — Москва: Мир, 1983. — 488 с.]. На посту претендента на эту вакантную должность теория категорий сменила теорию множеств, с которой также были связаны подобные надежды и ожидания математического сообщества. Насколько хорошо это получится – покажет время, как, собственно, оно это и показало с той же теорией множеств. «Обычного математика» это всё в любом случае касается не сразу и не прямо. «Обычный математик» предпочитает не обращать особого внимания на теоретические сложности, логические противоречия и философскую неоднозначность фундаментальных оснований своей дисциплины и удобно маневрировать между различными «интерпретационными позициями»: тихо полагать математические «сущности» истинно сущими где-то в «мире идей», как убеждённый платонист и ярый последователь учения Кантора; творить открытия конструктивным способом, как «преданный самурай» «феодала» Брауэра; а при указании на те самые теоретические противоречия, принимать сторону формалиста Гильберта, говоря что-то вроде «это просто формулы из символов». С одной стороны, на этих формулах стоит «рай Кантора», а с другой – они ничего не значат.
Довольно типичная для математики ситуация, в чём мы дальше не раз убедимся: использование одного и того же предиката для разных субъектов под видом одного предиката и использование одного и того же субъекта для разных предикатов под видом одного субъекта – довольно запутанная формулировка для гуманитария, но почти «банальная» для логика или математика. Хотя, конечно же, касаемо подобной подмены понятий, «так не кажется», по крайней мере, если не присмотреться очень внимательно. И вообще, большинство математиков занимаются некоторой прикладной, узкоспециальной деятельностью: математической физикой, теорией графов, эвристическими поисками средств оптимизации алгоритма «проблемы коммивояжера» и прочее. Это естественно и детерминировано обширностью самой дисциплины – со времён Пуанкаре и Гильберта универсалы в математике являются довольно уникальным явлением. Касаемо этого свойства, скорее не математики, а именно математиков, сказано ещё у Бурбаки: «Многие из математиков устраиваются в каком-нибудь закоулке математической науки, откуда они и не стремятся выйти, и не только почти полностью игнорируют все то, что не касается предмета их исследований. Более того, они не в силах даже понять язык и терминологию своих собратьев, специальность которых далека от них. Нет такого математика, даже среди обладающих самой обширной эрудицией, который бы не чувствовал себя чужеземцем в некоторых областях огромного математического мира; что же касается тех, кто, подобно Пуанкаре или Гильберту, оставляет печать своего гения почти во всех его областях, то они составляют даже среди наиболее великих редчайшее исключение» [c на Бурбаки [с. 99-100] «Математическое просвещение» (математика, её преподавание, приложения и история). М., Физматгиз, 1960. № 5. Редактор: И. Н. Бронштейн. – 304 с. – С. 99-113.]. Об одной из сторон данного вопроса – чрезмерной сосредоточенности не просто на специализации, но ещё и оторванной от реальности специализации, – указывал ещё сам Пуанкаре: «Нужно было бы окончательно забыть историю науки, чтобы не помнить, что стремление познать природу имело самое постоянное и самое счастливое влияние на развитие математики… Если бы чистый математик забыл о существовании внешнего мира, то он уподобился бы художнику, который умеет гармонически сочетать краски и формы, но у которого нет моделей. Его творческая сила скоро иссякла бы» [Пуанкаре «Ценность науки»  [с. 118] Пуанкаре, А. О науке : Пер. с фр. / Под ред. Л. С. Понтрягина. – 2-е изд. – М. : Наука, Гл. ред. физ.-мат. лит., 1990. – 736 с.]. Собственно то, о чём мы говорили выше.
Таким образом, до «прочности» и непогрешимости самих математических основ – фундамента всякой специализации – немногим вообще есть дело. Однако, философия науки – это своеобразная служба безопасности всей научной системы, в некотором смысле – «философский спецназ». И если уж Кантором, к примеру, доказано существование трансфинитных чисел и несчётность континуума, то мы хотим быть уверены, что его доказательства правомочны. Казалось бы, это же как раз сфера логики и эпистемологии. Отчасти верно. Однако, как показали и доказали Гёдель с Тарским, даже формальная логика, коя является достаточно богатой формальной системой, зачастую способна не некоторые парадоксы; а предметом эпистемологии всё же является знание – феноменологически, а также специфика его получения и формирования [НОВАЯ ссылка на Гёделя и ссылка на Тарского]. Паршин А. Н. Размышления над теоремой Гёделя // Историко-математические исследования. — М.: Янус-К, 2000. — № 40 (5). — С. 26—55.]. Gödel, Kurt [Feferman, S.]Collected works. Volume I: Publications 1929–1936. Ed. by Solomon Feferman et al. (English) New York: Oxford University Press; Oxford: Clarendon Press. XVI, 474 p. £25.00 (1986).    Tarski, Alfred. The concept of truth in formalized languages. (Der Wahrheitsbegriff in den formalisierten Sprachen.) (German) Stud. Philos. 1, 261-405 (1935).
То есть эпистемология заканчивается там, где бы хотелось нам начать, – неоднозначности в теориях, латентные противоречия в доказательствах, логические парадоксы и прочее. Ведь если и вовсе не обращать внимания на такие вещи, то можно очень быстро оказаться в сфере, имеющей довольно мало общего с собственно научной деятельностью. К тому же – это всегда продуктивно: если основам какой-либо теории, концепции и даже парадигмы удаётся устоять под «напором осмысления», то это ещё одно свидетельство в пользу её достоверности и нерушимости, а если нет, то это открывает «ящик Пандоры», из которого на сей раз, как из рога изобилия, сыплются новые подходы, теории и доказательства.
Также здесь мы бы хотели дополнительно обосновать своё, как философов, право не просто созерцать и осмыслять математику, но и делать определённые выводы об этой предметной области. Если сказать, что «любой математик» выведет n-й член натурального ряда при помощи одной только аксиоматики Пеано и без «всякой философии», или же что мы можем посчитать факториал числа, при этом никоим образом не обращаясь к глубинам философского понимания нашего священнодействия, то тот же «любой математик» согласится с этим утверждением [ссылка на аксиомы Пеано   Феферман С. Числовые системы. Основания алгебры и анализа. — 1971. — 445 с.      Н. Бурбаки. Основания математики. Логика. Теория множеств // Очерки по истории математики / И. Г. Башмакова (перевод с французского). — М.: Издательство иностранной литературы, 1963. — С. 37. — 292 с. — (Элементы математики)]. И это несмотря на то, что «вычислимость – это философское понятие», о чём, конечно же, «почти любой математик» совершенно не в курсе [Лолли    Габриэле Лолли. Философия математики: наследие двадцатого столетия / Пер. с итал. А.Л. Сочкова, С.М. Антакова, под ред. проф. Я.Д. Сергеева. – Н. Новгород: Изд-во Нижегородского госуниверситета им. Н.И. Лобачевского, 2012. – 299 с.]. Если выражаться тезисно и тафтологично, то «основания оснований» математики являются именно философскими и всё здание математики стоит на философском базисе. Одной из ключевых проблем, которые пыталась конструктивно решить теория множеств в период своего зарождения, – это проблема бесконечности. И какое бы развитие данное понятие не имело в наше время в математике и физике – оно остаётся фундаментально философским.
Данный факт и многие подобные становятся всё менее и менее заметными по мере удаления от фундамента и приближения к верхним этажам этой (да и вообще любой) точно-научной постройки – но он от этого не меняется. В этом смысле, конечно, многие математики и представители иных дисциплинарных областей, могут казаться «Иванами, родства не помнящими». Однако само использование философского понятийного аппарата для построения дисциплинарных основ в любом направлении даёт нам, как философам, право выступить в роли «приёмщика работ» и критически оценить то, что было сделано при помощи наших же «стройматериалов».
К тому же в философии, как отдельной дисциплине, наличествует весьма интересная особенность, для иллюстрации которой полезно будет привести ситуацию, которая произошла с одним из наиболее выдающихся математиков ХХ века – А. Н. Колмогоровым. В 20-е годы прошлого столетия, пребывая тогда в юношеском возрасте, Колмогоров пробовал применять свой талант в различных дисциплинах, в том числе в русле истории. Как-то раз на семинаре по этой дисциплине он сделал доклад с результатами проведённого им исследования. Руководитель высоко их оценил, но сказал, что в исторических исследованиях, для того чтобы иметь право сделать о чём-либо окончательный вывод, необходимо несколько доказательств, так что результаты Колмогорова хоть и ценны, но не могут претендовать на окончательность. После этого Колмогоров решил уйти в математику. Обоснование его ухода заключалось в следующем: как говорил сам Колмогоров, он решил заниматься наукой, в которой «для окончательного вывода достаточно одного доказательства» [Успенский В.А., Апология математики, СПб, «Амфора», 2010 г., с. 19-20.Источник: https://vikent.ru/enc/2653/].
Особенность же философии заключается в том, что в её рамках для того, чтобы полученный по итогу проведённого исследования результат мог считаться окончательным, необходимо, чтобы после его – результата – получения, в мире больше не осталось ни одного философа, включая того (не бывать парадоксу Рассела), кто этот самый результат получил. И это так потому, что никакое количество доказательств не способно утвердительно свидетельствовать философу о том, что результат окончателен – такова уж «философская традиция». В этом смысле Колмогорову тяжело пришлось бы в нашей дисциплине. Хотя, конечно, не всё так «эфемерно». О том, что доказательство истинности чего-либо лишь увеличивает вероятность истинности, а одно единственное однозначное опровержение доказывает ложность со 100% гарантией – писал ещё Карл Поппер [c Поппер, Карл Р. Предположения и опровержения. Рост научного знания. — М.: “Прогресс”, 1983. — С. С. 240-379.     Поппер, Карл. Объективное знание. Эволюционный подход / Пер. с англ. Д. Г. Лахути. Отв. ред. В. Н. Садовский.. — М.: Эдиторал УРСС, 2002. — 384 с.]. Собственно говоря, сам по себе выбор какого-либо конкретного доказательства в пользу некоторого утверждения уже ставит доказывающего в уязвимое (в контексте философии) положение. В этом смысле даже майевтик Сократ был не идеален, к примеру, хотя бы потому, что он хоть что-нибудь да утверждал при помощи «второй сигнальной системы», а именно – собственное незнание, которое, как известно, есть нечто большее по отношению к знанию, так как «уже содержит» оное [ссылка на Сократа и сигн сист    Интегративная деятельность мозга человека. Вторая сигнальная система // «Физиология человека» (под ред. В. М. Покровского, Г. Ф. Коротько), 2007. - 656 с.]    Апология Сократа // Творения Платона, перев. Вл. С. Соловьева, М. С. Соловьева и С. Н. Трубецкого. Т. 2. М., 1903.   Маковельский А. О. Глава II. Логика в Древней Греции до Аристотеля // История логики. — Жуковский; М.: Кучково поле, 2004. — С. 43—85. — 480 с.]. В этом смысле в относительной «безопасности» мог себя чувствовать, возможно, один только Диоген-киник [ссылка на Диогена   Диоген Лаэртский. О жизни, учениях и изречениях знаменитых философов / Редактор тома и автор вступительной статьи А. Ф. Лосев. — 2-е изд. — М.: Мысль, 1986. — (Философское наследие)  Антология кинизма. Антисфен, Диоген, Кратет, Керкид, Дион. Фрагменты сочинений кинических мыслителей / издание подготовил И. М. Нахов. Ответственный редактор А. А. Тахо-Годи. — М.: Наука, 1984. — (Памятники философской мысли)], да и то – не точно. А уж использование дискуссионных самих по себе философских понятий в контексте математических доказательств в качестве их неотъемлемых составных частей представляет собой поистине «красную тряпку». В сущности, своеобразная «прелесть» философских понятий именно в их дискуссионности и заключается, и философия, можно предположить, – единственная дисциплина, в которой дискуссионность не являет однозначный минус, а как раз-таки наоборот – представляет собой именно преимущество и характерную черту.
Так что мы – в своём праве. Более того – это наша святая обязанность как философов науки. Ну а также целесообразно дополнить: вся история науки свидетельствует о том, что изначально открытия совершаются именно философами, а уже потом – всеми остальными. И этот тезис тоже мы позже несколько раскроем.
 
ГЛАВА 1. ОСМЫСЛЕНИЕ МАТЕМАТИКИ КАК ПРОЦЕССА ФОРМИРОВАНИЯ АБСТРАКТНЫХ ОБЪЕКТОВ И СВЯЗЕЙ МЕЖДУ НИМИ

ПРЕДИСЛОВИЕ

В данной части нашей работы мы будем говорить о математике как отдельной дисциплине и одной из важнейших составляющих научного знания в целом. Рассматривать какую-либо отрасль науки в таком ключе означает стоять на почве эпистемологии и гносеологии, и по большей части это действительно так. По крайней мере, нам необходимо предварительно хотя бы в общем смысле сформировать некий набросок того, с чем мы будем иметь дело в дальнейших разделах данной работы. И именно в таком ключе мы планируем данную часть – определить общие основания для дальнейшей работы. То есть здесь нам представляется целесообразным не особенно глубоко вдаваться в мелкие детали, так как именно их мы и хотим прояснить в ходе дальнейшего изложения. А вот что же именно следует прояснять и зачем это делать – для этого как раз и предназначен данный раздел.
В случае с математикой давать некие общие определения особенно непросто: зачастую бывает так, что при оформлении некоторых высказываний на сложную тематику, к которой непосредственно относится математика, высказывание формулируется или слишком обобщённо, или чересчур конкретно. Однако основной вопрос в любом случае имеет экзистенциальную природу и звучит он так: существуют ли «математические сущности» и если существуют, то каким именно образом, то есть что означает для них «быть существующими»? Собственно, именно этот вопрос в своё время и привёл к расколу математики на несколько отдельных «математик». Истинно ли существует то, что может просто «быть подразумеваемым» аксиоматикой в контексте какой-либо теории, или же «истинно существуют» только лишь законы логики, а остальное – производные в общем смысле слова; а может статься, что существовать – значит иметь возможность быть определённым непосредственно конструктивным путём, то есть «быть построенным»? Существуют ли математические объекты в некоем до известной степени реальном, но весьма эфемерном измерении, в некоем мире Платоновских идей; или же они представляют собой не более чем объективации «когнитивной эмпирики»; а может быть, практически в прямом смысле, как утверждал Пифагор, «всё есть число» и сам мир «состоит из чисел»? Любой ответ подразумевает большую ответственность со стороны отвечающего. И это так по той причине, что единственного верного ответа здесь, конечно же, не существует. Так что любой «отвечающий», определяясь в некотором конкретном высказывании, просто «открывается» для критики со стороны других, альтернативных высказываний, мнений и соображений.
При исследовании математики деятельность довольно быстро сводится к рассмотрению абстрактных объектов, которые так или иначе понятийно определены. И можно заметить интересную закономерность: чем менее абстрактным является определение, тем более «односторонним» оно выглядит, а значит, более подверженным возможным «изъянам» с тех сторон, которые определение «не закрывает»; однако с иной позиции, чем более абстрактным и обобщённым представляется определение, тем менее оно подвержено «изъянам», но тем более логическим парадоксам. К примеру, один из самых известных «парадоксов всемогущества»: если Бог всемогущ, то может ли он создать такой камень, чтобы сам же его не смог поднять? И чем более мощная абстракция, тем больше парадоксов подобного рода можно сформировать на её основе. С другой стороны, на отдельно взятом понятии, скажем, синей вазы или деревянного стула, парадоксов особо много не сгенерируешь. Соблюдение должного баланса между этими двумя крайностями – задача непростая, но, как нам видится, необходимая.
С этих позиций мы и подходим к исследованию математики. Пожалуй, единственный момент, на который мы не видим особого смысла обращать внимание в контексте рассмотрения математики с эпистемологической точки зрения – это социальная обусловленность валидации тех или иных математических понятий и объектов. К примеру, обоснование комплексных чисел и комплексной плоскости было предложено ещё в конце 18 века, причём неоднократно. Однако только после того, как ровно то же самое сделал Карл Фридрих Гаусс, они получили право на признание и принятие в контексте математики. Была бы сейчас математика такой, какая она есть на данный момент, если бы не действия Гаусса, – вопрос открытый. Конечно, те, кто считает, что математические объекты представляют собой неотъемлемую часть реального мира, в данном аспекте указали бы на то, что в любом случае все открытия математики были бы рано или поздно осуществлены так или иначе; с другой стороны, те, кто, подобно Гильберту, полагает, что математика есть не более чем «игра символов», указал бы на конвенциональный характер истины в математике, разумеется, признав социальную обусловленность наличия тех или иных истин превалирующим фактором. Как видно, ситуация довольно противоречива, и одного несомненно верного ответа здесь не существует.
Немного охарактеризовав начальные условия ситуации, приступим непосредственно к ней самой.

1.1 О СУЩНОСТИ МАТЕМАТИКИ

Как говорил Рене Декарт: «Должна существовать всеобщая наука, которая объяснила бы все то, что возможно знать о порядке и мере, взятых независимо от их применения к какой-либо конкретной ситуации и, действительно, такая наука существует и имеет собственное, освященное длительным периодом ее эффективного использования имя – математика» [R.E. Moritz, Memorabilia Mathematica, Washington D.C., Mathematical Association of America, 1914 – 385 с.  Декарт Р. Д 28 Сочинения в 2 т.: Пер. с лат. и франц. Т. 1/Сост., ред., вступ. ст. В. В. Соколова.— М.: Мысль, 1989.— 654 с.]. Джон Аткинсон Гобсон же, к примеру, определял так: «Возможно, наименее неудовлетворительное описание (я не назвал бы это определением) основной цели современной чистой математики может быть сформулировано следующим образом. Математика занимается формой в наиболее широком и общем смысле этого термина» [там же R.E. Moritz, Memorabilia Mathematica, Washington D.C., Mathematical Association of America, 1914 – 385 с.  ]. С другой стороны, Бертран Рассел полагал, что «Чистая математика состоит всего-навсего из утверждений типа «Если, высказывание такое-то верно при каких-нибудь условиях, тогда высказывание сякое-то верно при тех же самых условиях»» [там же R.E. Moritz, Memorabilia Mathematica, Washington D.C., Mathematical Association of America, 1914 – 385 с С. 127      Последние работы по принципам математики, International Monthly, т. 4 (1901), стр. 84.]. Соратник Рассела А. Н. Уайтхед считал, что «Нельзя не признать, что занятие математикой – ниспосланное богами безумие человеческого духа» [также     С. 3 Е. М. Левич Исторический очерк развития методологии математики. Иерусалим 2008. – 222 с.]. Мнение Эвариста Галуа сходилось к тому, что математика – это «…творение человеческого разума, предназначенное не столько для знания, сколько для познания, для поиска, а не для отыскания истины» [также   Инфельд Л. Эварист Галуа. Избранник богов / Жизнь замечательных людей. М.: Молодая гвардия, 1965. С. 259—260.]. Эйнштейн так сказал о математике: «Если теоремы математики прилагаются к отражению реального мира, они не точны; они точны до тех пор, пока не ссылаются на действительность… Однако, с другой стороны, верно и то, что математика вообще и геометрия в частности обязаны своим происхождением необходимости узнать что-либо о поведении реально существующих объектов» [найти цитату    Эйнштейн Альберт Геометрия и опыт Издательство: Научное книгоиздательство Петроград, 1923. – 32 с.]. И это лишь ничтожно малая доля попыток, причём именно со стороны великих учёных, дать математике хоть сколько-нибудь одновременно и широкое, и точное, и корректное определение. Попробуем, соответственно, посмотреть сами.
Выше уже упоминалось о том, что математика считается наиболее точной наукой и в то же время не стремится описывать реальный мир непосредственно. Она скорее занимается формированием некоторого «нечто» для опосредованного описания реального мира. Если пошагово следовать идеалистической тропой за Платоном в «мир идей», считая, что все аксиомы математики были уже нам известны до рождения, а теперь просто необходимо их «вспомнить» или же принять физическую точку зрения Аристотеля, согласно которой реальный мир служит основой для «мира математики» и всё наше знание – эмпирическое, то мы в любом случае получим некую дуальную картину [ссылки на Платона и Аристотеля]   Васильева Т. В. Путь к Платону. Любовь к мудрости или мудрость любви. — М.: Изд-во «Логос», изд-во «Прогресс-Традиция», 1999. — 206 с..    Лосев А. Ф., Тахо-Годи А. А. Аристотель: Жизнь и смысл. — М.: Детская литература, 1982. — 286 с. — (Люди. Время. Идеи) Морис Клайн в своём труде «Математика. Утрата определённости» на эту тему говорит, что «…математика сама по себе превратилась в область знания, которую многие математики вслед за Платоном считали особой «внечувственной реальностью»» [Утр опр Клайн   Клайн М. Математика. Утрата определённости. — М.: Мир, 1984. — 446 с.].
По причинам, основными из которых полагаются здравый смысл и общая целесообразность, мы в этом случае более склоняемся, конечно же, к Аристотелю. Однако, в некотором роде, и Платону есть что добавить к общему контексту – и это также является понятным. Таким образом, более или менее корректным будет предположение о том, что объективная реальность представляет собой ту самую опорную точку, с которой для математики начинается «диалектика смыслов» и формирование самого «мира математики». Далее, после того как некоторый «мир» в контексте математики был сформирован, он начинает «жить своей жизнью» и, до некоторой степени, по своим собственным законам. Этот мир предельно, крайне абстрактен, ибо в нём нет места несовершенствам нашей обыденной жизни – это полностью идеальная «локация». Однако, есть один нюанс, который мы покажем на примере.
Допустим, «абстрагируем» некоторое реально существующее озеро со всеми его берегами, водорослями, живностью, волнами, рябью, купающимися в нём людьми до ведра воды из этого озера, соответственно. Затем добавим в это ведро 2 килограмма сахара. Можем ли мы на основе этого сделать вывод о том, что в озере вода также стала сладкой? Любой «не математик» скажет, что, разумеется, нет, а математик или логик ещё и добавят, что вывод по аналогии является слабым аргументом в пользу высказывания. Однако ведь существует же ненулевая вероятность того, что пока мы занимались с ведром воды и сахаром, вода в озере тоже по какой-либо причине стала сладкой. И иногда, пусть и почти никогда, но не ровно никогда, мы будет правы в нашем выводе о сладкой воде в озере. Теперь посмотрим в ведро и заметим, что в нём нет рыбы. Сделаем вывод о том, что в озере её также нет. Будем ли мы правы? Вероятность опять же ненулевая. Далее сделаем вывод о том, что вода из ведра обладает свойством некоторой текучести, и предположим, что в озере вода также обладает этим свойством – и, наконец, почти всегда (почти – потому, что пока мы осуществляли наши манипуляции, вода в озере могла измениться) будем правы.
Так можно предварительно заключить, что «мир математики» – это абстрактный аналог, идеальная модель «мира вещей», нашей объективной данности. Это – то же «ведро воды», но только именно уже долго и качественно «кипячённой» – выхолощенной до идеального состояния, до «сглаживания всех углов». Вообще, математические сущности имеют умозрительную природу: как говорится, «число не найдешь на дороге» и очень вряд ли есть хоть какой-нибудь пример существования в чистом виде хотя бы натуральных чисел [Лолли   Габриэле Лолли. Философия математики: наследие двадцатого столетия / Пер. с итал. А.Л. Сочкова, С.М. Антакова, под ред. проф. Я.Д. Сергеева. – Н. Новгород: Изд-во Нижегородского госуниверситета им. Н.И. Лобачевского, 2012. – 299 с.]. И это несмотря на то, что, как в своё время сказал один из самых непримиримых критиков отца-основателя теории множеств Георга Кантора Леопольд Кронекер: «Бог создал целые числа, всё остальное – дело рук человека» [ссылка на Кронекера   Бог создал целые числа : математические открытия, изменившие историю. / Перевод с англ. О. С. Сажиной [и др.]. — М. : АСТ, 2022. — 804, [6] с. : ил. — (Мир Стивена Хокинга)  И действительно, при попытке привести пример существования числа в нашем мире, именно самого числа в отрыве от какого-либо действительного материального объекта, мы со всей очевидностью испытаем некоторые, мягко говоря, затруднения. Даже при попытке просто «помыслить число» безо всякой опосредованной репрезентации мы должны будем установить, что невозможно это сделать. Однако в обыденной жизни это вовсе нам не мешает – мы привыкли буднично пользоваться плодами математических достижений, и если бы вдруг нас попросили хотя бы один день не применять никаких чисел ни для какого действия, то мы бы оказались в положении не менее затруднительном, чем когда пытались представить число безо всякой опосредованности – числа стали бы нам «мерещиться» буквально везде и во всём. Получается, что, с одной стороны, без чисел обойтись мы никак не можем, о чём, собственно, свидетельствует сама их фундаментальная целесообразность, ибо, будучи введёнными в контекст антропосоциогенеза в глубокой древности, они не просто «закрепились» в качестве неотъемлемой составляющей нашей цивилизации, но и пребывают в перманентном теоретическом развитии и концептуальном совершенствовании; с другой же стороны, при попытке узреть чистую сущность чисел – мы терпим явное фиаско, так как она будто ускользает за мельчайшую долю времени до того, как мы успеем её узреть во всём великолепии. Даже напрашивается неявная аналогия с «эффектом наблюдателя» в квантовой механике, хотя, конечно, это не совсем об одном и том же [ссылка  Гейзенберг В. Физика и философия. Часть и целое / Вернер Гейзенберг. — М. : Наука, 1989. — 400 с. ] . Видимо, прав был Сенека, когда говорил, что «…природа не сразу открывает свои тайны» [ссылка   Луций Анней Сенека. Философские трактаты. Алетейя, СПб, 2001. 2-е издание.
Перевод с латинского и комментарии Т. Ю. Бородай.]. По итогу, мы можем предположить, что «под» числами точно есть некоторая реальная основа, которая, впрочем, почему-то упорно не желает показываться.
Выше мы упомянули о том, что числа были «введены» в контекст антропосоциогенеза в глубокой древности. Мы употребили именно «введены», так как сказать, допустим, «открыты», «выявлены» или «обнаружены» не вполне корректно; с другой стороны, определить, что числа были «изобретены» или «придуманы» – некорректно ровно в той же мере. Таким образом: всё же так хотя бы более «правильно», если уж нельзя определить, как будет корректно однозначно – числа были «изобретены» или «открыты»? Если редуцировать числа к форме их репрезентации, то мы всегда (с соответствующими поправками) можем сказать, где и когда была разработана та или иная система счисления, также можем указать, где была впервые употреблена некая определённая форма записи той или иной численной системы. Однако то, что понимается под числами, – не редуцируется ни к форме записи, ни к системе счисления – это нечто гораздо большее и неизмеримо куда более абстрактное, чем просто некая запись. С другой стороны, если принять, что это нечто куда большее, чем записи и системы счисления, а также признать, что это нечто весьма абстрактное, то может вырисоваться хоть какая-то общая картина.
А именно, с одной стороны, является понятным, что формы репрезентации и системы счисления как раз-таки изобрели, придумали, разработали люди в ходе познания окружающего мира – точно так же, как и вообще сам язык общения – вторую сигнальную систему [c на вт сигн сист   Психиатрический энциклопедический словарь / Й.А. Стоименов, М.Й. Стоименова, П.Й. Коева и др. — Киев: МАУП, 2003. — 1200 с.   Психология. Иллюстрированный словарь / И.М. Кондаков. — СПб.; М.: Прайм-ЕВРОЗНАК, 2003. — 508 с. — (Психологическая энциклопедия)]. С другой стороны, также понятно, что если под числами понимается нечто абстрактное (даже без точного указания на то, чем же именно это «нечто абстрактное» является), то вряд ли можно утверждать, что это было прямо-таки «изобретено» людьми непосредственно. Оно могло быть открыто, выявлено и так далее, но не «придумано». В этом смысле вопрос очень быстро сводится к вопросу другому – «пахнет ли роза, когда её никто не нюхает»? И для того, чтобы сформировать более или менее корректный ответ на этот вопрос, нам представляется целесообразным немного разобраться в природе того, что же собой представляют числа именно в сущностном плане. Да и иные математические объекты тоже.
Мы говорим именно о «числах», как некой наиболее удобной математической аппроксимации реального мира, так как с иными математическими структурами всё обстоит несколько сложнее. Если при попытке представить «чего-нибудь пять» среднестатистический человек просто посмотрит на свою руку, то при директивном указании «представить какой-нибудь интеграл» на ум скорее всего и даже в лучшем случае придёт ближайший завод советской эпохи под этим названием. Однако, как мы показали на примере с числами, так и индуктивно (пока индуктивно) выведем для всех прочих, что любая математическая абстракция аппроксимирует некие «фрагменты реальности», порой весьма специфической и не всеми, и не всегда наблюдаемой, но всё же – объективной реальности. В этом смысле конкретно с множествами, как фундаментальным концептом теории множеств, дело по началу обстоит даже проще, чем со многими иными абстрактными объектами – достаточно попробовать представить себе какие-либо ёмкости (как наше ведро из примера), коробки, ячейки, файлы или папки, в общем, что-то подобное. Конечно, это крайне «шершавый» пример, но всё же так лучше, чем никак вовсе.
И теперь можно вновь вспомнить о мире идей Платона. Так.
Число 0 – не во всех подходах причисляемое к натуральным числам (но во всех – к целым), однако имеющее столь высокую значимость, что её переоценить было бы достаточно трудно. Это «эйдос» отсутствия связи и, ввиду его особого положения, мы скажем о нём отдельно в ином разделе, а пока же ноль – эйдос самого «ничто».
Число 1 – аппроксиматор всего того во Вселенной, что представлено единственным экземпляром в контексте хотя бы одной актуальной «позиции наблюдателя»: единый Господь Бог в христианстве и исламе; один единственный апейрон, как некое первовещество [c    Лебедев А. В. Апейрон // Новая философская энциклопедия / Ин-т философии РАН; Нац. обществ.-науч. фонд; Предс. научно-ред. совета В. С. Стёпин, заместители предс.: А. А. Гусейнов, Г. Ю. Семигин, уч. секр. А. П. Огурцов. — 2-е изд., испр. и допол. — М.: Мысль, 2010. ]; вообще само понятие первоосновы и начала начал; одна единственная точка бифуркации в синергетике [c    Бифуркация // Лебедев С. А. Философия науки: Словарь основных терминов. — М.: Академический проект, 2004. — 320 с. — (Серия «Gaudeamus»).]; анализ, как расщепление сущего на составляющие [c   Кузнецов Б. Г. История философии для физиков и математиков. — М.: Наука, 1974. — 352 с. — (История мировой культуры). ]; синтез, как объединение разрозненного воедино [c    Синтез // Малый энциклопедический словарь Брокгауза и Ефрона. — 2-е изд., вновь перераб. и значит. доп. — Т. 1—2. — СПб., 1907—1909.]. Один – это сама идея единичности, единственности, уникальности и первоначала.
Число 2 – вновь эйдос, только теперь уже биективности, дуализма, амбивалентности, бинарной оппозиции и вообще всего того, что может быть представлено «чем-нибудь двумя»: два аттрактора, как два возможных «выхода» из точки бифуркации; анализ и синтез как нерасторжимые составляющие познания и конструирования мира – как две стороны одной монеты [c   Левин Георгий Дмитриевич Анализ и синтез. М.: Канон+ РООИ «Реабилитация», 2022. 192 с.; часть и целое, общее и частное, истина и ложь, начальное и конечное, материальное и идеальное, причина и следствие – все бинарные категории философии и так далее [c  Трубецкой Н. С. Классификация оппозиций / Основы фонологии / пер. с нем. А. А. Холодовича. — М.: Аспект Пресс, 2000. — С. 72—88].
Число 3 – эйдос триединства, объединённости всего того, «чего три» – Святая Троица в христианстве; Ид, Эго, Супер-Эго в классическом психоанализе [c   Сверх-Я // Словарь группы лакановского психоанализа. — СПб, 2008.]; классический, неклассический и постнеклассический способы научного познания по Стёпину [c   В.С. Степин КЛАССИКА, НЕКЛАССИКА, ПОСТНЕКЛАССИКА: КРИТЕРИИ РАЗЛИЧЕНИЯ (опубликовано в кн.: Постнеклассика: философия, наука, культура. СПб.: Издательский дом «Мiръ», 2009. С.249 – 295)]; тезис, антитезис, синтез в диалектической философии Гегеля [c   Гегель, Георг Вильям Фридрих. Наука логики. В 3-х тт. Т.1 (неопр.). — Москва: Мысль, 1970. — (АН СССР. Ин-т философии. Философское наследие).; точка бифуркации и два аттрактора – как единое состояние системы в контексте синергетики и прочее прочее [c   Бифуркация // Лебедев С. А. Философия науки: Словарь основных терминов. — М.: Академический проект, 2004. — 320 с. — (Серия «Gaudeamus»).].
Число 4 – это «тетраксис» Пифагора, идея всего «чего четыре» [c   «Pythagorean Triangle», соч. о. Г. Оливера, – стр. 18, 19.  Ivo Kraus: Fyzika od Thaléta k Newtonovi. Praha, Academia, 2007 str. 20]; сильное и слабое ядерные взаимодействия, электромагнетизм и гравитация в контексте зарождения и функционирования Вселенной [c   Ньютон, И. Математические начала натуральной философии = Philosophiæ Naturalis Principia Mathematica : [пер. с лат.] / Исаак Ньютон ; ред. и предисл. Л. С. Полака ; пер. и комм. А. Н. Крылова. — М. : Наука, 1989. — 688 с. ]; метафизика аналитической психологии Юнга [c   Юнг К. Г. Тэвистокские лекции. Аналитическая психология: её теория и практика / пер. с англ. В. И. Менжулина. — М: АСТ, 2009. — 252 с.]; «четыре стадии Х» – где Х может быть представлен очень многими социально-культурными феноменами от рака до алкоголизма.
Число 5 – это пять Платоновых тел, пять точек Лагранжа, Пятикнижие Ветхого Завета и прочее [c   Lagrange, Joseph-Louis. Tome 6, Chapitre II: Essai sur le problème des trois corps // Oeuvres de Lagrange (фр.). — Gauthier-Villars. — P. 229—334].
…
Число 12 – дюжина, двенадцать колен Израилевых, двенадцать апостолов Христа, двенадцать месяцев года и так далее [c   Е. М. Штаерман Римская мифология «Мифы народов мира». Энциклопедия. (В 2 томах). «Советская энциклопедия», 1982. Т. II, с. 380—384.     ФЕНОМЕН ДВЕНАДЦАТИ ЦИКЛОВ Н.Н. Александров  https://www.trinitas.ru/rus/doc/0016/001d/2070-alx.pdf].
Число 13 – это «чёртова дюжина», тринадцать архимедовых тел и символ несчастью в некоторых культурных традициях и прочее [c    Лакенмайер Н. Чёртова дюжина: История одного суеверия = Nathaniel Lachenmeyer. 13: The Story of the World‘s Most Notorious Superstition. New York, 2004 / Натаниэль Лакенмайер; Пер. с англ. А. Турова; Составитель и редактор серии Александр Туров. — М.: КоЛибри, 2006. — 216 с. — (Вещи в себе)].
…
Число 21 – это эйдос азартной «удачи» и аллегория «везения», обозначение актуального века нашего времени и связь между всем тем во Вселенной «чего двадцать один» как целое [c    Севостьянова, Н.Г. С28 Логика : учеб.-метод. пособие для студентов высш. учеб. заведений / Н.Г. Севостьянова. – Минск : МГЛУ, 2012. – 284 с.].
…
Число 666 – «число Зверя» в христианской традиции, число, ассоциирующиеся с «символом Антихриста» [c   Куклев В. Зверя, число — 666 // Энциклопедия символов, знаков, эмблем / Сост. В. Андреева и др. — М.: Астрель, МИФ, АСТ, 2001. — С. 194     Романчук Л. «666» — число Зверя, человека или… Чужого? // «Порог». — 2002. — № 11. — С. 11—13.    Романчук Л. Символика числа 666 // Романчук Л., Щитов Дм. Демонизм. Зверь Апокалипсиса: литературные мифы, версии, реалии. — Москва: Мэйлер, 2012. — С. 211—216.]. 
…
Число 1729 – «число Харди-Рамануджана», первое натуральное число, представимое в виде суммы двух кубов двумя различными способами, и, само собой разумеется, абстрактная связь между всем, «чего тысяча семьсот двадцать девять» хотя бы где-то во Вселенной [c   Ламберто Гарсия дель Сид. Числа, любопытные с точки зрения арифметики → 1729 // Замечательные числа. Ноль, 666 и другие бестии. — DeAgostini, 2014. — Т. 21. — С. 16—17, 54. — 60 с. — (Мир математики).   Joe Roberts. Integer 1729 // Lure of the Integers (англ.). — MAA Spectrum, 1992. — P. 263—264. — 310 p. ].
…
И так далее с каждым натуральным числом – сама идея специфической репрезентации любого объекта или их совокупности. Если и есть нечто общее у всего во Вселенной, что может быть репрезентировано в единственном экземпляре, так это 1. То есть, как мы и говорили выше, максимально возможный уровень абстрагирования. 1, 2, 3, 4 и так далее – это именно идея наиболее слабой связи из всех возможных связей между объектами во Вселенной, которых хотя бы теоретически и хотя бы где-то может быть «один», «два», «три», «четыре» и далее по последовательности. Вообще, конечно, для «маленьких» чисел находится большее количество собственных имён по причине более частого использования.
И именно так – для всех математических «сущностей». Подводя некоторый промежуточный итог, мы определим математические объекты в виде абстракций.
Итак, математический объект – это концептуально структурированное представление наименее сильной и наиболее абстрактной связи между отдельными феноменами реальности. Здесь «объект» употребляется не как объект дисциплины, а как некая «сущность», «структура», «вещь» – ввиду высочайшего уровня абстрактности здесь по смыслу подходит довольно большое число общих дефиниций.
Соответственно, сама математика – наука об абстрактных связях между объектами и операциях с ними. Теперь, имея теоретическую опору на уже данные определения, мы можем реализовать ещё одно.
Ну, а «мир математики» – это когнитивное отражение совокупности объективированных абстрактных связей. Отсюда и далее мы будем определять этот самый «мир математики» термином – mundus, что в переводе с латинского означает «мир».
Как нам представляется, вышесказанное звучит более или менее корректно и, насколько это вообще возможно, просто и понятно. И так же просто и понятно можно теперь представить какое-либо число в виде тончайшей незримой бесконечной ниточки, пронизывающей всё то «чего 1», «чего 2» и так далее (да, всё равно не «само число», а «ниточка»); или же некоего контейнера, куда «сброшено» всё то, «чего 1» и контейнера с ячейками, в каждую из которых установлены пары всего того, «чего 2» и так далее. Но когда таких абстракций набирается огромное количество и когда они устанавливают разнообразные и уже внутренние связи друг с другом, причём именно такие связи, которые уже не имеют никакой аппроксимации в реальном мире, а лишь предполагают их возможное наличие, сами «существуя» только в виде чистых абстракций, то разобраться в этом становится очень непросто. Тем не менее, именно этот конгломерат и представляет собой тот самый mundus, о котором говорим мы, о котором говорят математики, философы математики и прочие причастные к данной области познания.
В этом смысле наши определения довольно близки к интуиционисткой позиции Гейтинг А. Интуиционизм. Введение. — М.: Мир, 1965. — 199 с.. Приведём мнение самого Лёйцзена Брауэра, касаемо специфики формирования интуитивного представления чисел, на примере числа 2: «…математика возникает тогда, когда сущность двойки, возникающая вследствие хода времени, абстрагируется от всего частного. Остающаяся пустая форма общего содержания всех двоек становится исходным интуитивным представлением математики и, повторяемая неограниченно, создает новые математические сущности» [c    "Über den Ursprung der mathematischen Anschauung" ("О происхождении математического представления"), опубликованной в 1907 году]. Говоря о «новых математических сущностях», Брауэр здесь имеет ввиду формирование одних математических абстракций на основе других, уже сформированных ранее – примерно так, при «переводе» дискурса Брауэра на «язык» дискурса нашей работы. Суть здесь, конечно, схожая, однако мы всё же не считаем, что формирование абстракции в контексте когнитивной данности является непосредственно актом интуиции. Нам представляется, что подобный подход представляет собой редукцию достаточно обширного спектра когнитивных функций к единому основанию – интуиции, таким образом, непомерно расширяя юрисдикцию данного понятия. Как полагается нами, интуиция являет собой скорее специфическое ощущение «тотального итога внутренних вычислений», но она не есть непосредственно все эти «вычисления» в их процессуальной совокупности. К тому же интуиция, по крайней мере, финальный этап её реализации, предполагается сознательным актом, а как видится нам – непосредственное формирование абстракций, по большей части, – явление бессознательное. То есть хоть сливки и могут с каких-то позиций представлять собой наиболее ценную часть молока, но сливки не есть само молоко в целом.
Здесь же дополним, что в нашей работе можно будет неоднократно наблюдать то, как имманентные связи самого mundus формируют новые оформленные объекты, приводят к возникновению новых имманентных связей, к выявлению ранее незамеченных пересечений, как с реальным миром, так и друг с другом. В этом смысле вновь представляется интересным следующее: мы всё-таки «выявляем» эти новации или же создаём их сами в ходе исследования? Любой ответ на этот вопрос будет подпадать под юрисдикцию того феномена, который будет нами позднее описан и более или менее подробно охарактеризован. А пока укажем на то, что высказывание «обе возможности истинны» и высказывание «обе возможности ошибочны» – равно значимы. И равно значимы они именно ввиду наличия того феномена, который позже мы предоставим для осмысления.
Однако предварительно всё же укажем на то, что, с учётом уже внесённых корректировок и определений, те самые данности, которые разумеются нами под числами и прочими объектами mundus, являют собой продукт именно человеческой когнитивной деятельности при осмыслении того «интенционального субстрата», которым представляется для нашего сознания наша реальность. Банальное заключение – «мы видим мир таким образом, каким способны его видеть». К примеру, мы можем себе помыслить нечто одно и следующим шагом становится абстракция числа 1, а затем и какая-либо форма его записи. Но мы не можем исключать того, что во Вселенной возможна такая форма когнитивной деятельности, которая не будет способна на то, чтобы мыслить категориями, подобными нашим. И вполне возможно, что потенциально осуществима форма жизни без категории количественного вообще: построение мыслительной деятельности только на разнообразии и категориях качественных отличий одного от другого – без фундаментального для нас понятия количественной меры и – как следствие – вообще безо всяких чисел хоть к какой-либо форме.
На эту тему высказывался Морис Клайн в своём труде «Математика. Утрата определённости» следующим образом «…за успехи математики заплачено определенной ценой, и эта цена – количественный подход к миру: мы рассматриваем его с точки зрения меры, веса, продолжительности и тому подобных понятий. Такое описание может давать о богатом и разнообразном опыте не более полное представление, чем рост человека о человеке. В лучшем случае математика описывает некоторые явления природы, но математические символы передают далеко не всё» [c    Клайн М. Математика. Утрата определённости. — М.: Мир, 1984. — 446 с.].
Будучи уже, как минимум, не в одиночестве, мы можем высказываться чуть смелее. К слову, специфика когнитивной деятельности некой гипотетической формы альтернативной жизни, то есть формы жизни с когнитивной деятельностью без категории количественного, достаточно легко аппроксимируется в виде множества с мощностью 2 и выше, для каждых 2-ух элементов которого не определена функция порядка, но в то же время определена функция сравнения. То есть мы не можем сказать, для этого множества, что какой-то один его элемент больше какого-то другого его элемента, но всегда можем сказать, что какой-то один его элемент не равен некоему другому элементу. Собственно, уникальность элементов множества априорна для любого множества, за логичным исключением мультимножеств – в них могут присутствовать одинаковые элементы с количеством более 1. Возможно даже подобная форма когнитивной деятельности сможет познать специфику происхождения Вселенной, её функционирования и эволюции – и всё это совсем без каких-либо количеств чего бы то ни было – просто на многообразии и качественных различиях этих многообразий. То есть вполне возможен способ описания функционирования такого объекта как Вселенная вообще безо всяких чисел. Будет ли это продуктивным? Для альтернативной формы когнитивной деятельности – возможно, для нас – очень вряд ли. По сути мы не можем однозначно утверждать, что «Вселенная математическая», а можем лишь сказать, что «она может быть описана математически». Однако, с тем же успехом она может быть описана и при помощи понятийного аппарата гипотетически предположенной нами «парадигмы качественных многообразий». С этих позиций можно попробовать осторожно скомбинировать некоторые формулировки.
Для формы жизни, подобной описанной выше – с когнитивной активностью, сильно отличающейся от нашей, числа, допустим, 2 действительно не будет существовать во Вселенной – в той Вселенной, которая видится и представляется такой форме жизни и когнитивной деятельности. Чисто гипотетически – это не лишено оснований. Но для нас – число 2 существует несомненно и при помощи работы с числами мы и полетели в космос – в эту самую «Вселенную», которая за границами нашей родной планеты. В этом смысле ответ на вопрос «пахнет ли роза, когда её никто не нюхает?» заключается в том, что «она потенциально способна пахнуть для всякого, имеющего обоняние», а без «наблюдателя» она просто генерирует физическую основу для запаха, но не непосредственно создаёт его – «запах розы» есть квалиа [c]. Так же и с числами, как неотъемлемыми «абстрактными фрагментами» нашей реальности.
На основе этого предварительно и заключим, что реальность предоставляет «субстрат для осмысления» (восприятия, осознания, когнитивной обработки, формирования абстракций и так далее), конечно же со своими пределами и условиями, которые могут быть заданы в виде условных границ «возможного» и «невозможного». Разумеется, чётко эти грани определены быть вряд ли могут, но предположим, что они существуют на некотором диапазоне. В рамках этих граней мы имеем тот самый «субстрат», который предоставляет в наше распоряжение «интерпретационный базис». Затем, на основе «возможного» и когнитивных способностей (здесь – совокупности способов получения и обработки информации) некой формы жизни, формируется отражение, синтез, «промежуточная реальность», которая и представляет то, что обычно понимается под объективной реальностью. На основе вышесказанного резюмируем: число 2, как наш «классический» пример в виде той абстрактной данности, которая является фундаментальным значением символической «записи» – реально существует для нас во Вселенной; для любой же альтернативной формы когнитивной деятельности в рамках нашей Вселенной – оно всего лишь потенциально способно существовать, так как находится в «диапазоне возможного» и вероятность его существования для любой формы жизни прямо пропорциональна специфике когнитивной деятельности, предполагающей работу с количественными категориями.
После же того, как математическая реальность уже наличествует в качестве некой, до известной степени, «особой реальности», объекты mundus способны динамически функционировать и на основе этой динамики можно делать некоторые выводы об окружающей действительности. Для того, чтобы как можно реже формировать некорректные выводы о мире реальном на основе математического, существуют правила логического вывода, которые не позволят валидировать «истинный путь к сладкой воде». Однако те же самые правила вывода порой делают непротиворечивыми самореферентные парадоксы и доказательства, такие как: парадоксы лжеца, брадобрея, льва, парадокс Рассела, доказательство Алана Тьюринга о неразрешимости проблемы остановки, доказательство Альфреда Тарского о невыразимости истины, доказательство Курта Гёделя теорем о неполноте и противоречивости, и прочие [ссылки на все парадоксы]. По поводу включения в приведённый перечень некоторых дискуссионных пунктов мы затем предоставим соответствующие обоснования.
В любом случае мы имеем дело с идеальной моделью, правила вывода которой хотя бы «стараются» не допускать явных противоречий, дабы заключения имели право экстраполироваться на реальный мир, пусть и с определёнными ограничениями и учётом специфики локального контекста. Можем ли мы, допустим, проверить «на чём-нибудь» реальном доказанную Григорием Перельманом гипотезу Пуанкаре [2 ссылки]? Очевидно, что ни на момент возникновения самой гипотезы, ни в наше время актуальный уровень научно-технического прогресса этого не позволял и не позволяет. Иначе бы – подобного рода гипотезы можно было бы проверить просто экспериментально и не было бы нужды в многофакторных логико-математических доказательствах и последующей столь же «умозрительной», как и они сами, их проверке. А так всё, что остаётся – это проверять насколько то или иное умозаключение, вывод, концепт будут соответствовать всему остальному абстрактному миру во всём его многообразии и особенно в близлежащих от «места попадания» частностях.
Законы логики довлеют над доказательным контекстом и, словно зоркие часовые, бессменно бдят в попытке недопущения в mundus того, чего там быть не должно. Однако на страже стоят, разумеется не только они. Математика – это всё-таки наука, а иногда даже её определяют, как саму «Царицу Наук». Мы так понимаем, что Карл Фридрих Гаусс и Михаил Ломоносов, когда определяли математику в данный контекст, добавляя также, что при всём этом математика ещё и «служанка физики», «несомненно» подразумевали, что это утверждение может полагаться истинным в том и только в том контексте, где «Богиня – Философия». В сущности, это даже не является обязательным, так как пренебрежение подобным никогда не длится достаточно долго для того, чтобы перерасти в форму «правящей парадигмы»: как некогда единая и нерушимая (это, наверное, должно что-то напоминать) математика была расколота на несколько направлений, основными из которых являются формализм, логицизм и конструктивизм, так и как физика была разобщена противоречиями между общей теорией относительности и квантовой механикой [ссылки]. Теоретико-множественный подход занимает особое положение, так как его в любом случае используют все названные выше направления – основание есть основание. В любом случае теперь дисциплина или даже дисциплины математики находятся в непривычной для себя обстановке. В ослабленном варианте той обстановки, в которой, собственно, и родилась философия – в контексте плюрализма мнений, противоречий и концептуальных пертурбаций – даже самые ранние философы, ионийской школы, не смогли сойтись во мнениях касаемо действительно первого философского вопроса – о первовеществе. Каждый доказывал, что именно его элемент – фундаментален, а остальные всего лишь вторичные и «производные».
И, собственно, с течением времени данная неотъемлемая характеристика философии лишь интенсифицировалась. На основе чего можно предположить, что математика на теперешний момент находится на той стадии своего развития, именно с точки зрения потенциала к совершенствованию, на которой философия была более двух с половиной тысяч лет назад. Касаемо же раскола в математике, который произошёл в первой половине прошлого века, можно, перефразируя Марка Твена, сказать так: как только математика раскололась на несколько «математик», мы обрели подлинную математику. Если же экстраполировать эту мысль на философию, то она, соответственно, «уже родилась» подлинной.
Математика в ходе своего развития шла более или менее последовательным путём, очень осторожно добавляя абстракции в mundus, делая это только при острой необходимости и целесообразности, а также тщательно их верифицируя.
Конечно, здесь можно указать также и на то, что в математике бывали открытия, которые, как казалось на момент самого открытия, никоим образом не пересекаются и даже потенциально не способны иметь пересечений с объективной реальностью. Всё верно, да, однако эти открытия либо имели до некоторой степени случайный характер, либо сама возможность открытия уже была заложена в контексте mundus в самый момент некоторого более раннего открытия, метафорически выражаясь, путём неявного трансформирования «абстрактного ландшафта» местности при заселении в некоторую локацию нового «жильца-абстракции». К примеру, так было с отрицательными числами, которые изначально считались не имеющими никакого смысла. Тот же Паскаль в своё время возмущённо говорил: «Я знаю людей, которые никак не могут понять, что если из нуля вычесть четыре, то получится нуль» [c]. На данный же момент отрицательные числа уже давно являются признанными «настоящими» (причём во всех математических подходах) и активно используются в расчётах. Далее, если, к примеру, с числами рациональными особых проблем не возникало ещё со времён древнегреческих математиков, то вот с иррациональными было совершенно иначе – ещё Эйлер вынужден был полемизировать с Даламбером о их пользе для сферы анализа, а тот упорно отказывался признавать их «настоящими» числами. Сам Эйлер был за свою волю к победе вознаграждён – в его честь «числом Эйлера» названо основание натурального логарифма (2.71828) [c]. С числами комплексными было также весьма интересно, но в этом случае – ещё показательнее, чем с отрицательными и иррациональными [c]. То есть, математики, используя в расчётах не признаваемые на тот момент «настоящими» комплексные числа, получали верные результаты, сами не особенно понимая почему так произошло. Лейбниц так отзывался о комплексных числах (о мнимой единице как составляющей комплексного числа): «Дух божий нашел тончайшую отдушину в этом чуде анализа, уроде из мира идей, двойственной сущности, находящейся между бытием и небытием, которую мы называем мнимым корнем из отрицательной единицы» [c]. То есть математикам того времени было довольно непросто принять саму идею числа, настолько насколько комплексное, отличающегося от натурального. Так что по большей части из-за их высокой полезности для вычислений, а также некоторым трудам Гаусса, комплексные числа были позднее признаны на одном уровне со всеми остальными.
Таким образом, можно заключить, что зачастую открытия в математике происходят, а связь этих открытий с миром реальным устанавливается много позднее – как с комплексными числами только позже стало понятно, что их можно успешно применять для осуществления расчётов в самых различных областях как в математике, так и в физике. Как будто мы изначально находим некую точку входа Х, которая представляет собой то самое изначальное открытие, изменившее «ландшафт mundus»; затем получаем некие смутные намёки и представления о специфике трансформированного «ландшафта»; а уже потом, после некоторого оформления этих самых смутных намёков в более или менее удобоваримый общий описательный вид, применяя их и идя по тому пути, куда они нас ведут в соответствии с «абстрактной геометрией», приходим уже в точку Y. И обе эти точки – и Х, и Y – имеют прямые связи с объективной реальностью, а вот непосредственно сам «путь через mundus» – нет. В общем, время и опыт – всё расставляют на свои места.
В истории математической науки вообще, как правило, наиболее значимыми и прорывными «абстракциями» являются те, которые сами представляют, как бы «абстракцию над абстракциями» – то, что способно связать воедино то, что само связывает воедино нечто иное – менее абстрактное, если угодно. И вот примерно такой абстракцией стали в своё время множества Георга Кантора, ученика Карла Вейерштрасса, ученика Карла Фридриха Гаусса [ссылки на Вейерштрасса и Гаусса]. И именно тогда и «издала младенческий крик» теория множеств.

ВЫВОДЫ ПО ГЛАВЕ

В данной части нашей работы мы рассмотрели математику с точки зрения того, что же она собой представляет, как дисциплина, каким образом «существуют» её объекты и что означает тот самый «математический мир», в котором эти самые объекты наличествуют.
Изначально нами была рассмотрена бинарная оппозиция подходов к обусловленности и «существованию» математических объектов. В контексте подобной постановки вопроса, на одну «чашу весов» нами был поставлен подход Платона к идеям, как чему-то реально существующему где-то в некоей реальности, а на другую – подход Аристотеля, в рамках которого любые концепты и понятия интерпретировались, как выводимые на основе опыта. Было предположено, что изначально все математические объекты формируются на основе эмпирических данных, воспринимаемых человеком в ходе обучения и взаимодействия с окружающей реальностью; затем же – после того, как достигнута некая необходимая и достаточная точка меры, что выражается в количественном «накоплении» на субстрате когнитивной карты абстракций и связей между ними, происходит их некоторая обособленность от непосредственно эмпирической почвы и эти самые объекты, в их совокупности, начинают в некотором роде «жить собственной жизнью». То есть мы пришли к выводу о том, что, собственно, верными оказываются обе точки зрения – и Платоновская, и Аристотелевская. Просто они верны для различных этапов формирования математических абстракций в контексте когнитивной сферы человека.
На основе вышесказанного нами было дано определение того, что собой представляет математический объект. Итак, математический объект – это концептуально структурированное представление наименее сильной и наиболее абстрактной связи между отдельными феноменами реальности.
Далее, на основе этого определения нами было выведено, что математика в целом может быть определена как наука об абстрактных связях между объектами и операциях с ними.
А тот самый «математический мир», о котором мы говорили ранее, был охарактеризован, как когнитивное отражение совокупности объективированных абстрактных связей. Этот самый «мир» был нами назван mundus, что в переводе с латинского означает «мир».
Далее была охарактеризована проблематика непосредственно фундаментального существования математических объектов и связей между ними. Именно фундаментального, а не просто – для человека. По сути нами был переформулирован вопрос «пахнет ли роза, если её никто не нюхает?» до «существуют ли числа, если никто ничего не вычисляет?». Именно о числах мы говорили, как о некой наиболее удобной математической аппроксимации реального мира, так как с иными математическими структурами всё обстоит несколько сложнее с интуитивной точки зрения. В контексте ответа на данный вопрос нами была предположена гипотетическая форма жизни, которая не использует категории количественного вообще. Было предположено, что в области познания Вселенной подобная альтернативная форма жизни и когнитивной деятельности вполне могла бы успешно существовать и даже «постичь тайны мироздания» вообще не пользуясь числами. Мы представили математическую аппроксимацию подобной специфики когнитивной деятельности в виде непустого множества с размером 2 и выше, для каждых 2-ух элементов которого не определена функция порядка, но в то же время определена функция сравнения. То есть мы не можем сказать, для этого множества, что какой-то один его элемент больше какого-то другого его элемента, но всегда можем сказать, что какой-то один его элемент не равен некоему другому элементу.
На основе вышесказанного был сделан вывод, что для некоторой альтернативной формы когнитивной деятельности понятия числа в буквальном смысле может «не существовать» во Вселенной: например, для них числа 2 действительно «не будет» нигде во Вселенной. Но для нас – число 2 в действительности существует. В этом смысле ответ на вопрос «пахнет ли роза, когда её никто не нюхает?» заключается в том, что «она потенциально способна пахнуть для всякого, имеющего обоняние», а без «наблюдателя» она просто генерирует физическую основу для запаха, но не непосредственно создаёт его. Так же и с числами, как неотъемлемыми «абстрактными фрагментами» нашей реальности. Из чего мы и вывели заключение о том, что реальность предоставляет «субстрат для осмысления», конечно же со своими пределами и условиями, которые могут быть заданы в виде условных граней «возможного» и «невозможного». В рамках этих граней мы имеем этот самый «субстрат», который предоставляет в наше распоряжение «интерпретационный базис». Затем, на основе «возможного» и когнитивных способностей некой формы жизни, формируется отражение, синтез, «промежуточная реальность», которая и представляет то, что обычно понимается под объективной реальностью, которая может иметь «пересечения» с реальностью математической – с mundus.
Далее было показано, после того, как математическая реальность уже наличествует в качестве некой, до известной степени, «особой реальности», объекты mundus способны динамически функционировать и на основе этой динамики можно делать некоторые выводы об окружающей действительности.
Также в контексте данного раздела было уделено немалое внимание логике – как некоему «стражу» и «валидатору» новых математических объектов и смыслов. Правила логического вывода были нами представлены в виде неких «верификаторов» тех или иных математических объектов. Также мы указали на то, что именно на почве логических противоречий и произошёл «великий раскол» в математике, который можно сравнить с историей о «Вавилонской башне». Однако изначально этому расколу предшествовало, наоборот – объединение на почве теории множеств, преимущества которой также были нами предварительно очерчены и сведены к уровню абстрактности, который, для случая с теорией множеств, был определён как весьма высокий.
Собственно, к теории множеств далее мы и переходим.
 
ГЛАВА 2. ТЕОРИЯ МНОЖЕСТВ, КАК ФУНДАМЕНТАЛЬНОЕ ОСНОВАНИЕ МАТЕМАТИКИ

ПРЕДИСЛОВИЕ

Говоря об основах математики, мы как бы имеем ввиду некую дуальную истину: с одной стороны, мы говорим о тех наиболее «абстрактных абстракциях», через которые могут быть эффективно выражены иные – менее абстрактные наличия; а с другой же – мы имеем ввиду нечто наиболее «нерушимое» для самой математики, ту самую опору, которая служит незыблемым фундаментом для всего здания. Вообще говоря, изначально в качестве подобного фундамента выступала и продержалась не одну тысячу лет – евклидова геометрия. Собственно, геометрия вообще сама по себе долгое время считалась основой для формирования «истинного знания», а числа были скорее неким «символьным придатком». Хотя в школе Пифагора и утверждалось, что «мир состоит из четвёрок», смысл этих четвёрок был именно геометрическим. Квадратное число для древних греков представляло собой не просто «число, для которого квадратный корень также является целым числом», но именно то число, для которого можно реально геометрически построить квадратный корень. Собственно, тогда и было обнаружено, что не для всех чисел он существует, по крайней мере, в виде натурального числа.
В общем – геометрия долгое время «выносила на себе истину» математики. Даже Кант в своих трудах утверждал, что геометрия несёт в себе истинное априорное знание и что евклидова геометрия представляет собой геометрию реального мира и невозможна никакая иная геометрия. Построение Лобачевским непротиворечивой (в случае непротиворечивости евклидовой) системы новой, неевклидовой геометрии стало событием, сравнимым по синтезу эффекта и сути, разве что с теоремами Гёделя о неполноте и противоречивости. Можно гипотетически предположить, что если бы на момент создания геометрии Лобачевского (а также Римана) не существовало математического анализа, то математика как дисциплина могла бы ещё во второй половине 19-ого века раздробиться на множество прикладных дисциплин не связанных друг с другом. Но тогда она устояла от разъединения – ряд оказался «почти сходящимся». Выручило то, что на тот момент уже усилиями таких фигур как Гаусс, Коши, Дедекинд и Вейерштрасс были доведены до уровня необходимой строгости понятия математического анализа, а также предложены способы конструктивного определения вещественных чисел – в том числе Кантором, и математика сумела найти необходимую концептуальную опору. Как несколько позже говорил о теории множеств фон Нейман «…нет ни одной аксиоматической системы для математики, геометрии и т.д., которая не предполагала бы теорию множеств» [c].
Не факт, что эта опора продержалась бы вечность, но через некоторое время после этого, также из стремления уточнить некоторые аспекты математического анализа – тригонометрические ряды, Георг Кантор заметил, что совокупности точек расхождения рядов целесообразно бы объединить в некую общую форму. Если бы точек для этих рядов было некоторое точное количество, то их можно было бы записать в виде натурального числа. Но ни точек не было некоторое определённое количество, ни смысл их наличия в целом не умещался просто в понятие числа – нужно было что-то ещё, какая-то новая абстракция. И Кантор назвал это – многообразиями. Довольно скоро название сменилось на множества, но сути это не меняет. К тому же перевод на русский язык слова set – всё же оставляет желать лучшего ввиду своей явной контринтуитивности: пустое множество – звучит не очень. Ещё одним вариантом перевода является слово «набор». Однако «теория наборов» звучит ещё хуже.
Значимость же нового понятия – множества, оказалась в его уровне абстракции. Точки объединяются Кантором не потому, что их какое-то количество, а потому, что они «направляются» к некой «общей цели». Абстрактность числа, как мы уже говорили ранее, в том, что оно позволяет объединить тончайшей связью все объекты во Вселенной в рамках категории количества. Числа есть порождение философской категории количества. А вот множества – уже порождение иной категории, гораздо более широкой. Объекты объединяются в множества – по смыслу, хоть и смысл этот должен как-то математически определяться в контексте теории.
И абстрактность этой категории была такой, что позволяла объединять практически любые математические объекты в совокупность – в единое целое. А затем оперировать ими, как понятиями, как математическими объектами. В сущности, значение комплексных чисел в том, что оказалось очень удобно рассматривать точки на комплексной плоскости в виде реальных чисел – в виде отдельных математических объектов. Можно себе представить насколько удобно оказалось рассматривать примерно в таком же качестве – как отдельный реально данный и оформленный объект – вообще, что угодно.
Разумеется, теория множеств, ввиду как раз-таки своего уровня абстракции, своему потенциалу «связывания» прочих объектов и «математических сущностей», довольно быстро продемонстрировала свою целесообразность. Математики обосновывали использование аппарата теории множеств теми успехами, которые она позволяла им получать в контексте их прикладных и теоретических изысканий. Это неудивительно – что-либо совершенно «ни на что» не похожее не смогло бы «быть утверждённым» в контексте mundus. Так что теория множеств, ввиду своей специфики, действительно позволяла достигать тех результатов, которые были невозможны без понятий подобного уровня абстрактности.
Однако, как мы указывали ранее: чем абстрактнее и шире понятие, тем более оно склонно к парадоксам. И на некотором этапе своего стремительного взлёта с этой неизбежностью столкнулась и теория множеств.

2.1. ГЕНЕАЛОГИЧЕСКИЕ АСПЕКТЫ «УЧЕНИЯ О МНОЖЕСТВАХ»

Теоретико-множественный подход изначально зародился в виде «наивной теории множеств» и самим Кантором назывался – «учение о множествах» [ссылка на одноименную работу Кантора]. Вообще говоря, изначально Кантор называл свою теорию «учение о многообразиях», но потом сменил термин на более нам привычный – множества. Собственно, сама теория оформилась во второй половине 19 века, однако её истоки, предпосылки и вообще те «концептуальные осколки», из которых она была затем «склеена» можно отыскать гораздо раньше в истории развития научной мысли.
Мы бы определили самые первые из них в работах уже упомянутых Платона и Аристотеля. Конечно, свойства отдельно чисел или отдельно геометрических фигур рассматривали в Древней Греции ещё до них. Более того, именно с абстрагирования и началась сама наука, как феномен. «Первые попытки дать рациональное объяснение природы и устройства Вселенной предприняли ионийские философы в VI в. до н.э. Каждый из знаменитых философов этой эпохи: Фалес, Анаксимандр, Анаксимен, Гераклит и Анаксагор – пытался объяснить устройство Вселенной, принимая за основу какую-нибудь одну субстанцию. Фалес считал, например, что все состоит из воды, находящейся в газообразном, жидком или твердом состоянии» [Морис Клайн «Утрата определённости»]. То есть абстрагирование всего разнообразия к некой, по мнению абстрагирующего наиболее значимой, составляющей – начало и философии и вообще науки. В целом подобное, как можно заметить, существенным образом отличается от довлевших ранее альтернативных вариантов объяснения мира, основным из которых являлась вера в сверхъестественное: богов, духов природы, демонов и прочее. Тогда была осуществлена первая попытка установить господство разума над миром. В учении Пифагора, например, аксиомой было «Всё есть число» и числа в его школе рассматривались как буквальные «строительные элементы» бытия, что потом в каком-то смысле продолжили атомисты Левкипп с Демокритом, «дифференцировав» сущее на атомарные неподвижные и неизменные средоточия [с на обоих].
Почему-то, когда говорят о зарождении такого философского направления как холизм, то далеко не всегда считают целесообразным упоминать Платона, в качестве одного из косвенных сооснователей. Нам же кажется иначе, в частности в диалоге «Филеб» Сократом употребляется «…совокупность вещей и это так называемое целое…» [Диалоги Платона]. Конечно, фраза вырвана из контекста, но не только из-за сказанного Сократом в «Филебе» мы считаем необходимым упомянуть о Платоне, как об одном из первопроходцев вполне холистского абстрагирования, которое много позже ляжет в основу теории множеств. А за «идею», а точнее уникальную и специфическую реинтерпретацию, понятия «идей». Вообще само это понятие изначально использовал Демокрит, но именно Платон привнёс в его понимание совершенно особый смысл – как некой умозрительной абстракции, которая есть по сути «шаблон вещи», к которому эта самая «вещь» стремится, никогда его не достигая. Например, ваза может разбиться и перестать существовать, а вот идея вазы – вечна, согласно Платону. Установление неких абстрактных вечных «надстроек» поверх всего реально сущего – основная заслуга Платона и то, за что ему должна быть благодарна математика в целом. Причём, именно эти самые «надстройки» у Платона признаются истинно сущими, а всё остальное – лишь блеклые эфемерные подобия и «тени от костра на стене пещеры» [с на миф о пещере]. Несколько напоминает наше определение математических объектов, как идеальных абстрактных взаимосвязей между разрозненными фрагментированными частями бытия. Определить «шаблоны вещей» в качестве отдельных непременно существующих наличий и пустить их «жить своей жизнью» – основной вклад Платона в тематику нашего исследования. Собственно, сама «идея» – это как раз-таки определение множества.
У Аристотеля здесь также своя роль. Если выразить тезисно, то эту роль можно свести к сформулированной им фразе «Целое больше, чем сумма его частей» [с на Аристотеля]. Здесь уже не только намечается холистский подход, но также и серьёзные философские основания для самой теории множеств. Провозгласить, что совокупность обладает некими особыми, только ей присущими свойствами и качествами, причём теми, которые не следуют линейно из суммирования всех частей, составляющих совокупность – это действительно значимо и на самом деле является чем-то новым в контексте осмысления «вещей» в то время. И это позволяет далее исследовать совокупности уже феноменологически. То есть, если ранее объекты и предметы рассматривались отдельно, как бы диссоциировано от совокупностей объектов и предметов, то благодаря Аристотелю появилась возможность исследовать уже сами совокупности – в отрыве от составляющих их объектов и предметов, как отдельные феномены со своими уникальными свойствами. Отдельные совокупные феномены с уникальными свойствами – так же можно определить, в общем то, и «идеи» Платона.
Евклид в первой книге «Начал» в 8-й аксиоме указывает «И целое больше части» [с]. Собственно, именно с этой аксиомой Евклида вступило в противоречие высказывание уже Галилея о натуральных числах из его работы «Две науки», которое стоит гораздо ближе и чуть ли не вплотную подходит к собственно теории множеств [с]. Там Галилей постулирует бинарную оппозицию в виде двух противоречащих друг другу суждений: 1. Если некоторые числа являются полными квадратами, а другие нет, то вместе полных квадратов и остальных чисел должно быть больше, чем просто полных квадратов; 2. Для каждого полного квадрата есть некоторое число, которое представляет его квадратный корень, а значит – целых чисел и полных квадратов одинаковое количество. Эти два высказывания получили потом название «Парадокс Галилея» [Н. Бурбаки. Основания математики. Логика. Теория множеств // Очерки по истории математики / И. Г. Башмакова (перевод с французского). — М.: Издательство иностранной литературы, 1963. — С. 37—53. — 292 с.].
Уже несколько позже, после введения Кантором понятия мощности множеств, было показано, что истинным является второе высказывание Галилея, так как мощность множества натуральных чисел и мощность множества полных квадратов – равны., несмотря на то, что множество полных квадратов представляет собой подмножество множества натуральных чисел. В этом случае истинным оказалось второе высказывание Галилея, а ложной – аксиома Евклида. Однако, здесь всё же важно понимать контекст. Высказывание Галилея и дальнейшее доказательство равномощности множеств натуральных чисел и полных квадратов относились к бесконечным множествам. В этом смысле высказывание Галилея интересно не только тем, что он задаётся вопросами о мощности множеств, не употребляя, впрочем, ни одного ни другого понятий, а ещё и тем, что само высказывание подразумевает именно бесконечное множество. Во времена же Евклида рассматривать актуальную бесконечность в качестве опорного пункта при формировании доказательства было неправомерно. То есть для любой конечной последовательности (идущих друг за другом в арифметической прогрессии с инкрементированием каждого последующего числа, по отношению к предыдущему, на 1) натурального ряда количество полных квадратов будет меньше, чем количество остальных чисел, не считая единственного варианта последовательности чисел: (1, 2, 3, 4) – для которой действительно мощности подпоследовательностей полных квадратов и остальных чисел одинаковы. Так что, как второе высказывание Галилея и доказательство равномощности полных квадратов и натуральных чисел – всегда истинны только для бесконечных множеств.
Как можно заметить из вышесказанного, понятие бесконечности играет роль далеко не последней скрипки как в качестве одной из ключевых детерминант процесса становлении теории множеств, так и в её теоретико-методологическом аппарате. И это ещё «недосгущение красок» – на самом деле с этим аспектом в теории множеств всё обстоит куда серьёзнее, и мы позже к этому ещё вернёмся. А произошло так потому, что актуальная бесконечность, после своего присутствия в дискуссиях древнегреческих философов, вновь «подняла голову» в Новое время, не в последнюю очередь в связи с зарождением такой математической дисциплины как анализ. В частности, что для нас актуальнее и что было ретроспективно раньше – анализ бесконечно малых. Как говорил фон Нейман на эту тему «Анализ бесконечно-малых был первым достижением современной математики, и трудно переоценить его значение. Я думаю, что оно, больше, чем что-либо ещё, однозначно определяет отправную точку современной математики, а математический анализ, который является его логическим развитием, по-прежнему определяет наибольший технический прогресс в точном мышлении» [von Neumann, J., «The Mathematician», in Heywood, R. B., ed., The Works of the Mind, University of Chicago Press, 1947, pp. 180—196. Reprinted in Bródy, F., Vámos, T., eds., The Neumann Compedium, World Scientific Publishing Co. Pte. Ltd., 1995, ISBN 9810222017, pp. 618—626.]. Разработка исчисления приписывается сразу и Ньютону, и Лейбницу – теперь принято считать, что они независимо пришли примерно к одному и тому же, хотя изначально Ньютон обвинял Лейбница в плагиате, но стороны умудрились прийти к согласию [ссылки]. Введение же символизма для исчисления приписывается Лейбницу.
Казалось бы, что анализ бесконечно малых обязательно должен привести математику того времени к принятию актуальной бесконечности, в качестве неотъемлемого и значимого понятия. Но в том случае математике удалось отделаться «малой кровью»: понятие актуальной бесконечности удалось свести к бесконечности потенциальной за счёт введения понятия пределов. Предел функции – это некоторое значение, к которому стремится функция при определённых аргументах, но которого никогда не достигает [с на предел]. В этом смысле получается и правда нет никакой необходимости вводить иные дефиниции – есть некий недостижимый предел и есть значения, которые к нему стремятся, но не смогут достигнуть, а будут просто бесконечно уменьшаться – то есть, по сути, здесь мы имеем редукцию вопроса от актуальной бесконечности к потенциальной. С потенциальной же бесконечностью интуитивно можно было разобраться и временно вопрос закрылся.
Но всё же за возникновением анализа бесконечно малых через некоторое время последовали попытки возрождения понятия именно актуальной бесконечности. И детерминанты этих попыток следует усмотреть, как нам представляется, в той «абстрактной мощи», которой обладает понятие актуальной бесконечности. И особенно этот фактор интенсифицируется при «введении» данного понятия в mundus. В контексте теории множеств актуальная бесконечность играет, в общем-то, ведущую роль. Без неё большая часть самой теории практически перестаёт функционировать должным образом.
Зачатки же теории множеств, наиболее к ней близкие в концептуальном смысле, принято относить к работам Гаусса, Галуа, Шёмана, Серре, Дирхиле, Штейнера и Штаудта [с]. Эти разрозненные теоретические наброски (применительно именно к вопросу о теории множеств), в каждом из которых явственно прослеживалась целесообразность актуальной бесконечности и, порой, зачатки концепции биективности, были затем обобщены Дедекиндом в виде его соображений и высказываний об отдельном феноменологическом рассмотрении совокупностей объектов [с]. В труде же Больцано «Парадоксы бесконечного» середины 19 века намечаются уже серьёзные методологические пересечения с, почти вплотную подошедшими к реализации в историческом моменте, идеями Кантора [с]. В частности, именно в этой работе даётся относительно явное определение биекции. Однако работа Больцано более тяготела к философской составляющей и в меньшей степени содержала строгие математические постулаты и выводы.
Также целесообразно обратить внимание и на то, что на момент середины 19-ого века в среде математиков уже не было довлеющим мнение о том, что они, в ходе своих научно-исследовательских изысканий, постигают «замысел Бога». Не было также уверенности в том, что вообще математика способна открыть «высшую Истину» и даже что математика вообще точно уверена в том, что и как делает и чем занимается. Этому в какой-то степени способствовало то, что после двухтысячелетней убеждённости в истинности евклидовой геометрии, которую даже сам Кант признавал «априорной» и утверждал, что не может существовать иная геометрия, после того как эта уверенность не просто несколько пошатнулась, а канула в пропасть – «все начали просить прощения». Ранее математики привыкли к постоянным сложностям, несоответствиям и «непонятному» в контексте алгебры, но вот именно геометрия – была нерушима со времён древних греков. И если отрицательные, иррациональные и комплексные числа вынуждены были «лбом пробивать дорогу» к признанию, раз за разом доказывая свою полезность и целесообразность – эти проблемы оставляли на откуп алгебре, почитаю ту «развивающейся» дисциплиной, «пока что» несколько не дотягивающей до идеальной геометрии.
Собственно, сами древние греки, не считая, по всей видимости, пифагорейцев («всё есть число»), вообще полагали сами числа как бы «вспомогательным» инструментом по отношению к познанию геометрических фигур. То есть для древних греков, например, квадратные числа были не просто «числами, которые считаются квадратными потому, что их квадратный корень является натуральным числом», а на самом деле это были числа, из которых в буквальном смысле получалось выложить квадрат – камешками или любыми иными подручными и пригодными для этого средствами. Только затем уже, в александрийский период, а также в особенности ещё позже – уже индийцами и арабами, числа начали рассматриваться отдельно от геометрических фигур, как некоторые самостоятельные «наличия» и алгебра начала становится более или менее полноценной дисциплиной, хоть и изначально практически полностью прикладной. Но до уровня «старшей сестры» – геометрии, как уже говорилось, она в любом случае не дотягивала как с точки зрения непротиворечивости, так и по показателям доверия и признания со стороны самих математиков. Этот прекрасный «рай Евклида», как обычно и бывает с чем-либо идеальным, несколько омрачался «неуместно» там расположенным «пятым постулатом», с которым постоянно пытались бороться любыми возможными средствами, но в целом – математика прекрасно себя чувствовала, имея опору в виде евклидовой геометрии, в которой практически никто не видел нужды сомневаться.
Однако, на момент начала 19 века ситуация с основаниями математики оставляла желать лучшего. Не было чётких определений понятий математического анализа – и это была только одна из насущных проблем. Когда за исправление этого недочёта взялся Коши, а затем и Вейерштрасс, то общий фон начал несколько налаживаться. Однако до идеала было всё же ещё далеко. К тому же, Коши, который дал строгие математические определения таким понятиям как интеграл, пределы, бесконечные ряды, дифференциал и прочие – строил обоснования на понятии числа, сам не имея строгого определения понятия вещественного числа. В этом плане Коши предложил только критерий сходимости рядов, который был позже назван «критерий Коши» и который в дальнейшем использовал Кантор для выведения конструктивного определения вещественных чисел на основе бесконечных последовательностей.
Но изначально лёгкие веяния того, что случилось в первой половине 19-ого века почувствовал уже Гаусс ближе к концу века 18-ого – возможность непротиворечивой неевклидовой геометрии. И когда появилась геометрия Лобачевского, то настроение математиков того времени, тех из них, которые обосновывали «непогрешимость» математики на непротиворечивости евклидовой геометрии, можно ретроспективно сравнить только, пожалуй, с настроением Гильберта после выступления Гёделя на конгрессе в Кёнигсберге 7 сентября 1930-ого. В общем, можно предположить, что математика, по состоянию на вторую половину 19-ого века остро нуждалась в чём-нибудь таком, что заставит её вновь «поверить в себя» после потери той опоры, которой и была ранее геометрия Евклида. Успехи в обосновании понятий математического анализа были довольно позитивным событием сами по себе и «счастливым предзнаменованием» на будущее, но этого было мало для формирования «монолитной» единой математики.
Из вышесказанного можно сделать вывод о том, что для того, чтобы собрать всё воедино – вывести синтез из результатов анализа в общем смысле слова, в принципе уже была готова высококачественная основа – это аспект теоретический. Социальный же аспект, который был нами также охарактеризован, находился в состоянии готовности как минимум не худшей – математика того времени была в состоянии принять новые идеи, теории и подходы, как это всегда случается после кризиса в какой-либо сфере.  И вот в 1874 на страницах «Журнала Крелле» увидела свет работа Георга Кантора «О свойстве класса всех действительных алгебраических чисел» [с]. Этот момент принято считать точкой отсчёта для «наивной теории множеств».

2.2. АКТУАЛЬНАЯ И ПОТЕНЦИАЛЬНАЯ БЕСКОНЕЧНОСТИ В КОНТЕКСТЕ ТЕОРИИ МНОЖЕСТВ

В связи с тем, что, как уже говорилось нами выше, понятие бесконечности, причём именно актуальной бесконечности, а не потенциальной, в некотором роде играет роль несущей конструкции для теории множеств Кантора, считаем целесообразным, прежде рассмотрения самой теории множеств в её непосредственном виде, раскрыть оба этих понятия и охарактеризовать разницу между ними. Собственно, Кантор порой добавлял к ним также и абсолютную бесконечность, но это уже, как нам представляется, несколько чрезмерное понятие, так как сам факт добавления ещё и его может свести все усилия и попытки охватить пониманием хотя бы бесконечность актуальную [ссылки на абс. Беск.].
Причём в данном случае мы не видим особой целесообразности в том, чтобы проводить более или менее глубокое исследование исторического развития понятия бесконечности, а полагаем, что более продуктивным будет осмысление того, что мы имеем по итогу в виде данных феноменов – потенциальной и актуальной бесконечности. В эту тему скажем лишь, что в Древней Греции также рассматривали понятие потенциальной бесконечности, но актуальную – полагали невозможной. Допустим, в «Физике» Аристотеля читаем: «В самом деле, о бытии можно говорить либо в возможности, либо в действительности, а бесконечное получается либо прибавлением, либо отнятием. Что величина не может быть бесконечной актуально, об этом уже сказано, но она может быть беспредельно делимой (так как нетрудно опровергнуть учение о неделимых линиях); остаётся, таким образом, бесконечное в возможности»; далее «Вообще говоря, бесконечное существует таким образом, что всегда берётся иное и иное, а взятое всегда бывает конечным, но всегда разным и разным. Так что бесконечное не следует брать как определённый предмет, например, как человека или дом, а в том смысле, как говорится о дне или состязании, бытие которых не есть какая-либо сущность, а всегда находится в возникновении и уничтожении, и, хотя оно конечно, но всегда разное и разное»; также «Итак, бесконечное есть там, где, беря некоторое количество, всегда можно взять что-нибудь за ним. А где вне ничего нет – это законченное и целое» [Перевод: В.П.Карпов, из книги "ФИЛОСОФЫ ГРЕЦИИ ОСНОВЫ ОСНОВ: ЛОГИКА, ФИЗИКА, ЭТИКА" издательство ЭКСМО-Пресс; Харьков 1999, 1056 с. OCR: Сергей Васильченко]. Примерно такого же мнения придерживались и его современники.
Собственно, и до самого Кантора, и во времена его активной работы над своим детищем, коим является теория множеств, многие другие, не менее выдающиеся, чем сам Кантор, математики (Гаусс, Пуанкаре, Лейбниц, Лагранж, Брауэр и многие другие) выступали принципиально против введения актуальной бесконечности в математику в качестве полноценного валидного понятия. И стоит предположить, что ведь неспроста.
Так что же это такое – бесконечность? И почему проводится демаркация на актуальную и потенциальную бесконечности? Зачем Кантору была нужна ещё и абсолютная? Может существуют и иные её виды?
О бесконечности можно точно и определённо сказать очень немногое, и, как мы уже упоминали ранее, бесконечность – понятие философское. То есть, с одной стороны, если сказать, что «о бесконечности можно говорить бесконечно», то это будет правомерным высказыванием; а с другой – можно сказать, что «бесконечность являет собой отрицание всякой конечности» – и на этом закончить повествование о ней, так как данное высказывание представляется исчерпывающим. Со стороны Кантора было весьма смелым шагом ввести в математику «кусочек» философии. Как тогда казалось, если «обмотать лампочки мокрыми тряпками», то всё должно отлично заработать. На деле же получилось, что не совсем так. Но тут имеет место ещё один весьма интересный момент. А именно, Кантор, «одолжив» у философии понятие бесконечности и преобразовав его в соответствии со спецификой mundus, стал критиковать философов же, в частности Канта и Гегеля, за специфику использования этого, ещё раз подчеркнём, философского понятия [ссылки на Канта и Гегеля].
Например, Кантор пишет «Кант без серьезной предварительной критической работы оперирует понятием бесконечности при рассмотрении четырех вопросов, стараясь доказать, что на них с одинаковым правом можно дать утвердительные и отрицательные ответы – лишь благодаря смутному неотчетливому употреблению понятия бесконечности этому автору уда-лось вызвать серьезное отношение к его антиномиям и к тому же только у тех лиц, которые, подобно ему, предпочитают уклоняться от основательного математического рассмотрения подобных вопросов» [Кантор Г. Труды по теории множеств. М., 1985. С. 266]. Также, согласно Кантору, у Гегеля с бесконечностью всё также «противоречиво» [там же. С. 280].
Можем, немного побыв в роли предсказателей, предположить, что подобное, в соответствии с канонами Ницшеанского «вечного возвращения» (не совсем его, впрочем) – ещё вернётся к самому Кантору и, возможно, уже очень скоро [ссылка на Ницше]. Вообще конечно, мы не хотим сказать, что Кантор был противником философии. Напротив, он очень уважительно относился к учению Лейбница, дружил с Эдмундом Гуссерлем – тоже, кстати, как и сам Кантор, учеником Вейерштрасса и даже переживал, что его учение о множествах не принимается святыми отцами церкви – Кантор на самом деле был глубоко верующим и религиозным человеком. И, можно предположить, что именно на этой почве Кантор и вывел понятие бесконечности абсолютной – как некоего сугубо Божественного атрибута. Ведь любой глубоко верующий человек скажет, что мы не в силах в полной мере постичь «замысел Создателя», «природу Божьей Воли» и прочие ноумены из этой же категории. В частности, в переписке с Гильбертом, интенсивно поддерживавшим Кантора в его разработке теории множеств, Кантор указывал на необходимость проведения демаркации между понятиями трансфинитного, как доступного человеку для понимания и абсолютного как, соответственно, недоступного – к которому можно только сколь-нибудь приблизиться. Гильберта, как сторонника того, что всё может быть объяснимо, а сама формулировка «недоступное для человеческого понимания» абсурдна, такой подход Кантора по понятным причинам не устроил. Переубедить Гильберта в его, прямо скажем, весьма благородном и очень даже конструктивном стремлении всё рационально и непротиворечиво выразить, смог только лишь Гёдель со своими теоремами о неполноте и противоречивости.
Однако, после этого небольшого отступления, вернёмся к бесконечности: актуальной и потенциальной. Также, теперь мы уже можем добавить, что порой среди всего «многообразия» бесконечностей выделяют бесконечности ещё количественную и качественную – и абсолютная бесконечность Кантора – это скорее уже про второе из этого.
Наиболее простой для понимания является бесконечность потенциальная, которая, конечно, всегда характеризуется, как количественная. Её можно эксплицировать как некий процесс, который никогда не завершится. То есть изначальные условия заданы именно так, чтобы завершения не предполагалось и конечным процесс не мог оказаться. Самый простой пример – подсчёт натуральных чисел: 1, 2, 3, 4, 5, …n, ∞, ∞+1, ∞+2, ∞+n, ∞+∞ и так далее. Таким образом, сколько бы мы не считали и на каком бы именно числе не находились в любой момент подсчёта – всегда есть возможность прибавить единицу (или ещё сколько-то единиц) и получить следующее число. Об этом также пишет Кант в Критике чистого разума: «Бесконечна та величина, больше которой (то есть больше определенного множества содержащихся в ней данных единиц) невозможна никакая другая величина. Но никакое множество не может быть наибольшим, так как ко всякому множеству можно прибавить еще одну или несколько единиц. Следовательно, невозможна никакая бесконечная данная величина» [Кант. Критика чистого разума. С. 274]. В контексте программирования данный пример проиллюстрировать даже проще: достаточно просто при помощи ключевых слов создать цикл с всегда истинным условием – и мы получим бесконечный цикл. В прямом смысле – если бы сколь угодно большое количество людей, сменяя друг друга по причине смерти, ждали бы завершения подобного цикла – Вселенная «закончилась» бы раньше, если она, конечно, подразумевается конечной.
В общем бесконечность потенциальную и количественную мы вполне можем себе вообразить, представить, «охватить умозрением» и даже реализовать – серьёзных проблем возникнуть не должно. Если абстрагировать саму потенциальную бесконечность, то это просто «начало без конца».
С актуальной же бесконечностью всё обстоит совершенно иначе, и в первую очередь – в интуитивном смысле. То есть человеческий «когнитивный аппарат» поначалу просто отказывается принимать подобное – к этому нужно привыкнуть. Даже с чисто качественной бесконечностью куда проще, особенно человеку религиозному – ему достаточно помыслить о Боге. И действительно, мы можем представить бесконечного, именно в качественном смысле безначального, бесконечного, всеобъемлющего и всемогущего Господа – и этот мысленный образ не будет восприниматься как нечто неестественное (всеми, кроме убеждённых сторонников диалектического материализма, конечно). То есть мы можем себе вообразить Абсолют как некий безначальный и бесконечный центр Мироздания и одновременно – всепроникающее и вездесущее наличие [ссылка]. Мы можем представить себе апейрон Анаксимандра, как деструктурированное, десистематизированное, беспредельное и безначальное первовещество [ссылка]. Вообразим хтонический хаос в его первозданности. Можем помыслить о Логосе, Эросе, Танатосе, Истине и прочих подобных ноуменах [4 ссылки]. Как мы и сказали – сугубо качественная бесконечность не вызывает отторжения и ощущения абсурда. Ранее мы показали то же самое на примерах потенциальной и количественной бесконечностей. И там тоже всё вполне вписывается в рамки среднестатистического понимания и интуитивно воспринимаемого мира.
А вот теперь для примера возьмём открытый интервал вещественных чисел от 0 до 1 – (0, 1). Открытость интервала означает актуальное отсутствие в нём тех чисел, которые указаны в качестве начальной и конечной «точек». То есть в нашем случае интервал не содержит чисел 0 и 1. Тогда что же он содержит? Все нецелые числа от 0 до 1 – рациональные и иррациональные, то есть – действительные или, синонимично – вещественные. Сколько их там? Бесконечность. Как такое может быть? Для любого числа, входящего в наш интервал, доказано, что в этом же интервале наличествует для него число, которое меньше, чем оно в два раза: то есть, более формально – для любого числа x в нашем интервале есть число, тоже входящее в интервал, которое равно x/2. То есть в наших вышеприведённых скобках прямо сейчас наличествует бесконечное множество вещественных чисел. Казалось бы, если множество бесконечное, то почему с ним ничего не происходит? Почему какая-либо из его имманентных интенций не устремляется куда-либо? Ведь некоторая «статичность» как раз является характеристикой и атрибутом всякой «завершённости» и «конечности». Именно так – множество «уже дано» в бесконечном виде и подразумевается «внутренне» бесконечным. Как может быть нечто бесконечное «уже данным» – это отдельный вопрос, и он уже вступает в прямое противоречие с нашей интуицией. Сочетание характеристик «уже дано» и «бесконечно» по отношению к одному и тому же объекту в один и тот же момент времени – изначально выглядит как абсурд. Но, тем не менее, несмотря на все упомянутые и неупомянутые затруднения, противоречия и парадоксы, требуется просто принять это и привыкнуть к этому, так как именно теперь перед нами – актуальная бесконечность во всей красе.
Здесь мы не станем приводить аргументации ни в поддержку правомерности и целесообразности подобной концептуализации бесконечности, ни против этого. Просто уточним, что актуальная бесконечность, и именно в таком виде, в контексте теории множеств – одно из основных понятий. То есть с этим работают, на это опираются при уточнении теории, это используют при её развитии и расширении. На данный момент здесь нет места для философского осмысления, казалось бы, очевидной проблемы. Пока просто примем, что это понятие используется и полагается правомерным в математике, а конкретно для теории множеств – это несущая конструкция всего здания. Говоря проще, если вдруг с актуальной бесконечностью «что-нибудь» случится, то это также прямо пропорциональным и непосредственным образом отразится и на всей теории множеств также.
Теперь же приведём утверждение: актуальная бесконечность – это бесконечность потенциальная, преобразованная в соответствии с законами и принципами mundus. И на разности данных понятий особенно хорошо просматривается также и ярко выраженная разность самих миров – объективной реальности с одной стороны и mundus – с другой. Мы полагаем наше определение правомерным и целесообразным, так как понятию актуальной бесконечности «больше негде жить», кроме mundus, «в районе» теории множеств. Так как в контексте любой социально-гуманитарной дисциплины актуальная бесконечность тут же будет преобразована в непосредственно качественную – и проблем не возникнет. В сфере естественных наук она тут же обратится бесконечностью потенциальной и, естественно, всегда количественной. И только в mundus возможно уникальнейшее явление – синтез актуальной и количественной бесконечностей. На данный момент это своеобразное резюмирование специфики mundus в целом – здесь лучше всего можно понять, что это за мир и «какая там погода».
Неудивительно, что как только актуальная бесконечность «выходит» оттуда – в сферу философии, методологии, психологии, просто интуиции и даже, что кстати несколько неожиданно, формальной логики – сразу же начинаются проблемы и парадоксы.
К проблеме бесконечности в контексте теории множеств мы ещё неоднократно вернёмся позже. А на теперешний момент мы должны просто принять как данность то, что «таким и только таким» образом «обстоят дела» в контексте математики, в «районе» теории множеств.
Возможно будет нелишним заметить, что порой возникающие проблемы в вопросе понимания бесконечности, можно редуцировать к проблеме времени. То есть можно сказать, что затруднения в понимании бесконечности во многом обусловлены тем, что конечность и бесконечность должны подразумевать некие точки отсчёта, некие деления на «до» и «после»; на «тогда», «сейчас» и «потом»; там, кажется, должна где-то быть точка «в тот момент» и так далее. Собственно, никого за подобный подход критиковать неправомерно, так как если попробовать «отнять время от бесконечности», то мы получим нечто уж совсем гротескное. К тому же важная проблема заключается в том, что мы по сути и сейчас, в наш век технологий и интеллектуальных технологий не можем внятно объяснить, что же такое вообще время. И даже более того, дискуссии на тему того, существует ли это самое время вообще и не является ли оно некой «фикцией» активно проводятся, то есть – проблема «живая». Чтобы знать, что получится по итогу при операции вычитания от некоего Х некого Y, необходимо знать, что же именно мы собираемся вычитать и от чего – а мы, по всей видимости, понятия не имеем ни про Х, ни про Y в данном случае.
Попробуем подойти к проблеме логическим путём и привнесём в этот самый путь немного диалектики в духе Гегеля. Предварительно же заметим, что нижеприведённое уже будет несколько отличаться от нашего «стандартного» дискурса до актуального момента изложения.

О времени и бесконечности

«В данном случае интересным является вопрос: существует ли бесконечность вне времени? Хорошо, для этого сначала нужно знать, что мы имеем ввиду под временем. Предположим, что мы знаем и что время – есть некое фундаментальное движение. Далее попробуем пойти от противного: существует ли вне времени нечто конечное? Если мы редуцировали время к движению, то представляется, что существует и, более того, конечным станет всё, так как всё застынет в статике. Тогда каким образом мы охарактеризуем существование? Если ничего не происходит и, более того, «никто не может сказать, что ничего не происходит», то происходит ли хоть что-то? Это вопрос в тему «пахнет ли роза, когда её не нюхают», только в абсолютном масштабе – вопрос про квалиа, про «рождение» феномена в моменте взаимодействия. А взаимодействие, по всей видимости, невозможно вне движения, к которому мы редуцировали время, а значит – и роза не пахнет и ничто не существует. Таким образом, при редукции времени к движению – конечное не существует. Тогда существует ли бесконечное при этих же условиях? Если не особенно цепляться за понятия, особенно за такие как качественная бесконечность, а более апеллировать к непосредственно логике, то мы, скорее всего заключим, что раз не существует ничего конечного, то и бесконечному также «взяться неоткуда». Таким образом, можно уже вывести постулат о том, что при редукции времени к движению вообще ни конечное, ни бесконечное существовать не могут, то есть – невозможно ни бытие, ни сущность.
Возможно, мы предоставили времени слишком большую юрисдикцию. Попробуем её сократить. Скажем, время не есть вообще атрибут всякого движения, а есть, например, атрибут передачи этого движения от одной части целого к другой. Всё, что вне этого – также и вне времени. Таким образом, что мы можем сделать вне времени? Например, мы можем опустить дальнейшую логическую цепочку, так как она представляется излишней, и сразу постулировать, что попытка «вычесть» время хоть из какой-нибудь составляющей того бытия, которое мы способны себе представить – «остановит» также и все остальные составляющие. И мы вновь придём к тому, что вне времени существование не представляется возможным.
Отсюда следует что, либо понятие времени настолько глубоко «засело» в нашем понимании и описании мира, что, хотя бы представить что-нибудь вневременное мы не способны; либо, как ещё один вариант, время и правда представляется краеугольным камнем бытия любой сущности.
С другой стороны, попробуем вернуться к нашему предыдущему выводу о том, что при не-существовании конечного ввиду всякого движения из-за отсутствия времени, бесконечному тоже существовать не придётся. Точно ли здесь всё логично? То есть, если не существует ничего конечного, то вот, например, сколько это не-существование должно будет продлиться? Логично предположить, что нисколько, так как нет никаких точек отсчёта и это даже не может никак измениться. Но ведь, если подобное не может никак измениться, то оно является вечным, а если оно является вечным, то оно также и бесконечно. Ну, по крайней мере, наше первое предположение мы уже опровергли. А второе значит является верным? Ну, учитывая тот факт, что мы только что вывели бесконечность безо всякого времени, то, по всей видимости, и оно также не является истинным. Что же у нас получилось по итогу? Бесконечное не-существование и небытие. Но тогда получается, что тот постулат, который мы выдвинули изначально, о невозможности существования вне времени ни конечного, ни бесконечного – неверен также, ведь мы вывели бесконечность вне времени. Ну да, получается, что неверен.
С другой же стороны, как мы можем считать подобное «существование» бесконечности в виде не-существования вообще валидной в данном контексте? Тогда предположим, что бесконечность в виде вечного не-существования – не валидна.
Но получается, что если бытие есть отрицание небытием самого себя, а существование есть отрицание «не-существования не-существованием», то мы только что как раз и «запустили» момент существования в виде отрицания возможности несуществующего существовать. Таким образом, бесконечность не-существования – существует. Или нет?»

Интересно как-то получается. Но вот понять, почему так получается – почему-то не получается. Попробуем обратиться к классике, возможно это поможет.

Стрела Зенона

«Летящая стрела в каждый момент времени занимает положение равное себе, а значит покоится. А если она покоится в каждый момент времени, то значит она покоится во все моменты времени, соответственно – она покоится всегда. То есть – стрела не движется вообще».

Касаемо «Стрелы Зенона» – с логической точки зрения в ней всё правильно. И до самого Гёделя никто не смел проинтерпретировать этот факт с тех позиций, что явно что-то не так с самой логикой в целом, раз уж в «Стреле Зенона» всё правильно. Но мы пока не пойдём дальше этого соображения. Собственно, «Стрела Зенона», как, в общем-то, и прочие его апории о движении, предназначены для опровержения постулата о том, что из бесконечно малых частей складывается неделимое целое. Ну и логически, соответственно, выходит, что не складывается. Со времён возникновения апорий Зенона с ними пробовали логическим путём «сражаться» многие великие учёные – от самого Аристотеля вплоть до наших дней. Но, так как апории логически безукоризненны, то и сражаться с ними той же логикой несколько затруднительно. Однако валидным опровержением может здесь считаться только логическое. Конечно, уже не раз доказывалось, что мы можем посчитать сумму бесконечного ряда и поэтому апории Зенона о движении – не верны. Но мы утверждаем, что не правомерно применять к апориям Зенона, в попытке их опровержения, ничего, кроме самой логики. И это так по той причине, что сам Зенон не применял ничего, кроме логики. Применять математический анализ к логическим концептам – это примерно то же самое, что прийти на соревнования по боксу с гранатомётом. Таким образом, валидным опровержением должно быть именно логическое. Причём примерно столь же краткое, сколь сами апории. Здесь интересно другое – почему сам Зенон не пошёл ещё дальше в своих апориях – логика способна на большее.

Момент Зенона

«Если летящая стрела в каждый момент равна сама себе, то каждый момент времени в каждый момент времени также равен самому себе, а значит – он покоится. А соответственно – нет такого момента времени, в который момент времени не покоится, а значит – он всегда сам равен самому себе и всегда покоится. Следовательно – существует только один момент времени, и он покоится. Соответственно, если существует только один момент времени, то из этого ничего не следует, так как чтобы «следовать» откуда-то куда-то нужно как минимум два момента времени. Следовательно, либо из того, что в апории Зенона стрела в каждый момент времени равна сама себе и, соответственно, не движется – ничего не следует, либо стрела движется и из этого следует, что она не движется, так как, если она движется, то моментов времени как минимум два, а значит – из апории Зенона следует, что стрела не движется. Но, если она не движется, то значит момент времени один и из этого следует, что из апории Зенона ничего не следует, а значит – стрела движется. Или не движется?».

В сущности, мы могли бы с таким же смыслом рассмотреть и прочие апории Зенона о движении, но нам они представляются логически «несвязными». То есть, к примеру, из апории Зенона о стреле с делением пополам расстояния, с равными шансами следует как то, что движение никогда не начнётся, так и то, что оно, в общем-то уже завершилось; а из парной к предыдущей апории «Ахилл и черепаха», если внимательно присмотреться к используемой Зеноном логике, следует что, движение Ахилла и черепахи рассматривается отдельно, хотя заявляется ровно наоборот, поэтому возможен один из двух вариантов – при совместном движении Ахилл догоняет черепаху на втором этапе, а при отдельном –  уже сразу на первом. Само собой разумеется, что проблема с апориями Зенона в основном в том, что они, при логической «непогрешимости» продуцируют выводы, которые противоречат реальным наблюдениям. Мы же просто хотели продемонстрировать, что, во-первых – апории Зенона содержат логическую аргументацию, которая, при буквально следующем логическом шаге прямо противоречит выводам из них же; а во-вторых, хотели показать способность логики формировать «замыкания», что пригодится нам при дальнейшем изложении.
В завершение продемонстрируем суть апорий Зенона о движении в более простой и краткой форме.

Погода Зенона

«В любом месте погода всегда такая, какая есть, то есть – равна сама себе, значит – неизменна. Следовательно, погода нигде никогда не меняется».

Здесь резюмируем. С тем, что мы только что показали на примерах, мы позднее познакомимся подробнее и детально это охарактеризуем. А на данном этапе стоит ещё раз напомнить ключевую мысль данного раздела: актуальная бесконечность количественного типа принята в теории множеств в качестве валидного понятия.

2.3. ТЕОРИЯ МНОЖЕСТВ: НАЧАЛО И «НАИВНАЯ» ВЕРСИЯ

Теорию множеств, изначально «наивную теорию множеств», проще всего рассматривать в контексте нашего определения математических объектов и самой математики [с на наивн.т.м]. Но всё же сначала покажем, как определял множества сам Кантор: «Под многообразием, или множеством, я понимаю именно то любое многое, которое можно трактовать как нечто одно, то есть всякое целостное понятие определенных элементов, которое – в соответствии с некоторым законом – может быть объединено в целостность. И полагаю благодаря этому определить нечто родственное платоновскому эйдосу или идее agathon, как и тому, что Платон в своем диалоге «Филеб, или высшее благо» называет agathon» [Кантор Г. Труды по теории множеств. М., 1985. С. 101]. Чем же, если не высочайшим уровнем абстрагирования, является «любое многое, которое можно трактовать как нечто одно»?
И это на самом деле так. Как мы говорили ранее, теория множеств потому и стала в своё время претендовать на вакантное место математической «теории всего» и прочного фундамента всей данной дисциплины, что предоставляла некую «абстракцию над абстракциями». Действительно, если мы мыслим число 1, как наиболее абстрактную связь между всем во Вселенной «чего один», то понятие множества идёт даже дальше этого и позволяет помыслить совокупность абстрактных наличий в качестве единой структуры, некоторого объединения любых возможных абстракций. То есть, мало того, что теперь можно установить связь между любым количеством объектов в виде некой совокупности с n-ой мощностью, но также и объединить все подобные совокупности (сколь угодно много раз) и они не будут представать апофеозом разрозненности, а напротив – будут представлять собой организованность с чёткой структурой и иерархией. Множество – это абстракция, «облекающая» собой абстракции более низкого или равного уровня. И это работает на всех этих уровнях, за исключением, возможно, непосредственно «абсолютного», на котором уже возникает парадокс Рассела и появляется «максимальное» порядковое число, которого, по словам самого Кантора – не существует [с на пор. числа].
Чем же именно множество «лучше» числа, если и то, и другое рассматривать, как абстрактные связи между некими объектами? Понятие множества – более широкое, более гибкое и является существенно функциональней. Допустим, мы можем сказать, что число 1 служит некой абстрактной надстройкой над всеми объектами во Вселенной, «которых один». Но если бы мы хотели как-то объединить в совокупность все те объекты, которых не просто один, но которые, например, ещё и красные – понятия числа уже «не хватит». Максимум, что мы можем – это придать связи положительную или отрицательную коннотацию в виде установления положительного или отрицательного знака числа. И тут на помощь приходит понятие множества. Если мы сформулируем высказывание «множество всех единичных красных объектов» – то из всех объектов во Вселенной, которые локально представлены в единственном числе и являются красными сразу же возникнет совокупность, то есть они более не «разбросаны где попало», а являются объединёнными в чёткую абстрактную структуру. И далее с этим множеством можно работать, как с математическим объектом. Попробуем что-нибудь посложнее: «множество всех директоров школ», «множество умных философов в этой комнате», «множество всех людей на планете Земля, у которых имя с фамилией составляют не более 9 символов», «множество притоков Дуная», «множество актуально возбуждённых нейронов центральной нервной системы» – числами всё это не выразить или было бы очень неудобно, а вот понятием множества – легко и просто.
Выше мы уже постулировали, что в mundus, чем более высокоуровневой является абстракция, тем более ценной и «значимой» она предстаёт. В данном контексте нисколько не удивительно, что появившаяся позже теория категорий, сумев определить сами множества в одну из своих, соответственно, категорий (представив множества в виде объектов категории, а их отображения – в виде морфизмов) – сама стала претендовать на почётный статус математической «теории всего». Возникни вслед за теорией категорий, скажем, «теория универсалий», которая сумеет представить теперь уже сами категории в качестве одной из своих универсалий – и данная последовательность возвышения абстрактных надстроек будет успешно продолжена. Вообще данная тенденция, конечно же, характерна для научного знания в целом, но на примере mundus этот феномен можно лицезреть в наиболее «чистом» виде.
Итак, множества – это своеобразные объединения абстрактных связей между объектами в некие совокупности этих самых связей. Кантор «почувствовал» (выявил, создал) возможность установления подобной связи во время работы над тригонометрическими рядами, в процессе которой он ввёл понятие предельной точки [2 с].  Попытавшись классифицировать точки сходимости рядов, он стал замечать, что если ввести их некоторые объединения, то станет актуальным вопрос о некой размерности этих объединений. Отсюда Кантор пришёл к идее о вычислении мощности – то есть именно соотношения размерности различных объединений. Заинтересовавшись этим, Кантор пишет и публикует работы о счётности (то есть о возможности индексации натуральными числами) рационального ряда, о равномощности натурального и рационального рядов, а также о не равной мощности множества натуральных и рациональных чисел с одной стороны и вещественных чисел – с другой [с на счётность]. Позже он также публикует доказательство о равной мощности множества вещественных чисел и этого же множества возведённого в n-ю степень, то есть по сути доказывает невозможность увеличения мощности континуума за счёт увеличения количества размерностей.
Ввиду высочайшей значимости данного понятия для теории множеств, на мощности следует немного задержаться. В сущности, мощность – это размер множества или, как обозначают в контексте самой теории – кардинальное число множества [с]. Для конечных множеств мощность всегда равна количеству содержащихся в нём элементов. Для бесконечных же – ситуация несколько иная. В этом случае мы уже имеем дело с, так называемой, иерархией алефов [с]. Алеф – это первая буква еврейского алфавита и именно её выбрал Кантор для обозначения мощности актуально бесконечных множеств. Поясним, причём же тут иерархия. Собственно, в контексте теории множеств приняты различные «бесконечные мощности», то есть по сути «разные бесконечности». Именно актуальные и непосредственно количественные – и различные. То есть, если сказать, «одна бесконечность больше другой бесконечности», то это будет абсолютно корректным высказыванием в рамках теории множеств. Кантором было доказано, что наименьшую мощность имеет актуально бесконечное множество всех натуральных чисел и равномощные ему множества (например, множество всех рациональных чисел) и называется эта мощность алеф 0. Множество же всех вещественных чисел имеет мощность 2^алеф 0. Следует заметить, что мощность множества всех вещественных чисел не определяется, как алеф 1, так как этот вопрос – про наличие или отсутствие промежуточных мощностей между алеф 0 и 2^алеф 0 – называется континуум-гипотезой Кантора, которая до сих пор не доказана [c на конт гипот]. Гёдель позже доказал, что в контексте аксиоматики Цермело-Френкеля отрицание этой гипотезы недоказуемо, а чуть позже Коэн доказал, что доказательство этой гипотезы в этой же аксиоматике также невозможно [3 с]. По итогу – вопрос остаётся открытым. Кардинальные числа или кардиналы не следует путать с порядковыми числами или ординалами, так как в контексте теории множеств они имеют разное значение. И если первые означают мощность множества, то вторые – порядковый тип, что может представляться несколько сложнее для понимания, но на самом деле – это по сути некоторое подобие индекса элемента в упорядоченном множестве. И да, ординалы, конечно, также бывают бесконечные. Более того, Кантор указывал, что не существует максимального ординала.
Арифметика кардиналов – это одно из «белых пятен» теории множеств. И проблема здесь также, как уже повелось, в той самой бесконечности. То есть, если с кардиналами конечных множеств всё более или менее понятно, то с множествами актуально бесконечными – ситуация вовсе иная. Также, при редуцировании множеств к кардинальным числам, с конечными множествами всё остаётся так или иначе в рамках понимания, если принимается аксиома выбора. Однако с бесконечными множествами имеет место наличие несоответствий. К примеру, редуцируем множество всех натуральных чисел к мощности этого множества, то есть к «алеф 0», который мы определим, как Х и мощность множества всех чётных чисел также к его мощности, то есть вновь к «алеф 0», который определим, как Y. Затем попробуем совершить арифметические операции с этими значениями. Допустим, «Х + Y» равно «алеф 0», так как подобная мощность не может стать ещё больше, то «Х – Y» – сколько будет? Если бы мы, например, отнимали мощность множества всех натуральных чисел от мощности множества всех натуральных чисел, то мы бы логичным образом получили 0, но если мы отнимаем мощность множества всех чётных чисел от мощности множества всех натуральных чисел, то мы, вновь логичным образом, получаем мощность множества всех нечётных чисел, которая равна мощности множества всех натуральных чисел – то есть мощность не изменилась вообще. Подобные примеры призваны продемонстрировать, что в контексте «оперирования бесконечностями» парадоксы и «нелогичные» в некотором роде результаты являются обыденным делом, что может свидетельствовать как в пользу необходимости более тщательной проработки понятийного аппарата теории, так и в пользу того, что «природа неохотно раскрывает свои тайны». Ну и, как ещё один вариант, можно попытаться предположить, что классическая арифметика вовсе не пригодна для операций с бесконечностями, в данном случае – на примере работы с алефами.
Операции же с самими множествами, такие как включение, объединение и пересечение описывает в 1877 году Дедекинд – друг и сподвижник Кантора. Позже будет установлено, что эти операции со множествами эквивалентны операциям в контексте классической логики, а соответственно – будет сделан вывод о том, что между теорией множеств и математической логикой наличествует глубокая связь.
Вскоре после этого, уже самим Кантором, вводятся понятия пустого множества и метод трансфинитной индукции – одни из наиболее фундаментальных по значимости концептов теории множеств.
Пустое множество – это множество, в котором нет ни одного элемента, его мощность равна нулю, а также оно является своим подмножеством, но не является своим собственным элементом (иначе это сразу привело бы к бесконечной рекурсии, конечно). На наличии пустого множества базируется аксиома бесконечности в теории множеств, то есть возможность построить бесконечное множество. Собственно, следует заметить, что аналогия с нулём из натурального ряда (или из ряда целых чисел, для тех случаев, где нуль не считается натуральным числом) здесь крайне целесообразна, так как иначе попытка интуитивно понять, что же собой представляет пустое множество – может, в общем-то, ничем не закончиться. Ну, а нуль представить ведь не должно стать проблемой. Или всё же должно? Ну, по крайней мере к самому непосредственно нулю из десятичной системы счисления, как цифре и числу – большинство людей привыкшие.
Существование нуля не следует из аксиоматики Пеано, так как там в качестве аксиомы полагается существование числа 1, а существование пустого множества – как раз-таки следует из аксиоматики Цермело-Френкеля. Собственно, отсутствие оснований для наличия числа 1, то есть введение его в контексте аксиоматики не как обоснованно существующего, а как существующего безусловно – это скорее про веру и удобство, но не про действительную основу существования. То есть аксиоматика Пеано не содержит обоснования существования числа 1, но вся система аксиом строится на его наличии. В этом смысле нам представляется куда более целесообразным выводить существование 1 из существования нуля, а затем уже развивать аксиоматику. В теории множеств, как нам представляется, с этим поступили гораздо лучше, и именно на пустом множестве базируется возможность построения неограниченно больших и бесконечных множеств.
Нуль, в качестве именно формульного символа, то есть цифры, играет весьма значимую роль в нашей десятичной системе счисления. К слову сказать, его добавил к стандартным девяти индийским цифрам Аль-Хорезми – великий арабский учёный, которому мы обязаны также понятиями алгоритм и алгебра [с]. И именно ему показалось, что нуль поможет корректно формулировать большие числа – и так оно и есть на самом деле. Но вот именно в качестве некоей абстрактной связи между объектами – нуль скорее репрезентирует её полное отсутствие. Казалось бы, как именно это должно восприниматься и зачем вообще это хоть как-то обозначать – отсутствие всех наличий. В этом смысле с пустым множеством ещё сложнее – оно несёт на себе смысл отсутствия даже самой возможности возникновения хоть какой бы то ни было связи. Как уже говорилось выше, на основе нуля в арифметике (в наиболее конструктивных подходах в контексте арифметики – точнее) строится натуральный ряд – вплоть до бесконечности, на основе пустого множества в теории множеств строятся бесконечные множества – до актуального верха иерархии алефов. В этом смысле ответ на вопрос «может ли из ничего возникнуть что-то?» – закрыт. Позже мы ещё вернёмся к данному вопросу так как он представляется нам гораздо более глубоким, чем подразумевает контекст текущего изложения.
Трансфинитная индукция же означает экстраполяцию результатов, полученных при работе с конечными математическими объектами – на объекты, соответственно, бесконечные. И да, конечно же, на именно актуально бесконечные и несчётные. Правомерность трансфинитной индукции вообще представляет собой, конечно же, открытый вопрос. Но не в контексте теории множеств – тут она принимается, разумеется, безоговорочно. Иначе – «всё перестаёт работать». Здесь следует отдельным образом заметить, что трансфинитная индукция видится нам уникальным явлением в mundus.
Поясним приведённое утверждение. Для начала скажем, что как уже упоминалось, математика, в противовес той же физике, не пытается описывать саму реальность в её непосредственном виде. Вместо этого она строит некую «карту реальности», её идеализированную модель, которая в тех или иных аспектах берёт начало в реальности, далее продолжаясь уже без явных пересечений. Это то, тот мир, который мы определили в качестве mundus. Высочайшая точность предсказаний, которые являются результатами вычислений и расчётов свидетельствует о том, что на основе абстрактных связей и закономерностей mundus и при помощи законов вывода, можно утвердительно и конструктивно определять специфические особенности реальности уже объективной, вещественной и выражать их в количественном виде. Это показывает, насколько порой целесообразным и продуктивным является само абстрагирование, как метод научного поиска. Принцип простой – если есть связь в mundus, то она должна быть и в объективной данности, причём именно такая-то и именно с такими-то параметрами. Успехи применения математического аппарата в космологии, в виде расчёта движения небесных тел для прогнозирования, со времён Древней Греции, через крайне значимый в данном контексте 19-й век и вплоть до нашего времени – убедительно доказывают, что законы mundus можно экстраполировать на реальный мир в том или ином виде.
Фигурально выражаясь, войдя из реального мира в mundus в одной локации и пройдя непосредственно его – mundus – тропами, можно выйти в реальном мире в локации вовсе иной – вычисленной и смоделированной, чтобы затем, вернувшись в мир реального, утверждать, что при таких-то условиях будет получен такой-то результат. В этом и заключается, собственно, вся суть математики, согласно нашему абстрактному виденью.
И вот в данном контексте трансфинитная индукция предстаёт несколько выбивающейся из общего фона. В самом деле, если же мы говорим о том, что результаты математики, пусть даже промежуточные результаты, должны хоть каким-то образом говорить нам нечто о мире реальном, то есть, если уж мы идём «тропами mundus», то мы в любом случае хотим рано или поздно «выйти в реальности», а не просто всё дальше и дальше от неё уходить в неизвестном и неизведанном направлении. Так вот вопрос: какому именно аспекту реальности может, хотя бы отдалённо и пусть даже чисто теоретически, даже пусть «очень потенциально» соответствовать «путь трансфинитной индукции»? И данный вопрос представляется весьма и весьма актуальным, особенно учитывая, что, как мы уже упоминали, сам Гильберт, когда настаивал на целесообразности «пропуска в mundus» абстракции актуальной бесконечности, сам же и утверждал, что «актуальной бесконечности в реальном мире существовать не может». А трансфинитная индукция «нужна» только для неё – только для актуальной бесконечности. То есть, в случае с трансфинитной индукцией мы имеем дело с методом исследования непосредственно mundus – вообще без намёков на связь с реальностью. И мы в данном контексте не хотим сказать, что трансфинитная индукция не является целесообразной – перспективы покрыты туманом, но это не значит, что за туманом нет Истины. Как мы указывали ранее, в истории математики уже бывали примеры, когда подобное заканчивалось очень даже конструктивным образом. Мы же всего лишь хотим указать на то, что этот метод изначально формировался для изучения именно самого mundus, а не для исследования объективной данности – и вот в этом он, пусть и не уникален, но уж точно несколько подобен уже бывшим в подобном положении отрицательным, иррациональным и комплексным числам. А вот в чём уже он уникален, так это в том, что, во-первых, он специально таковым и создавался – именно для изучения особенностей «абстрактного ландшафта» mundus. В чём его отличие от, к примеру, упомянутых иррациональных, отрицательных и комплексных чисел, которые просто последовательно сформировались из «абстрактных осколков». А также, что во-вторых, трансфинитная индукция представляет собой не просто повышение уровня абстракций, а будто бы уже «возведение одной абстракции в степень другой абстракции», так как это, по сути, не имеющая явной связи с реальным миром абстрактная надстройка над ещё одной абстракцией, не имеющей явной связи с реальностью – над актуальной бесконечностью.
К вопросу о том, имеет ли смысл подобный исследовательский феномен с точки зрения целесообразности, мы бы подходили достаточно осторожно. Конечно, представляется весьма заманчивой перспектива «впихнуть» mundus в прокрустово ложе классической механики и впустить в «рай Кантора» «демона Лапласа». Однако, как показывают научные достижения последнего времени, мы не можем, подобно математикам 17-18 веков, быть абсолютно уверены в высокой мере точности, с которой наши теории и воззрения отражают «замысел Творца». Плюрализм и несоответствие различных подходов как к самому описанию реальности, так и к методологии и моделям этого описания – в определённом смысле «Кант лежит под «гильотиной Юма»» [c]. То, что ещё вчера казалось истинным и правильным, сегодня предстаёт наивным и устаревшим – мир меняется и меняется очень быстро. Ещё в начале нашего века мысль о том, что «идея думающей машины есть сведение духа к абсурду» – казалась вполне себе разумной, а теперь, спустя всего два десятилетия – когда мы уже на данном этапе со всех сторон окружены этими самыми «думающими машинами», то абсурдом представляется уже уверенность в завтрашнем дне [Лолли]. И точно так же в контексте развития научных дисциплин вообще и математики – в частности. Сложно не согласится с Пуанкаре, который постановил, что «Лучший метод для предвидения будущего развития математических наук заключается в изучении истории и нынешнего состояния этих наук» [Пуанкаре А. О науке. М.: Наука. 1990]. Поэтому нам представляется не особенно конструктивным отбрасывать и признавать негодными перспективные абстракции из mundus просто потому, что между ними и объективной реальностью мы на данный момент не можем установить прямой связи. Скорее подобные абстракции целесообразно, что называется «испытывать на прочность» с точки зрения логики, на предмет гармоничного сочетания с абстракциями прочими, и, конечно же, с философско-методологических позиций.
Далее, следует отметить понятия совершенного и несовершенного множеств. Совершенное множество – это замкнутое множество, совпадающее со множеством своих предельных точек. То есть это такое множество, в котором для каждой точки этого множества в её окрестности есть такая точка, которая не совпадает с ней, а как бы «приближается». Несовершенное множество – это, соответственно, множество, для которого вышеприведённое правило не выполняется. Ранее мы приводили пример с множеством вещественных чисел на открытом интервале (0, 1). Так вот это множество не является совершенным, так как не содержит всех своих предельных точек – в данном случае 0 и 1. А, соответственно, множество, построенное на закрытом интервале [0, 1] – уже будет являться совершенным. Теперь скажем про несколько пересекающиеся понятия плотного и неплотного множества. Плотное множество – это некоторое множество «Х», для которого между любым его элементом «х» и некоторым элементом «y» некоторого иного множества «Y», расстояние меньше, чем некий эпсилон, где эпсилон – любое положительное число. Неплотное множество – это множество «Х», для какого-либо элемента «х» которого, нет такого элемента «y» в ином множестве «Y», расстояние между которыми было бы меньше некоторого эпсилона. Типичный пример для данного случая – это множество рациональных чисел в множестве чисел вещественных, которое является плотным. В этом случае говорят, что множество Х является плотным в множестве Y. Для нас, в контексте текущего исследования, все эти понятия представляют значимость в первую очередь как основа для рассмотрения ещё одного понятия, а именно – Канторова множества, которое являет собой пример нигде не плотного совершенного множества.
Итак, Канторово множество [c]. Опишем метод его построения. Возьмём множество вещественных чисел на закрытом интервале [0, 1] и удалим из него среднюю треть, то есть множество всех элементов на открытом интервале (1/3, 2/3) и получим множество, состоящее из двух множеств на интервалах [1, 1/3] и [2/3, 1]; затем применим рекурсивно первый шаг к каждому из имеющихся на данный множеств и получим разделение каждого ещё на два; и так далее. В итоге мы получим некую структуру, состоящую из бесконечного количества точек, но при этом – без интервалов и имеющее нулевую меру Лебега [c]. Для нас Канторово множество является весьма значимой и показательной структурой, но на данный момент мы хотим просто её охарактеризовать в контексте текущего изложения, с тем чтобы позже к ней вернуться.
Мы охарактеризовали многие ключевые понятия «наивной» теории множеств, и это, по нашему мнению, может представлять собой более или менее достаточный базис для общего понимания того, с чем мы имеем дело вообще. И на момент окончания 19-ого века теория множеств основывалась на том материале, который мы уже предоставили (не только на нём конечно, но всё же это была существенная часть). А вот затем начали происходить парадоксы, которые привели к основательному пересмотру всего здания теории.

2.4. ПАРАДОКСЫ ТЕОРИИ МНОЖЕСТВ И КРИЗИС ОСНОВАНИЙ МАТЕМАТИКИ

Теория множеств в своё время стала тем самым концептуальным фундаментом, на котором могла быть объединена и, собственно, в какой-то весьма немалой степени объединилась вся математика. Причина этого заключается в том, о чём мы уже говорили выше – наиболее ценными и значимыми в mundus будут считаться те абстракции, которые способны «связать собой» абстракции иные – более низкоуровневые. Вообще на самом деле помыслить нечто более общее, чем множество – само по себе задача не тривиальная. Конечно, далее в истории математики появилась теория категорий, о которой также уже говорилось, и в её рамках было показано, что множества могут быть «абстрактно связаны» в качестве одной из категорий – категории set, где объектами являются множества, а морфизмами – их отображения. Однако в самом конце 19-ого века и начале 20-ого теория множеств казалась наиболее общей совокупностью инструментария, который только можно себе вообразить. И так как её практические прикладные успехи также были весьма значительны, то начало возникновения проблем сразу представлялось математикам чем-то эфемерным, не особенно значительным и потенциально легко преодолеваемым. Однако, всё оказалось не так просто.
Вообще существуют сведения о том, что первый парадокс обнаружил ещё сам Кантор в 1895-ом году, однако позже этот парадокс был определён под именем открывшего его ещё раз Чезаре Бурали-Форти в 1897-ом году [c]. Хотя, порой он также обозначается и как парадокс Кантора, ввиду общей формальной схожести обоих интерпретаций и, по сути, их явной взаимообусловленности – этими факторами детерминируется некоторая историческая путаница. Рассмотрим, для начала, их отдельно, возможно несколько нарушив хронологию – но, ввиду имеющей место быть путаницы – это не точно.
Итак, парадокс Кантора, впервые описанный им в письме Дедекинду в 1899-ом году, однако, по некоторым сведениям, замеченный им ещё в 1895-ом году. Суть парадокса заключается в следующем. Представим себе некое множество всех множеств «Х». Кантор определял подобное множество как универсум, апеллируя, по всей видимости, к значимости данной конструкции в качестве некоего единства всего в одном. То есть, это такое множество «Х», что для любого множества «Y», это «Y» принадлежит «Х». Это будет означать, что мощность множества «Х» – представляет собой максимально возможное кардинальное число. Однако, вследствие того, что множество всех подмножеств для любого множества имеет мощность 2 в степени мощности исходного множества, то мы сразу получаем противоречие. Ведь это означает, что наше изначальное предположение о том, что мощность множества всех множеств является максимальным кардинальным числом – ошибочно, ведь максимальным кардинальным числом должно будет обладать множество всех подмножеств для множества всех множеств «Х». А значит та теория, в контексте которой вообще возможно вывести подобное заключение – ошибочна также.
Сам Кантор, при попытке разрешения тех парадоксов, логических антиномий и общих проблем, которые возникали в контексте теории множеств, прибегал к чистой метафизике – безо всякой опоры на непосредственно строгое научное знание, которое резюмировалось в математике. К примеру, для «сглаживания» явных противоречий касаемо универсума, Кантор предложил демаркацию на множества, как непосредственно математические объекты и некие «многообразия», которые представляют собой, по всей видимости, что-то вроде Кантовских «вещей самих по себе» – нечто, что невозможно полностью охватить человеческим разумом. Причём это касалось не только ситуации с универсумом, а также, как мы ранее уже говорили, и понятия абсолютной бесконечности, и, о чём мы ещё не упоминали – демаркации также между вообще трансфинитностью и абсолютностью. Конечно, подобные попытки не особенно помогали в разрешении парадоксов и противоречий. Это бы могло сработать в некой иной сфере, такой как, к примеру, чистая метафизика или иные из подобной же категории. Но не в математике, которая в то время стремилась, по выражению Пуанкаре, к «абсолютной строгости». Поэтому того же Гильберта подобные попытки разрешения парадоксов теории множеств оставили неудовлетворённым и заставили идти «искать правды в другом месте», надеясь за счёт сведения математики и логики к символьным операциям не оставить внутрисистемным противоречиям никаких шансов. И это также оказалось не так просто, как, возможно, хотелось бы.
Хронологически первым зафиксированным был парадокс Бурали-Форти, «обнаруженный» Чезаре Бурали-Форти в 1897-ом году. О некоторой путанице с парадоксом Кантора мы уже сказали выше – повторять здесь не будем. Итак, парадокс Бурали-Форти относится к множеству всех порядковых чисел. Сначала вспомним, что такое вообще порядковые числа. Мы определили их как некие условные «индексы» упорядоченных множеств. Так вот, в рамках данного парадокса мы имеем дело с множеством всех порядковых чисел. Но, если мы предполагаем наличие множества всех порядковых чисел, то число, которое является порядковым числом такого множества должно быть больше или равно, чем все те числа, которые, собственно, и содержаться в этом самом множестве. А вот порядковое число булеана такого множества, то есть множества всех его подмножеств, должно быть равно 2 в степени мощности этого множества, а значит уже строго больше любого из чисел, содержащихся в множестве. И это, соответственно – невозможно, ведь, согласно условию, в множестве должны содержаться все порядковые числа, так что мы пришли к противоречию.
Как уже говорилось выше – парадокс Кантора и парадокс Бурали-Форти в общем-то тесно пересекаются по смыслу и форме.
Далее – парадокс Рассела. Он был «обнаружен» Бертраном Расселом в 1901-ом году и является несколько более сложным по форме, чем предыдущие. К тому же – с ним связана одна интересная история, в контекст которой также включён ещё один выдающийся учёный того времени, а именно Готлоб Фреге [c]. В тот момент, когда Фреге закончил 2-е издание своей книги «Основные законы арифметики» и она уже находилась в печати, он получил от Рассела письмо, в котором тот указывал, что «…испытал трудности только в одном месте. Вы утверждаете (стр. 17), что функция может сама выступать в качестве неизвестного. Раньше я тоже так считал. Но теперь такой взгляд мне кажется сомнительным из-за следующего противоречия. Пусть w предикат: «быть предикатом, который не приложим к самому себе». Может ли w быть приложим к самому себе? Из любого ответа следует обратное. Следовательно, мы должны заключить, что w — не предикат. Аналогично не существует класса (как целого) тех классов, которые, взятые как целое, не принадлежат себе. Отсюда я заключаю, что иногда определённое множество не формирует целостного образования» [c]. По итогу Фреге понял мысль Рассела и осознал то противоречие, на которое тот указывал, но был уже не в силах серьёзно исправить что-либо в своей книге. Поэтому он ограничился следующим к ней дополнением в опубликованной версии: «Вряд ли с учёным может приключиться что-нибудь худшее, чем если у него из-под ног выбьют почву в тот самый момент, когда он завершит свой труд. Именно в таком положении оказался я, получив письмо от Бертрана Рассела, когда моя работа уже была завершена» [c].
На самом деле Фреге и правда стоит посочувствовать – за последующие чуть более 20-ти лет оставшейся жизни он больше не выпустил ни одной книги, ограничиваясь лишь преподавательской деятельностью – события, подобные произошедшему с ним, действительно способны сильно «выбивать из колеи». В особенности же тех, кто использует то, что полагает непротиворечивыми «истинными» положениями и тогда, когда оказывается, что всё на самом деле «немного» иначе. В обоснование этого стоит привести высказывание самого Фреге, взятое из его другой книги «Основы арифметики: логически-математическое исследование о понятии числа» 1884-ого года, в которой тот говорит следующее «Обычно поступают так, будто принятие постулатов само по себе достаточно для того, чтобы все постулаты выполнились. Мы постулируем, что операция вычитания, деления или извлечения корня всегда выполнима, и считаем, что этого вполне достаточно. Но почему мы не постулируем, что через любые три точки можно провести прямую? Почему мы не постулируем, что все законы сложения и умножения остаются в силе для комплексных чисел с тремя единицами точно так же, как они выполняются для вещественных чисел? Это происходит потому, что такого рода постулаты содержат противоречие. Прекрасно! Но тогда первое, что нам необходимо сделать, – это доказать непротиворечивость наших остальных постулатов. А пока это не будет сделано, вся строгость, к которой мы так стремимся, останется столь же зыбкой и призрачной, как лунное сияние» [c]. Строгость и непротиворечивость – то, за что радел и к чему стремился Готлоб Фреге. В этом смысле фаталистическое высказывание о «коварстве судьбы» как будто приобретает некий определённый смысл. К чести самого Фреге также стоит отнести то, что он честно признал своё заблуждение и публично об этом заявил, не став ничего скрывать и вступать в заведомо обречённые полемические взаимодействия. Тем самым его поведение в данном ситуативном положении представляется воистину стоическим.
Теперь же – непосредственно сам парадокс Рассела [c]. Допустим у нас есть два типа множеств – обычные и необычные. Множество называется обычным, если оно не содержит само себя в качестве элемента и необычным – в противном случае. Теперь попробуем сформировать то, что было уже после названо Расселовским множеством – множество всех обычных множеств. И зададим вопрос: должно ли это множество содержать само себя? То есть, с одной стороны – должно, ведь оно является обычным множеством, а соответственно – должно содержать само себя; но, с другой стороны, ведь если оно будет содержать само себя, то оно будет уже необычным множеством и значит – оно не должно содержать само себя, ведь мы говорим именно о множестве всех обычных множеств, а Расселовское множество получилось необычным. И так, собственно, далее – в любом случае мы приходим к противоречию.
Сам Рассел в дальнейшем попытался избегать подобных парадоксов за счёт введения многочисленных и до известной степени искусственных ограничений в своей теории типов и разрабатываемом подходе – логицизме. Ни теория, ни сам подход в будущем не снискали особой популярности в большой степени за счёт излишней сложности и, как раз-таки множества ограничений. Здесь также целесообразно указать на то, что в письме Фреге Рассел, в контексте изложения сути открытого им парадокса, использовал несколько иной понятийный аппарат и выстраивал логическую структуру на основе понятий функций и предикатов. Также здесь «просвечивает» то, что и парадоксы Кантора и Бурали-Форти являются тесно связанными по смыслу – нет особой разницы, что именно использовать для построения логической структуры парадокса: сами множества или их некие «отображения» и «представления» в виде кардиналов и ординалов. На основе этого само собой напрашивается предположение о том, что в основе всех представленных парадоксов лежит какой-то единый «механизм».
Далее скажем ещё об одном интересном парадоксе, который несколько яснее, чем прочие указывает на то самое «место», откуда у них у всех «ноги растут».
Наш очередной пример называется парадокс Тристама Шенди и предложен он также Бертраном Расселов в его книге «Мистицизм и логика» [c]. В контексте данного парадокса персонаж Рассела – Тристам Шенди, жалуется на то, что чтобы подробно описать первый день своей биографии ему понадобился целый год и далее задаётся вопросом о том, что как же ему успеть записать свою биографию, если на описание одного дня уходит целый год. В этом месте изложения Рассел утверждает, что если бы жизнь данного персонажа была бесконечной, то «ни один день из его жизни не остался бы незаписанным».
По сути – это прямое указание на то, что уже упомянутое нами ранее правило «целое больше части» – не работает для случаев с актуально бесконечными множествами. Это мы уже ранее показывали и на иных, принятых в рамках теории множеств, примерах: про равномощность множеств всех натуральных чисел и множества всех полных квадратов, равномощность множества всех вещественных чисел и его подмножества, образованного на интервале между любым его элементом и этим же элементом, разделённым на 2 и прочие. В случае же с парадоксом Тристама Шенди ровно то же самое – при введении в контекст бесконечности целое становится вовсе не обязательно больше части, а сама часть будто бы «расширяется» до размеров целого. И при подобном подходе единственное, что гарантированно больше любого множества – это множество всех его подмножеств, да и то – для некоторых множеств, как видно из примеров выше, оно также может иметь некую неопределённую природу. Хотя, к этому моменту мы ещё вернёмся позже. Но вот здесь и сейчас – почему возможны подобные парадоксы?
На этот вопрос можно сформулировать различные ответы, однако нам всё же представляется наиболее корректным редукция проблемы ко всё той же бесконечности – как только в рамках теории множеств употребляется определительное местоимение «всех» по отношению к каким-либо элементам некой совокупности, то мы имеем дело с актуальной бесконечностью со всеми (вот здесь тоже, например) из этого вытекающими последствиями. То есть, является совершенно понятным, что ни один (а здесь отсылка к пустому множеству) из вышеупомянутых парадоксов в принципе невозможен в реальном мире. Их «вотчина» – непосредственно mundus, причём именно «рядом» с актуальной бесконечностью. В рамках этой «локации» возможны такие феномены, которые по определению не способны хоть каким-то малейшим образом «принадлежать» также и реальному миру, а не только mundus. Ранее мы говорили о том, что законы логического вывода в какой-то мере «оберегают» mundus от возникновения на его субстрате противоречий. Но когда именно сама же логика помогает в формировании логических парадоксов – это оказалось серьёзным ударом для математики.
Опять же, в каком-то смысле логика «сработала» и непосредственно указала на те противоречия, которые следовали из некоторых концептуальных особенностей теории множеств. С другой же стороны – ранее между теорией множеств и логикой же были установлены глубокие ассоциативные связи и показана некоторая эквивалентность логических операций и операций со множествами. Следовать далее по этой дедуктивной цепочке и прямо указывать на потенциальную возможность глубоких противоречий в самой логике, мы на данный момент полагаем всё же излишним, хотя это предположение в данном контексте и не выглядит контринтуитивным, а напротив – смотрится конгруэнтным логическим шагом. Здесь же только заметим, что ещё со времён Зенона при введении в контекст рассмотрения некой проблематики понятия актуальной бесконечности – проблемы и парадоксы не заставляли себя ждать.
Вообще говоря, приведённые здесь для примера парадоксы вполне могут напоминать некоторые фрагменты нашего предыдущего изложения, той его части, которая как раз-таки и касалась осмысления понятий актуальной и потенциальной бесконечности. В частности, мы показали, каким образом формируется парадокс на основе понятий о времени, движении и бесконечности, а также на фундаменте апории Зенона о стреле. Здесь дополним анонсом о том, что позже, в ходе нашего исследования, мы вернёмся к данной проблеме и покажем каким именно образом и на основе какого механизма формируются все эти парадоксы и даже, собственно, не только они.
Теперь же укажем лишь на то, что и далее в истории математики также обнаруживались парадоксы, такие как, например, парадокс Банаха-Тарского применительно к аксиоме выбора, парадокс Мириманова – как обобщение парадокса Бурали-Форти, парадокс Ришара, парадокс «интересных чисел» и прочие [cссс]. Каждый из них вносил свою лепту в, и без того уже выглядящий довольно зыбким, логико-математический фундамент. Но положили начало самому кризису оснований математики именно упомянутые и описанные нами выше парадоксы. После того, как они были сформулированы, математическое сообщество, в основном та его часть, которая поддерживала Давида Гильберта и продвигаемую им парадигму формализма, стала рассчитывать на конструктивное разрешение возникших противоречий путём построения прочного, полного и непротиворечивого формального основания теории множеств. Другая, несколько меньшая часть, представленная Лёйтзеном Брауэром и интуиционистами, в виде некоей реакции на ситуацию решила отказать от «всего подозрительного» и подвергла основательному пересмотру основания математики и математическую логику. Про Рассела и его логицизм с теорией типов мы уже упомянули выше. Теоретико-множественный же подход, возглавляемый к тому моменту как сам Кантор уже отошёл от дел, Эрнстом Цермело пошёл по пути чистой аксиоматики, что будет нами освящено в следующей части.
В дальнейшем же стало понятно, что чаяниям Гильберта не суждено было сбыться, ибо кризис оснований математики резюмируется доказательством Курта Гёделя его теоремы о неполноте. И об этом мы позже скажем отдельно. Теперь же обратимся к тем основаниям, на которых всё же была в дальнейшем перестроена наивная теория множеств и сформирована её более или менее «классическая» версия.

2.5. АКСИОМАТИКА ТЕОРИИ МНОЖЕСТВ

После того, как были обнаружены парадоксы, которые вскрыли глубокие внутренние противоречия теории множеств, математическому сообществу потребовались строгие формальные основания и чёткие определения того, с чем они работали – уже нельзя было слепо верить не только в то, что теория множеств истинна, но также и в то, что истинна сама математика. В общем – требовалась серьёзная аксиоматизация и в первую очередь – аксиоматизация теории множеств.
Здесь мы полагаем, что для начала целесообразно разобраться с тем, что вообще такое аксиомы и аксиоматические системы. Иначе дальнейшее изложение рискует стать малопонятным. Итак, аксиомы трактуются, как некие высказывания, которые не требуют доказательств. И в этом, собственно, их главное отличие от теорем. Теорема – это то, что выводится из аксиом и/или из ранее доказанных теорем и далее доказывается при помощи принятых правил логического вывода.
Сами эти правила логического вывода, стоит заметить, отнюдь не являются некими абсолютными высшими истинами или, так сказать, «мыслями Бога», а могут различаться в зависимости от той формальной системы, для которой выводится некое доказательство. К примеру, в контексте интуиционистской логики закон исключённого третьего не принимается в качестве валидного правила вывода доказательств, хотя во всех остальных «логиках» – он считается одним из главных, наряду с законом противоречия, двойного отрицания, тождества и прочими; а, допустим, в тернарной логике Лукасевича наряду с истиной и ложью – как фундаментальными значениями и определяющими качествами тех или иных высказываний, принято также третье значение – «unknown», что означает неизвестность и невозможность однозначно определить некое высказывание в качестве истинного или ложного [c]. Также, тоже для примера, в нечёткой и вероятностной логиках значение «истинности» или «ложности» того или иного высказывания определяется функцией, заданной на закрытом диапазоне вещественных чисел между 0 и 1, где 0 – означает ложь, 1 – истину, а все остальные свидетельствуют об аппроксимации «полной истинности» и/или «полной ложности» [c]. А вообще различных «логик», кроме уже нами упомянутых, существует достаточно большое количество – достаточно большое для того, чтобы ни одну из них не полагать несомненно верной и единственно истинной: некоммутативня, немонотонная, квантовая, линейная, темпоральная (из нововведений) и прочие [c]. На всякий случай заметим, что мы в текущем исследовании и формировании наших собственных выводов и заключений, будем всё-таки придерживаться классического варианта бинарной логики с законом исключённого третьего. Главное, что необходимо здесь уяснить: логические особенности в данном контексте вообще важны в первую очередь в том смысле, что ими определяется внутренняя непротиворечивость любой аксиоматической системы и они используются для доказательства теорем, основанных на аксиомах.
Итак, аксиомы – это некие утверждения, не нуждающиеся в доказательствах, а напротив – представляющие собой основу для вывода доказательств. При изначальном знакомстве с аксиомами вообще – феноменологически – сразу же напрашивается вполне закономерный вопрос: а почему это аксиомы не требуют доказательств? Разве они являются некой неоспоримой высшей истиной? Может быть где-то утверждается, что аксиомы были «переданы» людям таким же образом как 10 заповедей Моисею на горе Синай [c]? На самом деле всё не совсем так. Аксиомы, в сущности, совершенно не обязаны даже быть интуитивно «правильными». К примеру, можно взять первую аксиому Евклида: от всякой точки до всякой точки можно провести прямую линию [c]. Затем возьмём доску и отметим точку с одной стороны доски и другую точку – с противоположной стороны доски. Затем простым карандашом проведём между ними линию, следуя по доске. Далее посмотрим, что у нас получилось. А получилась у нас линия, которая огибает доску по краю и которая, как мы видим собственными глазами – не «прямая». Иначе соединить точки при таком их положении на доске мы не можем. Получается, что неверна уже первая аксиома евклидовой геометрии? Ведь мы не можем соединить точки «прямой» линией. В геометрический смысл этой гипотетической ситуации здесь и сейчас не станем глубоко вникать, ведь мы хотим показать совсем иное. А именно то, что значение аксиом не в том, что они – истинны. В сущности, никто не мешает им быть откровенно «ложными» и при этом не переставать быть, собственно, аксиомами. Их смысл вовсе в ином. В том, что они служат в качестве непротиворечивой основы для некой формальной системы и задают её контекст. То есть мы можем, условно говоря, сформировать некое множество базовых необоснованных высказываний, множество правил логического вывода также любой произвольной природы, искусственно создать какой угодно алфавит в виде совокупности символов и на этой основе сформировать множество правильно построенных высказываний – и то, что мы только что описали и будет характеризоваться как непосредственно формальная система. Таково в общем смысле логико-математическое определение, по крайней мере. Ещё раз, аксиомы не требуют доказательств не потому, что они истинны абсолютно, а потому, что они истинны конкретно для данной формальной системы и истинны они для неё только потому, что она же на них и построена.
Таким образом, предварительно подытожим. Аксиомы – это некие высказывания, которые определяют контекст формальной системы и служат основой для любых доказательств в рамках этой системы. Общая целесообразность их наличия сводится к тому, что в рамках точной науки мы не можем «слепо верить» во что-либо. То, что принимается как неотъемлемая часть научного знания, должно быть соответствующим образом доказано. Однако, если мы станем требовать строгого доказательства для вообще любого вывода и утверждения, то это вполне логичным образом приведёт к необходимости бесконечного формирования доказательств. И это поняли уже древние греки, которым и принадлежит «изобретение» аксиоматики. Также отсюда следует и, как указывал ещё Аристотель, необходимость наличия неопределяемых понятий в контексте аксиоматики. Что же такое неопределяемые понятия? Это, можно сказать, субъекты без предикатов. То есть некие «наличия», о сущности, ключевых свойствах и качествах, неких внутренних характеристиках которых ничего прямо не утверждается. Всё, что мы можем сказать об этих самых «наличиях» – должно следовать из самих аксиом. Такими понятиями, к примеру, у Евклида в его «Началах» служат «точка», «прямая», «угол», «круг». О них самих непосредственно – ничего не говорится. Указывается только на то, как они себя «ведут» в тех или иных «ситуациях», заданных аксиоматически.
Для чего именно служат неопределяемые понятия? Их значение мы бы охарактеризовали как дуальное: во-первых, они необходимы для того, чтобы нивелировать неизбежность определения всех понятий без исключения, иначе это также привело бы к необходимости бесконечных определений одних понятий через другие; во-вторых, неопределяемые понятия формируют потенциальную возможность для большей гибкости и «смысловой широты» формальной системы, иными словами – для более высокого уровня абстрактности системы. В эту тему высказывался ещё Гильберт, полагая, что «Следует добиться того, чтобы с равным успехом можно было говорить вместо точек, прямых и плоскостей о столах, стульях и пивных кружках» [c]. То есть, в идеале является совершенно неважным то, чем же именно являются неопределяемые понятия, главное – чтобы из аксиоматики было понятно «на что они способны», «чего мы можем от них ожидать» и «как ими пользоваться». И в данном случае мы вновь сталкиваемся с той самой особенностью математики, о которой говорили ранее – о том, что в контексте этой дисциплины (но не только сугубо её, конечно) наиболее значимыми понятиями будут являться те, которые позволяют «связать собой» понятия другие – менее абстрактной природы. И сами множества стали настолько значимым явлением в математике только потому, что на основе понятийного аппарата теории множеств была получена возможность описать с точки зрения единой терминологии подавляющее большинство всех остальных подходов, теорий и направлений.
Также одним из ключевых критериев «качественной» аксиоматической системы является некоторая «изолированность» каждой отдельно взятой аксиомы. Смысл конкретно этого пункта весьма прост и понятен: если мы можем вывести одну аксиому из других аксиом при использовании принятых правил вывода, то это уже не аксиома, а теорема и в контексте именно аксиоматики она совершенно точно является избыточной. Если бы было можно выстроить всю систему на основе одной единственной аксиомы – это было бы «хорошо». Конечно, в действительности так обычно не случается и аксиом требуется всё-таки несколько больше: для евклидовой геометрии – 5, для системы геометрии, описанной Гильбертом – 20 и ещё одна не входящая в канон; аксиоматика Цермело-Френкеля для теории множеств – 5 основных и несколько дополнительных (точное количество варьируется в зависимости от подхода); и так далее.
Единственным же фундаментальным требованием к аксиоматической системе является только одно – внутренняя непротиворечивость. То есть, является строго недопустимым, чтобы из одной и той же аксиоматики и при помощи одних и тех правил вывода можно было вывести как некое утверждение, так и его противоположность – нельзя, чтобы одна аксиома утверждала некий «Х», в то время как другая утверждает «не Х». И для аксиоматических систем истиной является именно это – непротиворечивость. Конечно, теоремы Гёделя о неполноте и противоречивости сказалась на специфике построения аксиоматик, но так как иного столь же целесообразного критерия, кроме непротиворечивости, найдено не было – на неполноту и противоречивость любой достаточно богатой формальной системы просто не обращают внимания, так как в любом случае «нет вариантов».
К слову сказать, здесь также наличествует очень интересный момент, который касается специфики определения непротиворечивости формальных систем. В контексте математики это работает следующим образом. Зачастую о некой системе просто говорят «система непротиворечива». Конечно же, вследствие доказательства Гёделя, всем известно, что именно полнота и непротиворечивость невозможны в синтезе для любой формальной системы с определённым символизмом, операциями сложения и умножения. Но всё же зачастую о достаточно сложных формальных системах говорят «непротиворечива»: система аксиом геометрии Гильберта, система комплексных чисел и прочие. И всегда, если приступить к разбору столь интересной ситуации о непротиворечивости – находятся соответствующие нюансы. Оказывается, что «система непротиворечива» – это только первая часть высказывания. А на самом деле высказывание является импликативным, и вторая его часть, которая «записана мелким шрифтом» или вообще опущена, всегда несёт в себе опору на некоторое условие. Для системы аксиом геометрии Гильберта вторая часть звучит, как «…при условии непротиворечивости арифметики». Интересным является то, что как раз-таки противоречивость арифметики и следует из доказательства Гёделя. Для системы комплексных чисел вторая часть высказывания звучит, как «…при условии непротиворечивости системы вещественных чисел». А является общеизвестным в сфере математики, что непротиворечивость системы вещественных чисел не является доказанной.
Таким образом, общий смысл этих высказываний о непротиворечивости тех или иных формальных систем, зачастую сводится к формированию некой «видимости» и «кажимости» непротиворечивости, которая на самом деле является обусловленной допущением о непротиворечивости некой иной системы, которая уже практически всегда оказывается как раз-таки противоречивой по определению. Примерно таким образом математическое сообщество на самом деле, в действительности – внутренне, отреагировало на доказательство теоремы Гёделя о неполноте – просто «приняв к сведению» и несколько изменив определение непротиворечивости применительно к формальным системам. Конечно, здесь же нельзя не признать, что практический смысл в этом всё же есть – не особенно приятно знать, что занимаешься принципиально «ложным» и «неправильным» делом, да ещё и постоянно «держать перед глазами» прямое на это указание; а что же касается прикладных аспектов математики, то, по большому счёту, их теоремы Гёделя не касаются вообще никак – как в 19-ом веке успешно вычисляли то, что актуально необходимо вычислить, так и в наше время – и даже гораздо быстрее и куда точнее, чем раньше – за счёт использования вычислительной техники и, конечно, при помощи алгоритмической оптимизации, которая также на месте не стоит и активно развивается в актуальную эпоху.
Несколько концептуально очертив общий вид аксиом и совсем немного затронув теоремы, перейдём непосредственно к аксиоматике теории множеств. Изначальный вид аксиоматической теории множеств придал Эрнст Цермело в 1908-ом году [c]. Он предложил аксиоматику теории множеств, которая стала основой для дальнейшей аксиоматизации в контексте теоретико-множественного подхода в математике. Доработанная версия этой аксиоматики, осуществлённая в 1921-ом году Абрахамом Френкелем получила название системы аксиом Цермело-Френкеля и стала наиболее «классической» и часто используемой в контексте теории множеств [c]. Ещё есть альтернативное расширение этой системы – аксиоматика фон Неймана-Бернайса-Гёделя, но всё же система Цермело-Френкеля используется наиболее часто и принята в качестве основной [c]. Данная система должна была устранить те парадоксы и противоречия, которые возникали из-за нечёткого определения понятий. Ввиду «особого положения» аксиомы выбора она обычно рассматривается отдельно от общей аксиоматики и принято дополнительно уточнять с какой именно аксиоматикой мы имеем дело. Поэтому говорят либо «система аксиом Цермело-Френкеля с аксиомой выбора», что означает рассмотрение какого-либо вопроса теории множеств с априорным допущением об истинности аксиомы выбора; либо говорят просто «система аксиом Цермело-Френкеля», не упоминая об аксиоме выбора и не подразумевая её истинной в контексте рассмотрения какого-либо вопроса. Итак, аксиомы Цермело-Френкеля для теории множеств – всего 5 основных аксиом:
•	экстенсиональности
•	объединения
•	степени (булеана)
•	регулярности
•	бесконечности
Отдельно следует указать ещё следующие аксиомы:
•	пустого множества
•	пары
•	выбора
И ещё «отдельнее»:
•	схема преобразования
•	схема выделения
Также следует заметить, что схема преобразования и схема выделения, строго говоря, это уже не аксиомы, а скорее методы манипулирования множествами. Аксиомы же пустого множества и пары порой полагаются избыточными, так как их можно вывести из остальных аксиом согласно правилам вывода. Аксиома выбора же, как уже говорилось, стоит особняком и об этом мы позже тоже скажем отдельно. А пока кратко охарактеризуем каждую аксиому и её значение для теории множеств. Символом «->» мы будем обозначать принадлежность левого операнда правому операнду, например, «буква А» -> «русский алфавит», что означает «буква А принадлежит русскому алфавиту». Сами множества мы будем представлять в классическом виде – в фигурных скобках, диапазоны же – в квадратных скобках.
Аксиома экстенсиональности [c]. Она определяется так: для любых 2 множеств, если каждый элемент 1-ого принадлежит 2-му, а также каждый элемент 2-ого принадлежит 1-му, то 1-е множество идентично 2-му. Достаточно интуитивно понятная аксиома. Для примера, представим себе два множества: {1, 2, 3} и {3, 1, 2}. Каждый элемент первого множества принадлежит второму, а каждый элемент второго – первому. Соответственно, из аксиомы экстенсиональности следует, что множества равны. Здесь следует обратить внимание на то, что при сравнении множеств не учитывается порядок элементов, а принимается в расчёт только принадлежность элементов множеству. То есть если, например, мы сформируем множество все комбинаторных перестановок множества из трёх натуральных чисел и отсортируем их в порядке возрастания сначала по первому числу, затем по второму и после по третьему, то мы сможем пронумеровать каждое из этих комбинаторных множеств натуральными числами. Эти числа можно будет определить, как порядковые – о которых мы уже говорили ранее. Так вот, порядковые числа множеств с различающимся порядком элементов – равны не будут. В данной аксиоме этот момент не учитывается, что необходимо иметь ввиду.
Аксиома объединения [c]. Она определяется так: из любого семейства Х множеств Y можно образовать такое множество Z, каждый элемент z которого принадлежит хотя бы одному множеству Y семейства множеств Х. Если проще, то эта аксиома говорит о том, что на основе некоего множества других множеств, мы можем образовать ещё одно множество, которое будет состоять из всех элементов всех множеств изначального множества. На примере это смотрится интуитивно понятным. Допустим, у нас есть множество множеств {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}} и на основе него и его элементов мы можем сформировать следующее множество {1, 2, 3, 4, 5, 6, 7, 8, 9}. Здесь следует дополнить, что для случаев рассмотрения конечных множеств – проблем никаких не возникает. А вот для множеств, соответственно, актуально бесконечных – уже есть нюансы.
Аксиома степени [c]. Также иногда определяется, как аксиома булеана. Она определяется так: для любого множества Х существует множество Y, которое является множеством всех подмножеств Х. Несмотря на некую внешнюю схожесть, эта аксиома не следует явным образом из аксиомы объединения. А говорит она о том, что для некоего отдельно взятого множества мы всегда можем сформировать множество его подмножеств. На примере это выглядит следующим образом. Допустим, у нас есть всё то же множество {1, 2, 3}. И вот на его основе мы и можем сформировать множество всех его подмножеств, которое будет выглядеть следующим образом – {{}, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}}. Для примера мы взяли множество довольно маленького размера – 3, и получили новое множество – с размером 8. Если бы мы попробовали сформировать такое же множество всех подмножеств для объединённого множества из нашего предыдущего примера, то оно бы уже имело размер – 512 элементов. Отсюда видно, что размер множества всех подмножеств для некоторого множества будет определяться степенью 2, то есть будет равен 2 в степени размера исходного множества. Отсюда далее следует, что алгоритм построения множеств всех подмножеств будет иметь экспоненциальную асимптотическую сложность, то есть – будет работать очень медленно, что, в свою очередь, определяется огромным объёмом той работы, которую ему предстоит выполнить. Однако актуальная бесконечность и здесь решает проблему – условно представить это мы себе можем. Ещё среди интересных замечаний по данной аксиоме целесообразно указать следующее. Если мы представим исходное множество в качестве некой части, а множество всех подмножеств – в качестве целого, то перед нами окажется единственный случай, для которого не выполняется постулированное Расселом правило «равенства части и целого» для актуально бесконечных множеств. То есть, согласно аксиоме, множество всех подмножеств всегда больше исходного множества и всегда имеет размер двойки в степени его размера. К этому замечанию мы также ещё вернёмся в соответствующей части нашей работы. Как и, собственно, к глубокой связи между этой аксиомой (и прочими, кстати, тоже) и аксиомой выбора.
Аксиома регулярности [c]. Ещё иногда называется аксиомой фон Неймана. Она определяется так: для любого непустого множества Х существует х, такой что х -> X, и пересечение х с Х равно пустому множеству. Собственно говоря, на этой самой аксиоме, как предполагалось изначально, должна формироваться невозможность возникновения парадоксов, подобных парадоксу Рассела. То есть, согласно аксиоме регулярности, в контексте конструктивного построения множеств заложено отсутствие возможности формирования циклов. Так ли это на самом деле или нет – мы разберём позже.
Аксиома бесконечности. Она определяется так: существует множество Х, такое что для каждого х -> X, объединение х с х – также принадлежит Х. Иными словами, согласно аксиоме бесконечности, существует такое множество Х, которое содержит пустое множество и для любого элемента этого множества Х, также содержит некое иное множество, содержащее этот самый элемент и само множество Х. Аксиома бесконечности гарантирует существование хотя бы одного бесконечного множества и лежит в основе построения бесконечного ряда натуральных чисел при помощи самой теории множеств.
Аксиома пустого множества или аксиома существования [c]. Определение: существует такое множество Х, что для любого элемента Y истинно высказывание «Y не принадлежит Х». Дадим ещё одно распространённое определение, которое также является одним из наиболее интересных, и оно звучит так: существует такое множество, которое содержит элемент z, для которого верно высказывание «z = не z». То есть, само собой понятно, что в контексте классической бинарной логики, такого элемента существовать не может. Но вот существование именно множества – аксиома утверждает. Из чего вытекает существование множества, которое не содержит элементов – пустого множества. Иногда эта аксиома формулируется несколько проще и «просто» утверждает существование множества, которое обладает мощностью, равной 0. Пустое множество является своим подмножеством, но не своим собственным элементом и представляет собой единственное множество с мощностью булеана равной 1. На основе пустого множества выстраивается, собственно, вся остальная система множеств. И бесконечные множества базируются изначально именно на наличии пустого множества. Оно для теории множеств играет ту же роль, что и 0 для арифметики целых (или натуральных – в зависимости от конкретного подхода) чисел. В некотором (неформальном) роде, все остальные аксиомы системы Цермело-Френкеля находятся в концептуальной зависимости от аксиомы пустого множества.
Аксиома пары [c]. Её определение звучит так. Для любых двух множеств Х и Y существует такое множество Z, которое содержит только эти два множества. То есть, для каждой пары Х и Y существует такое Z, что Z = {X, Y}. Эта аксиома свидетельствует о возможности построения на основе отдельных множеств более сложных структур и гарантирует возможность объединения в пару множеств для любых двух множеств.
Аксиома выбора [c]. Ни одна другая аксиома не вызывала столько споров и противоречий среди математиков, как аксиома выбора. Собственно, до конца 19-ого века она использовалась, можно сказать, «неявно» и «из-под полы» – математики могли её применять как бы «неосознанно». Её определение звучит так. Для любого семейства W непустых множеств q может быть определена функция выбора, которая ставит в соответствие каждому q элемент z, такой, что z -> q. Звучит достаточно просто и интуитивно понятно. Однако, как сказал Бертран Рассел об аксиоме выбора «Сначала она кажется очевидной; но чем больше вдумываешься, тем более странными кажутся выводы из этой аксиомы; под конец же вообще перестаешь понимать, что же она означает» [Виленкин Н. Я. Рассказы о множествах. – 3-е изд. – М.: МЦНМО, 2005. – С. 95. — 150 с.]. Традиционно следует заметить, что для случаев конечных множеств никаких особенных проблем с аксиомой выбора не возникает – всё «работает как часы». Однако, как только приходится рассматривать аксиому выбора в контексте операций с бесконечными множествами – практически сразу же начинаются проблемы и противоречия. Опять же, изначально её смысл интуитивно понятен: у нас просто есть некий шкаф с какими-то непустыми коробками, и мы вполне естественным образом можем выбрать из каждой коробки по одному предмету, руководствуясь каким-либо критерием – самый большой предмет, самый маленький, самый зелёный, самый красивый и так далее. То есть мы можем определить функцию выбора на конечном множестве. Однако если мы захотим определить функцию выбора на бесконечном множестве, то могут возникнуть некоторые, мягко сказать, затруднения. Здесь стоит заметить, что всё же не во всех случаях с бесконечными множествами они возникают. Например, в случае с бесконечными, но счётными множествами, эти затруднения снимаются, так как мы всегда можем определить наименьший элемент всякого счётного множества и, соответственно, выбрать его. Тем самым функция выбора на бесконечном множестве будет определена. То есть мы можем себе представить множество множеств в виде полуоткрытых диапазонов натуральных чисел: {[1, ∞), [10, ∞), [100, ∞), [1000, ∞), …}. Даже в таком случае мы можем выбрать из каждого множества его наименьший элемент и тем самым определить функцию выбора.
Примерно таким же образом дело обстоит и в случае с бесконечным закрытым диапазоном вещественных чисел [0, 1]. Несмотря на то что у данного диапазона явно заданы и начало, и конец, он является бесконечным в плане внутренней «плотности». То есть можно доказать, что для любого числа X, входящего в этот диапазон, есть число X/2, которое также входит в этот диапазон, а, соответственно, множество бесконечно. Однако в данном случае, так как границы диапазона явно заданы, мы всё равно можем выявить как наибольшее, так и наименьшее из чисел, в него входящих и, как следствие, определить функцию выбора. Такой же итог будет и у попытки определить функцию выбора на множестве, представляющем любой замкнутый диапазон, например – [0, 1], или полуоткрытый диапазон, например – [0, 1) или (0, 1]. То есть в приведённых случаях мы можем выбрать наибольший или наименьший элемент заданного множества, а значит, определить функцию выбора для каждого множества, и здесь противоречий также не возникает. А вот в случае с открытым диапазоном вещественных чисел, то есть (0, 1), который является и бесконечным, и несчётным, и не имеем никаких строго заданных пределов, ситуация становится вовсе иной. А именно – мы не можем здесь явно определить функцию выбора, так как нет явно заданного наименьшего, наибольшего или же хотя бы каким-либо образом определённого элемента множества со строго «утверждённой» спецификой.
То есть, при попытках применения аксиомы выбора к бесконечным несчётным множествам, мы достаточно быстро приходим к противоречию, так как аксиома выбора утверждает, что для любого множества непустых множеств функция выбора может быть определена, а попытка это сделать – приводит к утверждению о том, что мы не можем этого сделать. Соответственно – возникает противоречие.
Далее мы также покажем, что аксиома выбора, резюмирующаяся в определении функции выбора, тесно пересекается не только с конечностью/бесконечностью и счётностью/несчётностью множеств, но также и со свойством упорядоченности/неупорядоченности. Здесь же дополним, что попытки глубокого осмысления аксиомы выбора достаточно быстро приводят к экзистенциальной составляющей математических объектов и вопросу об их «существовании». Примерно такая постановка вопроса в своё время и послужила далеко не последним фактором разделения единой математики на множество «математик». В частности, интуиционисты, к примеру, полагали, что существует только то, что может «быть построено», иными словами – сконструировано. Иные же определения признавались ими бессмысленными, бесполезными и «ложными».
Аксиома выбора и по сей день принимается не всеми математиками и не во всех «математиках». Те математические результаты, которые были получены с помощью аксиомы выбора, некоторыми математиками полагаются имеющими «иную», более низкую ценность, в отличии от тех результатов, которые были получены без использования этой аксиомы. Аксиома выбора и её признание/непризнание определяется как настолько принципиальный момент, что при работе с какими-либо объектами теории множеств обязательно следует указывать, какая именно аксиоматика используется. И, так как классический вариант, как уже упоминалось выше, один, то и указание выглядит соответствующим – «система аксиом Цермело-Френкеля с аксиомой выбора» и «система аксиом Цермело-Френкеля без аксиомы выбора». Ни одна другая аксиома таким образом не выделяется, что подчёркивает специфическое «положение» аксиомы выбора.
Схема преобразования [c]. Как и говорилось выше – это уже не аксиома, а скорее высказывание, задающее специфику построения новых множеств на основе уже существующих. Её определение звучит так. Любое множество Х может быть преобразовано в множество Y, при условии определения некой функции f для всех элементов множества Х. Это проще всего понять на примере. Допустим, у нас есть некое множество Х, состоящее из натуральных чисел – {1, 2, 3, 4, 5}. Мы можем определить функцию f, которая будет возвращать истинное значение в том случае, если натуральное число меньше 4 и ложное – в противном случае. Если возвращаемое значение истинное, то мы оставляем элемент в множестве, а если ложное – удаляем его. Попробуем применить эту функцию к нашему множеству Х. И по итогу мы получим новое множество Y, которое будет выглядеть так – {1, 2, 3}. Конечно, это только один пример, к тому же очень простой. На самом деле схема преобразования позволяет формировать очень сложные структуры на основе структур уже наличествующих.
Схема выделения [c]. Её определение звучит очень похожим образом на только что охарактеризованную схему преобразования. На основе любого множества Х может быть сформировано (выделено) его подмножество, за счёт использования некоторого критерия выделения. Под критерием выделения подразумевается, конечно, некоторая функция, которая в зависимости от того, соответствует ли элемент множества критерию или же не соответствует, возвращает значение – истинное или ложное. То есть, мы можем представить то же самое множество Х, которое использовалось в предыдущем примере, и ту же самую функцию f. Далее мы, соответственно, сможем сформировать то же самое множество Y, что и в предыдущем примере. Но разница заключается в том, что в предыдущем примере мы формировали буквально новое множество – отдельное от изначального множества Х, а здесь – формируем некоторое подмножество множества Х. Понятно, что и предыдущее множество Y и наше новое подмножество будут представлять собой одно и то же множество. Но вот сам аксиоматический смысл здесь весьма неплохо детализируется и приводит к тому, что на основе одного и того же множества мы можем формировать как новые множества, так и подмножества исходного множества.
Приведённая выше аксиоматика составляет концептуальный костяк теории множеств. Она формировалась для того, чтобы теория получила прочную, строго очерченную понятийную основу и для того, чтобы избежать противоречий, которые могут приводить к парадоксам, подобных тем, которые были нами рассмотрены в предыдущем разделе. Насколько хорошо это получилось – покажет время. И, может быть, даже очень скоро.
Однако в любом случае примерно в таком виде теория множеств существует в наше время. Но, если всё же попытаться «увидеть лес за деревьями», то можно заметить, что основы аксиоматической теории множеств остались теми же самыми, которые использовались в рамках «наивной теории множеств» и ключевым из них является всё та же актуальная бесконечность и её следствия: несчётность некоторых множеств, равномощность «части и целого», трансфинитная индукция и так далее. Аксиоматические системы просто более чётко определили «пределы» использования понятия множеств и манипулирования ими. В дальнейшем мы проверим основы и некоторые «аксиоматические связки» на «внутреннюю непротиворечивость» с философских и логических позиций.

ВЫВОДЫ ПО ГЛАВЕ

Изначально нами были исследованы истоки возникновения теории множеств, в первую очередь истоки философского характера. Было показано, что основы самой формы мысли о феноменологическом рассмотреним целого, совокупности, единого и так далее – восходят ещё к Платону и Аристотелю. Далее, через тысячелетия и века, через такие фигуры, как Галилей, Ньютон, Лейбниц, Гаусс, Больцано, Дедекинд и прочие – эта форма мысли достигает своего апогея в теории множеств, как бы резюмируясь в ней.
Так как наиболее характеристической чертой теории множеств является, поносимое всеми ранее от милетской школы философии вплоть до самого Георга Кантора (не включая его, конечно же), понятие актуальной бесконечности – мы осуществили некоторое исследование актуальной и потенциальной бесконечностей, а также несколько коснулись иной демаркации – на бесконечность количественную и качественную. Упомянули также введённое Кантором понятие бесконечности абсолютной, но не стали особенно на ней задерживаться, ввиду её малой непосредственной значимости для нашего предмета рассмотрения. Мы указали, что потенциальная бесконечность представляет собой имеющий некоторое начало и не имеющий завершения процесс – потенциальная бесконечность процессуальна. А вот, что касается понятия бесконечности актуальной – всё несколько сложнее. Она подразумевается «уже» данной. Это вне всякого сомнения противоречит нашей интуиции и, как мы неоднократно указывали – к этому необходимо несколько привыкнуть, так как изначально бесконечность актуальная представляется простым абсурдом. Однако, как мы также указывали, нам придётся с этим свыкнуться, так как в теории множеств актуальная бесконечность служит в качестве «сердечно-сосудистой системы» для всей теории.
Мы приводили пример с множеством всех вещественных чисел на открытом диапазоне (0, 1) и указывали на то, что оно – актуально бесконечно, согласно представлениям, принятым в рамках теории множеств. И бесконечно оно потому, что для каждого вещественного числа х, входящего в это множество, есть число х / 2, также входящее в это множество.
Основная идея, которая была нами высказана в части о бесконечности, заключается в том, что актуальная бесконечность – это бесконечность потенциальная, преобразованная в соответствии с законами и принципами mundus. Мы также предположили, что при малейшем «выходе» за рамки теории, а порой даже и в них, на основе актуальной бесконечности начинают формироваться парадоксы. Также, что важно для понимания этого раздела, мы провели некую логическую связь между бесконечностью, временем и движением. На основе этой связи нами было показано, насколько легко и непринуждённо образуются логические парадоксы. Причём, парадоксы эти формируются согласно законам логики и непротиворечивым самими по себе основаниям, которые взяты для формирования ситуации. В контексте усиления смысла нашей позиции мы взяли для примера одну из самых известных апорий Зенона – «О стреле». Мы предложили в качестве некоего противовеса и дополнения к этой апории нашу собственную, которую мы обозначили, как «Момент Зенона». На основе этих двух логических конструкций мы вывели то, что нестрого определили в качестве «логического замыкания» – парадокса, подобного парадоксу Рассела.
Далее нами были охарактеризованы некоторые основные понятия теории множеств: непосредственно сами множества и понятие пустого множества, кардинальные и ординальные числа вместе с иерархией алефов, трансфинитная индукция, совершенные/несовершенные и плотные/неплотные множества, арифметика множеств, а также понятие Канторова множества и сам метод его построения, предложенный Кантором.
После этого мы вплотную подошли к парадоксам теории множеств, парадоксам, которые мы немного затрагивали ранее – в связи с исследованием бесконечности. Нами были освещены такие из них, как парадокс Кантора, Бурали-Форти и Рассела, а также упомянуты некоторые другие – Банаха-Тарского, Ришара и так далее. В данном случае нами было указано, что «открытие» парадоксов теории множеств послужило началом пересмотра всего теоретико-методологического аппарата системы Кантора, которая позже получила название «наивной теории множеств». Итоги этого пересмотра были резюмированы в виде аксиоматики Цермело-Френкеля, о которой мы далее сказали отдельно.
Изначально мы пояснили, что такое вообще аксиомы и как устроена аксиоматика – феноменологически. Предварительно аксиомы были нами представлены, как как некие высказывания, которые не требуют доказательств, а теоремы, в свою очередь, как высказывания, которые выводятся из аксиом и/или из ранее доказанных теорем и далее доказываются при помощи принятых правил логического вывода. Далее мы раскрыли смысл самих правил логического вывода, отдельно указав на наличие «логического плюрализма» в контексте непосредственно логики и до известной степени автономное существование различных «логик», порой косвенно противоречащих друг другу. На основе проведённого исследования мы переопределили понятие аксиом, чтобы оно приобрело более корректный и соответствующий своей сути вид. Аксиомы – это некие высказывания, которые определяют контекст формальной системы и служат основой для доказательств в рамках этой системы. Общая целесообразность их наличия сводится к тому, что в рамках точной науки мы не можем «слепо верить» во что-либо, а должны опираться на некие доказательства. А эти доказательства, в свою очередь, должны быть на чём-то построены для предотвращения формирования бесконечного цикла доказательств. Здесь же нами был раскрыт смысл неопределяемых понятий в контексте аксиом: во-первых, они необходимы для того, чтобы нивелировать неизбежность определения всех понятий без исключения, иначе это также привело бы к необходимости бесконечных определений одних понятий через другие; во-вторых, неопределяемые понятия формируют потенциальную возможность для большей гибкости и «смысловой широты» формальной системы, иными словами – для более высокого уровня абстрактности системы. Далее мы рассмотрели значимость независимости одной аксиомы от другой в контексте единой аксиоматической системы и указали на единственное фундаментальное требование к этим системам – внутренняя непротиворечивость.
После того, как мы разобрались с основами, мы перешли непосредственно к самим аксиомам Цермело-Френкеля. Среди них мы охарактеризовали следующие: экстенсиональности, степени, регулярности, объединения, бесконечности; отдельно мы рассмотрели аксиомы пары, существования пустого множества и аксиому выбора; также, в несколько иной группе, рассмотрели схему преобразования и схему выделения.
Резюмируя, мы указали на то, что несмотря на всю аксиоматику, теория множеств продолжает базироваться на тех же самых «китах», что и «наивная теория множеств», а именно – на понятии актуальной бесконечности и логических из неё следствиях: несчётность некоторых множеств, равномощность «части и целого», трансфинитная индукция и прочие.
 
ГЛАВА 3. СУЩНОСТЬ И ПОНЯТИЕ АЛГОРИТМА: ПОПЫТКИ ФОРМАЛИЗАЦИИ

ПРЕДИСЛОВИЕ

В данной части нашей работы мы несколько диссоциируемся от главной линии изложения и посмотрим чуть в сторону – локально в одну из областей, активно формируемых в те же годы, когда и разразился тот самый кризис оснований математики, о котором уже было нами сказано выше. Актуальность этого отступления частично в том, что далее нам придётся довольно много говорить о таком феномене, как рекурсия. А истоки этого феномена восходят как раз в том числе и к Курту Гёделю, который больше известен своими теоремами о неполноте и противоречивости. Однако Гёдель также был в числе тех, кто на основе самореференции разработал один из подходов к формализации алгоритма – рекурсивные функции.
Основной же детерминантой целесообразности является то, что «вычисления резюмируются в алгоритме», то есть любое вычисление в контексте математики (и вообще – феноменологически) представляет собой алгоритм. Таким образом, фундаментальность данного феномена трудно переоценить, а так как нам в дальнейшем придётся прямым образом иметь дело с вычислениями в том смысле, что мы полагаем их одним из «китов», на которых будет формироваться наша финальная концепция, то считаем целесообразным эксплицировать данное понятие и его сущность.
Мы собираемся рассмотреть некоторую область теории алгоритмов, а именно – осуществление попыток формализации алгоритма, определения того, что же он есть такое – именно фундаментально, базисно, в общем смысле этого слова. Из названия данной части может быть понятно, что мы на самом деле до сих пор не имеем представления о том, что же такое в сущности алгоритм – у нас есть лишь аппроксимации ответа на этот вопрос.
Вообще само понятие – алгоритм – названо по имени азиатского учёного Аль-Хорезми, жившего примерно между 780-ым и 850-ым годами. Со временем понятие «алгоритм» стало обозначать выполнение неких последовательностей счётных действий, оперируя при этом арабскими цифрами. Известно, что понятие алгоритма использовали в своих сочинениях, к примеру, как Г. Лейбниц и Л. Эйлер. В русском языке зачастую использовалось формулировка «алгорифм», то есть написание термина осуществлялось через букву «ф». Однако постепенно это использование термина сохранилось лишь в понятии «нормального алгорифма» Маркова, что вполне можно считать исключением и своеобразным «брендом».
Постепенно, примерно к началу 20-ого века, в интерпретировании понятия алгоритма акцент несколько смещался именно на формализацию процесса, то есть под алгоритмом стало пониматься всякое действие, выполняемое в рамках строго определённых правил. К слову сказать, именно фокусированию внимания непосредственно на формализации всякого процесса, каким бы он при этом не являлся, мы обязаны современной дифференциацией алгоритмов на управляющие и вычислительные, за вторыми из которых порой сугубо и закрепляется это понятие, что, к слову сказать, не является правомерным. Разумеется, что далеко не везде понятие алгоритма сразу стало восприниматься как вообще формализация всякого процесса. К примеру, в СССР считалось, что данное понятие имеет своей специализацией исключительно область математики, что собственно и было отражено в изданиях того времени: например, как в Большой советской энциклопедии, так и в Словаре русского языка.
Однако, вместе с наполнением понятия алгоритма всё более глубоким и расширенным смыслом происходила также и экстраполяция данного понятия и в иные, кроме математики, сферы. Данному процессу во многом способствовало появление компьютеров и, как следствие, развитие информатики и некоторых смежных дисциплин, к примеру, – кибернетики. Причём в этом случае имеет место весьма интересная взаимосвязь между развитием компьютерной техники и развитием непосредственно алгоритмов. То есть, сам генезис компьютеров также представляет собой довольно длительный путь, который проходит от разработки первой в мире вычислительной машины В. Шиккарда, совместно с машинами Б. Паскаля и уже упомянутого Г. Лейбница, через Жаккардов станок и вычислительную машину А. Лавлейс и Ч. Бэббиджа к программируемому компьютеру К. Цузе и разработке фундаментальных принципов построения вычислительной техники – гарвардской архитектуре и принстонской архитектуре [2, 3, 4, 5, 6, 7]. Принстонскую архитектуру, ввиду некоторого случая, чаще называют архитектурой фон Неймана, и принципы, лежащие в её основе, до сих пор повсеместно используются в построении современной вычислительной техники, к примеру, использование бинарной системы представления данных в актуальных нам компьютерах и прочее.
Соответственно, конструктивным будет заметить, что если бы не было алгоритмов, как вообще феномена, то не было бы и вычислительной техники; с другой же стороны следует, казалось бы, гораздо менее очевидное предположение о том, что не будь вычислительных средств, то не было бы и алгоритмов, по крайней мере именно вычислительных.

3.1. ПРОБЛЕМА ФОРМАЛИЗАЦИИ АЛГОРИТМА И ПОПЫТКИ ЕЁ РАЗРЕШЕНИЯ

Базис сущности современных нам алгоритмов закладывался в начале-середине 20-ого века и полагается, что основой для этого послужила постановка Гильбертом «проблемы разрешения». Общеизвестно, что Гильберт сформировал знаменитый «Список Гильберта» – подборку из 23-х фундаментальных математических проблем [1]. Однако сам список был опубликован в 1900-ом году, а проблема разрешения была сформулирована Гильбертом в 1928-ом. Походя предположим, что сама проблема разрешения была детерминирована попытками разрешить проблемы из упомянутого списка. Проблема разрешения представляет собой «логическую проблему нахождения в рамках какой-либо теории общего метода, позволяющего решать, может ли отдельное утверждение, сформулированное в рамках данной теории быть доказано в ней или нет» [2]. Этот самый «общий метод» также обозначается как «эффективная процедура», а ещё, соответственно, как уже нам знакомый алгоритм. Сам Гильберт изначально сформулировал данную проблему для языка формальной логики, то есть проблема заключалась в необходимости понять является ли некое утверждение на языке формальной логики истинным или нет. И для того, чтобы иметь возможность непротиворечиво это понять, предполагалось наличие некой «эффективной процедуры».
Открытие первой и второй теорем Гёделя показало, что подобного «общего метода» не существует по крайней мере для системы арифметики и всех ей подобных. В этой же связи следует упомянуть и о теореме Тарского, согласно которой понятие истины, принадлежащей какой-либо «достаточно сильной» формальной системе, невыразимо средствами этой самой системы. «Достаточно сильная» формальная система или можно ещё сказать – богатая (в плане предоставления широкого спектра возможностей по оперированию символами своего алфавита), это такая формальная система, в рамках которой всякая синтаксически валидная замкнутая формула или её опровержение доказуемы, или более точно: полная формальная система – это система, в рамках которой на множестве её аксиом и при помощи её правил вывода может быть получена любая правильно построенная формула (истинность или ложность которой могут быть, соответственно, проверены при помощи определённой эффективной процедуры валидации). Также здесь нельзя не упомянуть о том, что хронологически первым частным случаем общей проблемы разрешения стала именно алгоритмическая разрешимость, то есть способность некой формальной системы обладать алгоритмом, определяющим по предоставленной ему формуле, выводима ли она непротиворечивым образом из совокупности фундаментальных положений, то есть аксиом, данной системы или же нет.
В любом случае, вне зависимости от результатов разбора и исследования поставленной Гильбертом проблемы разрешения, научному сообществу потребовалось уточнение критериев того, что вообще подразумевалось под «эффективной процедурой». Под уточнением критериев здесь, учитывая специфику контекста, имелась ввиду непосредственно формализация. Формализация же – это представление некоего феномена в формальном виде, как правило в виде формальной системы. И наиболее фундаментальные попытки формализовать сущность алгоритма были следующими, причём независимыми друг от друга: рекурсивные функции (К. Гёдель, Ж. Эрбран, С. Клини), λ-исчисление (лямбда-исчисление) А. Чёрча, разработанная Э. Постом «Машина Поста» и уже упомянутая выше «Машина Тьюринга» за авторством, соответственно, А. Тьюринга. Ещё в этот список целесообразно будет добавить комбинаторную логику М. Шейфинкеля и Х. Карри, а также нормальный алгорифм А. Маркова.
Однако, перед тем как мы начнём, целесообразно заметить, что алгоритм – изначально именно философское понятие. И в этом смысле он означает обработку неких данных на основе изначально имеющихся «блоков» этих данных с последующей выдачей неких «результирующих» «блоков».
Основоположником рекурсивных функций считается К. Гёдель, а также значимый вклад в данное направление был внесён Ж. Эрбраном и С. Клини. Рекурсивные функции в математике подразделяются на 3 класса: примитивно рекурсивные, общерекурсивные и частично рекурсивные. К слову сказать, как общерекурсивные, так и частично рекурсивные функции валиднее было бы называть следующим образом: функции, определённые на всём множестве аргументов и, соответственно, функции, определённые на некотором подмножестве множества аргументов. А их расхожие идентификаторы прижились по историческим причинам ввиду нюансов перевода с английского языка слов «all» и «partial» и могут несколько вводить в заблуждение.
Далее, если не особенно углубляться в математические формулы, а сосредоточиться непосредственно на сути, то абстрагированный смысл рекурсивных функций заключается в многократном применении функцией своей собственной логики к своим, на каждом шаге изменяющимся и редуцирующимся к терминальному случаю, аргументам, то есть данным. Подчеркнём, что сама функция – как некий отдельный объект – не должна претерпевать внутренних изменений, иначе это должно будет считаться нарушением самого принципа рекурсии.
Итак, определим ещё раз, рекурсия – это вызов некой функции «F(x)» самой же собой в рамках самой себя – «F(1)=1 F(x)=x*F(x*F(x-1))» – несколько вольная запись примера вычисления факториала числа x. Ключевыми свойствами рекурсии являются следующие: наличие терминального случая, то есть такого состояния входных данных, для которого уже, без дополнительных вычислений, известно выходное значение; вызов функцией самой себя; наличие у функции равных возможностей вне зависимости от уровня вложенности – фундаментальный принцип рекурсивного проектирования и вывода. Конечно же языки программирования (возможно, за редким исключением) поддерживают возможность имплементации рекурсивных функций, а парадигма функционального программирования так и вовсе целиком на них построена. Однако каким именно образом рекурсивные функции формализуют понятие вычислимости, а, соответственно, и алгоритма?
Это проще всего пояснить на примере примитивно рекурсивных функций. В данном контексте наличествует некоторое множество базовых рекурсивных функций и два оператора, которые было бы корректнее определить, как функции высшего порядка, так как они могут принимать другие функции в качестве аргументов и возвращать другие функции в качестве результата: оператор суперпозиции или подстановки и оператор примитивной рекурсии. То есть, у нас имеется некоторое множество функций, результат выполнения которых для определённых аргументов заранее известен и не требует вычисления – грубая аналогия с арифметическими аксиомами или правилами вывода в контексте формальной логики. А также у нас имеется возможность, за счёт наличия операторов, формировать различные комбинации рекурсивных функций, пошагово редуцируя аргументы функции к аргументам базовых функций, для которых результат уже известен. То есть при каждом шаге рекурсии подразумевается редукция актуальных аргументов к аргументам с уже имеющимися ответами, а таким образом рекурсивная функция априори конечна. Дабы было ещё понятнее, приведём конкретный пример. Мы знаем, что сумма всех чисел от 1 до 1 равна 1. Теперь же мы можем взять функцию с любым целочисленным аргументом и просуммировать «обратный ряд» от числа n до числа 1 за счёт вызова функции на каждом шаге с аргументом, уменьшенным на единицу. И смысл здесь именно в том, что, как может быть индуктивно понятно из условий, таким образом можно представить, определить, формализовать любую вычислимую функцию. В этом и заключается фундаментальность рекурсивных функций и их значимость для формализации алгоритма.
В качестве ключевой особенности рекурсивных функций можно выделить следующее: для того, чтобы рекурсивные функции могли быть успешно применимы для вычисления хоть чего-нибудь, мы должны иметь до известной степени априорное знание некоторых «истин» о исследуемом. Если мы вообще ничего не можем сказать о том, что исследуем, то мы не можем в полной мере использовать мощь рекурсии, так как одним из основных критериев рекурсии вообще является наличие терминального случая – случая остановки вычислений. Без этого случая мы просто получим бесконечную рекурсию без какого-либо конструктивного завершения. Поэтому, подчеркнём ещё раз – при рекурсивном исчислении мы должны исходить либо из эмпирического опыта о том, с чем мы работаем, либо из некой аксиоматической системы, которая что-либо утверждает, касаемо того, с чем мы работаем. А в целом, с учётом нашего замечания, рекурсивные функции действительно имеют весь необходимый потенциал для имплементации любого (предположительно) алгоритма.
Второй попыткой формализовать понятие алгоритма было λ-исчисление (лямбда-исчисление) А. Чёрча. Лямбда-исчисление представляет собой формальную систему для исследования и определения вычислимости, а также раздел метаматематики и, по сути, первый функциональный язык программирования. В рамках лямбда-исчисления производится оперирование термами или «обами» – математическими объектами и переменными, представляющими собой некие данные. Функциональным (в общем смысле слова) фундаментом лямбда-исчисления являются аппликация – применение некой функции к переданному ей аргументу и абстракция – формирование функций из наличествующих выражений, построенных из символов алфавита лямбда-исчисления. Также в рамках данной формальной системы наличествуют три ключевых свойства: α-эквивалентность, β-редукция и η-преобразование. Если отвлечься от конкретных деталей и, в соответствии с нашим подходом, абстрагировать исследуемый смысл, то можно сказать, что перечисленные свойства ответственны за преобразование переменных и термов из одного вида в другой, их объединение, отождествление и прочее. То есть, обобщим, в рамках лямбда-исчисления у нас наличествуют некие математические объекты – термы, определённые переменные, и функции, а также возможность осуществления преобразования данных.
Таким образом, на основе представленных особенностей лямбда-исчисления, логично следует некоторые выводы. Во-первых, также, как и рекурсивные функции, лямбда-исчисление действительно полностью подходит под общее философское определение алгоритма, а также, на основе широты возможностей, целесообразно предположить, что всякую вычислимую функцию можно выразить при помощи формальной системы лямбда-исчисления. Во-вторых, также стоит заметить, что лямбда-исчисление является несколько более «функционально свободным», чем рекурсивные функции по той причине, что здесь подразумевается более общий способ обработки данных, а не просто «редукция неизвестного к истине» – вычисляемого к вычисленному.
Здесь стоит сказать о том, что Гёдель и Чёрч, по крайней мере судя по их переписке, осознавали то, что ведут в общем-то весьма схожую работу в одном и том же направлении. В этом контексте интересно высказывание самого Гёделя о том, что его «…единственная идея в то время состоит в том, что может быть возможно задать термин эффективной вычислимости как неопределенного понятия в виде набора аксиом, которые бы воплощали общепринятые свойства этого понятия и затем что-то делать на этой основе» [3, 8]. То есть Гёдель говорил по сути именно о том, о чём мы и постулировали в рамках определения рекурсивных функций – о некоем множестве априорных «истин» по отношению к некоторой «размерности» абстрактных объектов, на основе которого возможно провести выявление новых «истин», относящихся к иным «размерностям» объектов путём сведения неизвестного к известному и фиксированию соответствующей разницы. Как мы и говорили выше – «редукция неизвестного к истинному».
Также, в связи с разбором лямбда-исчисления, целесообразно упомянуть и о комбинаторной логике М. Шейфинкеля и Х. Карри. Она сформировалась в общем-то даже несколько раньше лямбда-исчисления и была по сути дважды переоткрыта: впервые М. Шейфинкелем, а через несколько лет после него – Х. Карри. Итак, комбинаторная логика представляет собой, как и лямбда-исчисление, абстрактную формальную систему и один из разделов метаматематики, а объектом изучения для данной дисциплины также являются вычисления. И, вновь «в ногу» с лямбда-исчислением, комбинаторная логика тоже послужила основой для парадигмы программирования, в данном случае – комбинаторного. Ключевыми понятиями комбинаторной логики являются: функция с арностью равной 1; аппликация – то есть применение функции к аргументу. Аксиом же в комбинаторной логике 2: вычёркиватель – «Kxy=x» и распределитель «Sxyz=xz(yz)». И к виду этих двух аксиом, которые также сами являются комбинаторами, можно, согласно правилам вывода комбинаторной логики, свести любой другой комбинатор. Комбинатор же определяется как результат одно-или более-кратного применения функции к аргументам при помощи каррирования. Это можно рассмотреть на примере. Допустим, у нас есть некая функция с энным количеством аргументов: «f(а1, а2, а3…an)». Мы можем преобразовать эту единственную функцию с энным количеством аргументов в энное количество функций с одним единственным аргументом: «f(а1)(а2)(а3)…(аn)» – и данный процесс известен как, названное по фамилии Х. Карри – каррирование. Вообще же среди комбинаторов встречаются такие как: тождество, канцеллятор, коннектор, композитор, пермутатор, дубликатор и прочие.
И здесь также целесообразно вывести, что, ввиду общности в контексте обработки информации и достаточно богатому инструментарию этой самой обработки, при помощи формальной системы комбинаторной логики может быть представлена всякая вычислимая функция. По внешним показателям особенно прочные связи, не только по смыслу, но также и по способам выражения наличествует у комбинаторной логики с лямбда-исчислением, то есть всё, что можно выразить на «языке» комбинаторной логики, также можно переформулировать на «языке» лямбда-исчисления. Но, к слову сказать, на «языке» рекурсивных функций тоже – как следствие глубокой эквивалентности.
Нормальный алгорифм Маркова (или сокращённо – НАМ) [4]. Был предложен в 1940-х годах А.А. Марковым. В общем смысле представляет собой метод работы с последовательностями символов некоторого заданного алфавита и имеет 3 составляющие: сам заданный алфавит, правила подстановки символов и начальное состояние. Сразу поясним на примере. Допустим, у нас есть некий заданный алфавит: «АВС», правила перехода «А->B, B->CС, C->A» и начальное состояние «AAB». Согласно правилам вывода, следующим состоянием будет «BBCC», потом «CCCCAA», затем «AAAABB» и так далее. Было показано, что с точки зрения возможности представления алгоритмов, НАМ сопоставим с «Машиной Тьюринга» и на его основе может быть выражена любая вычислимая функция.
Теперь же, в связи с переходом от формальных систем (рекурсивные функции, лямбда-исчисление и комбинаторная логика) через некий промежуточный этап (НАМ) к формальным автоматам («Машина Поста» и «Машина Тьюринга»), целесообразно несколько раскрыть что это – формальные автоматы – вообще такое.
Изучением упомянутых автоматов занимается один из разделов дискретной математики – теория формальных автоматов. Синонимичная дефиниция – теория абстрактных автоматов. Абстрактным или же формальным автомат определяется в контексте его отличия от автомата, соответственно, структурного – станка на заводе или робота, например. Когда же очевиден контекст, то говорят просто – теория автоматов. И «Машины» Поста и Тьюринга подпадают под юрисдикцию непосредственно теории автоматов. Итак, абстрактный автомат представляет собой математическую модель, формально определяемую как надмножество для пяти множеств: множества состояний, входного алфавита, выходного алфавита, условных переходов, условий выхода. Автоматы могут быть конечными или бесконечными (в зависимости от размера входящих в них множеств), детерминированными или недетерминированными (в зависимости от определённости условных переходов), инициальными (в зависимости от определённости множества начальных состояний), вероятностными, клеточными, дискретными, континууальными и так далее – вариаций очень много.
В рамках тематики актуальной части нашего исследования теория автоматов значима именно по той причине, что она тесно связана с теорией алгоритмов таким образом, что всякий вычислимый алгоритм может быть представлен в виде некоего формального автомата и соответствующей программы к нему. То есть автомат – это ещё один способ представления алгоритма. Также немаловажным является тот факт, что абстрактные автоматы могут служить не только для формализации алгоритмов, но также и для формирования теоретической основы среды реализации этих самых алгоритмов – вычислительных средств. Так, любой компьютер представляет собой структурную имплементацию конечного автомата. Ещё одной значимой особенностью формальных автоматов является возможность редукции любого реализуемого алгоритма к следующей форме: «Дан язык X и последовательность символов Y. Определить, принадлежит ли Y к X». То есть любую вычислимую задачу можно, в сущности, свести к этой формулировке и представить в виде абстрактного автомата и программы к нему. Также любой язык программирования и среда его реализации представляют собой формальный автомат.
«Машина Поста» [19]. Предложена Э. Постом в 1936-ом году с целью формализации понятия вычислимости. Опубликована всего на несколько месяцев позже «Машины Тьюринга» и является несколько более «простой», несмотря на их логико-математическую эквивалентность. Собственно, машина состоит из бесконечной в обе стороны ленты, состоящей из некоторых ячеек, каждая из которых может находиться в одном из двух состояний: 0 или 1 (бинарный «язык»). Также наличествует указатель (каретка, считывающее-записывающее устройство) и список команд. Команды могут быть 6 типов: поставить метку, перейти к определённой строке программы; стереть метку, перейти к определённой строке программы; сдвинуться вправо-перейти; сдвинуться влево-перейти; если в ячейке есть метка, то перейти к X, если нет, то к Y (где X, Y – команды); остановиться. После начала работы машина может оказаться в одном из трёх возможных конечных состояний: невыполнимая команда, корректное завершение, бесконечная работа.
И здесь тоже можно вывести, что на основе «Машины Поста» может быть выражен и реализован любой алгоритм, хоть его запись и нетривиальна. Конечно же, данный автомат является полным по Тьюрингу. Сама же полнота по Тьюрингу обычно определяется, как потенциально возможное наличие определённой программы для «Машины Тьюринга», которая реализует тот же самый алгоритм.
«Машина Тьюринга» [5]. Теперь же перейдём непосредственно к наиболее известной формализации сущности алгоритма. Зачастую именно «Машина Тьюринга» отождествляется с неким идеальным абстрактным вычислителем, который способен вычислить вообще всё, что в принципе вычислимо. То есть именно полнота по Тьюрингу используется в качестве ключевого критерия оценки формальных вычислительных моделей, а не «полнота по Посту» и не «эквивалентность лямбда-исчислению», хотя эти высказывания сущностно синонимичны. Итак, «Машина Тьюринга» состоит из бесконечной в обе стороны ленты, представленной ячейками в каждую из которых может быть записан символ из некоторого предоставленного конечного алфавита. Также, как и в «Машине Поста», имеется каретка (указатель), который – уже в отличие от «Машины Поста» – находится в нескольких возможных состояниях (множество которых определено и конечно, а размер которого больше нуля). Указатель может записывать символы, стирать их, а также осуществлять движение влево или вправо по ленте. Программа же, то есть некая последовательность команд для автомата, локализована в самой каретке, а не в дополнительной структуре данных, как в случае с «Машиной Поста». Эта самая программа может быть удобно визуализирована в табличном виде и представляет собой таблицу с пятью столбцами: входное состояние, входной символ, выходное состояние, выходной символ, направление движения. Имеется определённое начальное состояние, а также конечное состояние, после которого программа прекращает работу («останов»). Интерпретация программы осуществляется следующим образом: если машина находится в некоем состоянии X и на ленте записан некий символ Y, то каретка переходит в состояние K, записывает на ленту символ Z и двигается налево или направо (возможно, не двигаться вообще). А. Тьюринг показал, что на данном абстрактном автомате может быть выражен любой вычислимый алгоритм. Также он показал, что не всякий вообще алгоритм может быть реализован на данной машине, так как некоторые задачи (в общем смысле слова) не вычислимы. 
Здесь же можно было бы ещё упомянуть о «Машине Минского», которая представляет собой многоленточную версию «Машины Тьюринга», но вариаций абстрактных автоматов очень много и нет возможности разобрать каждую, а общий смысл уже и так ясен. И выражается он в виде «Тезиса Чёрча-Тьюринга», который был высказан в 1930-х годах. В упрощённом виде он устанавливает эквивалентность между лямбда-исчислением Чёрча и «Машиной Тьюринга» с логико-математической точки зрения. А так как мы помним об эквивалентности с этих же позиций друг другу рекурсивных функций, лямбда-исчисления, комбинаторной логики, нормального алгорифма Маркова, «Машины Поста» и «Машины Тьюринга», то вердикт один – все вышеприведённые формальные системы и абстрактные автоматы разными средствами постулируют одно и то же.
Формально тезис выглядит как: «Любую функцию, вычислимую физическим вычислительным средством, можно вычислить на машине Тьюринга» [6, 7]. Ну и как следствие при помощи остальных формальных систем и автоматов также можно вычислить всё то, что вычислимо на «Машине Тьюринга». Если определять более формально, то математико-логическая эквивалентность вычислительных систем А и B друг другу означает возможность передать на вход системе А описание принципов работы системы В и некие входные данные для системы В, и получить на выходе тот же самый результат, который бы вернула система В. И в этом смысле все вышеприведённые попытки формализации алгоритма эквивалентны.
Как видно из вышесказанного, несколько групп исследователей независимо друг от друга и пользуясь разными средствами пришли, по сути, к одним и тем же выводам. Нечто подобное в науке происходит, как правило, в тех случаях, когда исследователи сталкиваются с чем-либо действительно фундаментальным. И фундаментальной здесь является именно природа алгоритма, его сущность как нечто всеобщее и базовое. Если смотреть с этой точки зрения, то всякое физическое явление в природе представляет собой алгоритм, а точнее его локальную реализацию и воплощение: ситуативную акциденцию глобальной субстанции и физическую имплементацию эйдоса. 
Как уже постулировалось выше – не существует общепризнанной формализации алгоритма, а наличествуют лишь попытки этой формализации, упорядоченные аппроксимации его природы. Лямбда-исчисление, рекурсивные функции, комбинаторная логика, нормальный алгорифм Маркова, «Машина Поста» и «Машина Тьюринга» – наиболее фундаментальные попытки формализации и они эквивалентны друг другу. Однако уже сама специфика этих формализаций может предоставить достаточно богатую почву для непосредственно философского осмысления проблемы. И то, что самые различные и разнообразно выраженные способы формализации с логико-математической точки зрения представляют собой одно и то же – свидетельствует о многом. Конечно же здесь дело, как и говорилось выше, в фундаментальности природы алгоритма – потому и различные точки его рассмотрения и интерпретирования сводятся к единому основанию.
Но всё же, не претендуя на формализацию алгоритма в контексте метаматематики, а лишь абстрагируя единую суть основных наличествующих попыток это совершить, можно сделать некоторые предположения. Для начала напомним, что мы определяли алгоритм, как строгую последовательность действий по преобразованию данных из одного состояния в другое, выполняемую на некоем вычислительном средстве. То есть абстрактно, алгоритм – это упорядоченная деятельность по преобразованию данных. Данные же – это по сути формализованная информация, которая, в свою очередь, определяется как мера устранённой неопределённости в каком-либо контексте. Со всей очевидностью данные, а точнее их локальное представление и алгоритм должны быть конгруэнтны друг другу в том смысле, в котором ключ конгруэнтен замку или ферменты клеточной мембране. К примеру, алгоритм парсинга JSON-объекта не будет выполняться корректно на XML-объекте, хотя сами данные могут быть одними и теми же. Также отдельно скажем, что здесь мы рассматриваем данные не в качестве оппозиции к ко-данным и не сугубо в виде «пищи для рекурсии», а в общем смысле. И здесь не особенно важна конкретная формализации информации в виде данных – в любом случае основа – информация. Таким образом, можно первично предположить, что информация обладает свойством трансформирования из одного состояния в другое при помощи алгоритма. Однако, как именно это происходит и какова здесь фундаментальная составляющая?
Для примера возьмём число 3. За счёт применения к этому числу различных операций, которые сущностно являют собой атомарные составляющие алгоритма, мы можем получить другие, совершенно различные числа: за счёт возведения в квадрат – 9, в куб – 81, нахождения квадратного корня – 1.7320508075688772, кубического корня – 1.4422495703074083, умножения на 2 – 6, вычисления факториала – 6, вычисления остатка от деления на 2 – 1, и так далее. Данный пример настолько тривиален, что казалось бы – всё и так понятно. И это действительно так при первом взгляде, но – приведём такие же примеры, однако с другими числами и иными операциями. Число 15. Факторизуем его и получим множество размером 2, в которое включены 3 и 5 в качестве простых (простые – то есть не составные и делящиеся только на 1 и самих себя) множителей. К слову сказать, 15 – это на данный момент наибольшее число, которое удалось факторизовать при помощи квантовых алгоритмов, реализованных на соответствующем устройстве. А, допустим, число 123456789000 при факторизации даёт множество размером 5, в которое включены следующие простые числа: 2, 3, 5, 3607 и 3803. Предложенное нами число имеет размер 37 бит, то есть вписывается в диапазон 64-битных чисел и является, в данном контексте, весьма малой величиной. Если же мы возьмём число размером, скажем, 128 бит – то его факторизовать будет уже экспоненциально (в данном случае примерно в два раза) сложнее, а если число будет размером 1024, 2048 или более бит, то задача станет и вовсе, мягко говоря, нетривиальной.
Причём взятая нами для примера проблема – факторизации целых чисел, имеет крайне высокую именно практическую значимость, так как криптосистемы с открытым ключом (подмножество систем ассиметричного шифрования) имеют своим «гарантом безопасности» именно эту проблему – сложности факторизации больших полупростых чисел. И в случае решения этой проблемы над многими финансовыми транзакциями, цифровыми подписями, секретными данными и «всем зашифрованным» – был бы утрачен контроль в глобальном масштабе.
Проблем же, подобных вышеприведённой, весьма много: это и необходимость нахождения сверхбольших простых чисел, и вопрос о конечности множества совершенных чисел, и проблема останова и так далее – перечень весьма обширен. Они могут быть локализованы в различных областях математики, но их все объединяет общая проблема – отсутствие эффективного алгоритма. А у самого алгоритма отсутствует даже должная формализация, то есть проблема не только острая, но и многоуровневая – мы, по сути не знаем, что такое алгоритм, а знаем только каковы его аппроксимации.
Для начала предположим, что отсутствие формализации алгоритма детерминировано недостаточной фундаментальностью представлений и попыток формализации – в первую очередь, а во вторую – отсутствием должного философского обоснования. Здесь же и сразу же выдвинем тезис о том, что именно сущностно и фундаментально алгоритм – это синтез директивы и способа упорядочивания информации, представленной некоторыми информационными блоками. И именно упорядочивания, а не просто обработки или переработки; и не просто директива или способ её выполнения, а именно синтез того и другого – как абстрактная связь между всеми объектами во Вселенной взятыми в пары и числом 2 в десятичной системе счисления, записанным арабскими цифрами – в рамках «одной Вселенной» иначе быть не может.
И именно на способе – вообще безо всякой опоры на директиву – и базировались формализации алгоритма. И предположим, что именно потому они не увенчались успехом – они брали в расчёт только часть того, что «хотели» описать и выразить формально. Задание способа обработки данных и пошаговая реализация этого способа – собственно, это всё что в общем смысле полагается под понятием алгоритма. Нам же представляется, что данное понятие шире по меньшей мере «вдвое».
Поэтому мы хотим показать общий набросок способа формализации алгоритма, построенный в соответствии с нашим вышеприведённым подходом. Итак, ещё раз: алгоритм – это правило и способ упорядочивания неких информационных блоков. Мы полагаем, что реализация этого упорядочивания эквивалента с логико-математической точки зрения имплементации любого алгоритма вообще – в целом. Из нашего определения следует также, что то, что понимается под классическим алгоритмом, являет собой практическую аппроксимацию абстрактного правила упорядочивания. А фундаментальный вопрос теории алгоритмов «можем ли мы вычислить некий Х?», где под Х может подразумеваться любое правильно построенное высказывание на «языке» некоторой формальной системы, преобразуется в однозначное правило сортировки. Таким образом, ответ на вопрос о возможности/невозможности вычисления чего-либо мы получаем в виде существования/не существования однозначного правила сортировки для любой перестановки множества всех возможных состояний вычисляемой системы таким образом, чтобы для неё (для конкретной перестановки) любая последовательность системных переходов являлась подпоследовательностью. Ещё конкретнее: если мы можем задать некоторое правило, в соответствии с которым следует упорядочивать любую случайную перестановку множества всех возможных состояний системы так, чтобы она «трансформировалась» в ту перестановку, для которой все возможные последовательности системных переходов являются подпоследовательностями – мы можем вычислить Х. А иначе – нет.
Возможно наше определение покажется слишком абстрактным или наоборот – чрезмерно узким. Но всё же приведём пример. Допустим, у нас есть конечное множество натуральных чисел на закрытом интервале [1, 1000]. Также у нас есть конечное множество каких-либо алгоритмов с размером 100: то есть, допустим, алгоритм умножения на 2, умножения на 3, деления на 5, возведения в куб и так далее – всего 100. Каждый алгоритм формирует состояние системы, представление которого заключается в сочетании двух элементов: входное значение и выходное значение. Теперь представим в неупорядоченном виде всё множество состояний этой системы, то есть буквально применим все алгоритмы из множества алгоритмов к каждому натуральному числу из множества и перемешаем получившееся множество случайным образом. Далее возьмём один алгоритм из нашего множества алгоритмов и применим его ко всем подмножествам нашего множества чисел. Мы получим множество последовательных системных переходов. Теперь сформируем множество всех перестановок для неупорядоченного множества всех возможных состояний системы. После попробуем найти такую перестановку, для которой, если пронумеровать её элементы от 1 до N, каждая последовательность из множества последовательностей системных переходов, если пронумеровать её от 1 до N, будет составлять подпоследовательность. Для каждого алгоритма из нашего множества алгоритмов должна будет существовать такая перестановка. И та перестановка, которая будет соответствовать нашему требованию, будет упорядочена по абстрактному правилу, вычислительной аппроксимацией которого и является реализованный нами алгоритм.
Наш подход и определение не позиционируются нами как окончательные, а скорее подразумевают указание тех направлений дальнейшей дисциплинарной работы, которые представляются нам целесообразными.

ВЫВОДЫ ПО ГЛАВЕ

В контексте данного раздела нами было рассмотрено понятие алгоритма и те попытки его формализации, которые были предприняты в связи с необходимостью математического описания того, что должна представлять собой «эффективная процедура».
Было показано, что проблема формализации была детерминирована постановкой Давидом Гильбертом так называемой «проблемы разрешения», которая представляет собой задачу верификации высказываний в контексте отдельно взятой формальной системы на предмет истинности или ложности. Вопрос был, соответственно, в том существует ли такая «процедура верификации» в общем виде.
Как мы указали, открытие первой и второй теорем Гёделя свидетельствовало о том, что подобного «общего метода» не существует по крайней мере для системы арифметики. В этой же связи следует упомянуть и о теореме Тарского, согласно которой понятие истины, принадлежащей какой-либо «достаточно сильной» формальной системе, невыразимо средствами этой самой системы. То есть в общем смысле было понятно, что именно «общим», в виде некой «панацеи», подобный метод быть не может.
Однако, как было нами указано, в ходе работы над доказательством или опровержением существования процедуры эффективной верификации высказываний для формальных систем, математическое сообщество вынуждено было задаться вопросом о том, что же вообще из себя представляет «эффективная процедура». И здесь начинаются формализации алгоритма.
В самом начале, для более предметного осмысления различных формализация, мы предложили общее, можно сказать – философское, определение алгоритма, под которым мы подразумеваем обработку неких данных на основе изначально имеющихся «блоков» этих данных с последующей выдачей неких «результирующих» «блоков».
Нами были рассмотрены следующие попытки формализации: рекурсивные функции (К. Гёдель, Ж. Эрбран, С. Клини), λ-исчисление (лямбда-исчисление) А. Чёрча, «Машина Поста» и «Машина Тьюринга», а также комбинаторная логика М. Шейфинкеля и Х. Карри и нормальный алгорифм А. Маркова (НАМ).
Нами были обозначены основные свойства рекурсивных функций: наличие терминального случая, то есть такого состояния входных данных, для которого уже, без дополнительных вычислений, известно выходное значение; вызов функцией самой себя; наличие у функции равных возможностей вне зависимости от уровня вложенности – фундаментальный принцип рекурсивного проектирования и, собственно, основа для вывода, «возврата» значения рекурсивной функцией вообще. В качестве наиболее характерной особенности рекурсивных функций мы представили следующий момент: для того, чтобы рекурсивные функции могли быть успешно применимы для вычисления хоть чего-нибудь, мы должны иметь до известной степени априорное знание некоторых «истин» о исследуемом.
Далее нами было исследовано лямбда-исчисление, на основе представленных особенностей которого мы сделали следующие выводы. Во-первых, также, как и рекурсивные функции, лямбда-исчисление полностью подходит под общее философское определение алгоритма, а также, на основе широты возможностей, целесообразно предположить, что всякую вычислимую функцию можно выразить при помощи формальной системы лямбда-исчисления. Во-вторых, также стоит заметить, что лямбда-исчисление является несколько более «функционально свободным», чем рекурсивные функции по той причине, что здесь подразумевается более общий способ обработки данных, а не только лишь сведение вычисляемого к вычисленному ранее.
После рассмотрения формальной системы комбинаторной логики, нами также было выведено, что на её основе может быть потенциально представлена всякая вычислимая функция.
Также мы рассмотрели НАМ – нормальный алгорифм Маркова и пришли ровно к тем же самым вывода, что и при разборе остальных формализаций – достаточно богатый инструментарий для потенциального представления на основе формальной системы любой вычислимой функции.
Затем мы перешли к исследованию конечных автоматов – «машины Поста» и «машины Тьюринга». Нами были уточнены различия между этими моделями и, собственно, продемонстрирована их логико-математическая эквивалентность.
Эта самая логико-математическая эквивалентность была нами далее уточнена для всех рассматриваемых подходов к формализации. Мы также указали на то, что именно она и резюмируются в тезисе Чёрча-Тьюринга. Эквивалентность означает то, что любая из тех формальных систем, которые были нами рассмотрены, способна, получив в качестве входных данных описание другой формальной системы из числа нами рассмотренных и входные данные для этой системы – полностью воспроизвести алгоритм работы другой системы с её входными данными.
Далее мы перешли к исследованию детерминант отсутствия единой общепринятой формализации и наличию множества эквивалентных формальных систем, которые являют собой аппроксимацию формализации, но не её саму непосредственно. Нами было предположено, что причина этого может быть в более общей, более широкой природе алгоритма. Значение этого предположения в том, что при попытке формализовать алгоритм, рассматривая его просто как упорядоченную последовательность вычислительных действий, часть сущности алгоритма была как бы «сокрыта». Соответствующие пояснения на тему того, что мы предполагаем под этой самой «сокрытой» частью, мы предложили в контексте нового определения. Итак, алгоритм – это синтез директивы и способа упорядочивания реальности, то есть информации о реальности, представленной некоторыми блоками данных. И именно упорядочивания, а не просто обработки или переработки; и не просто директива или способ её выполнения, а именно синтез того и другого. Мы также привели практический пример для нашего определения алгоритма.
Резюмируя, мы подчеркнули, что полагаем наш подход и прилагаемое определение скорее не окончательным завершением исследовательского пути, а указанием направления для дальнейшего успешного продвижения.
 
ГЛАВА 4. ТЕОРЕМЫ ГЁДЕЛЯ О НЕПОЛНОТЕ И ПРОТИВОРЕЧИВОСТИ И ИХ СЛЕДСТВИЯ ДЛЯ ОСНОВАНИЙ МАТЕМАТИКИ

ПРЕДИСЛОВИЕ

Теорема Гёделя – это уже давно нечто большее, чем просто доказанная в контексте математики теорема, это – «притча во языцех». При помощи концептуальной опоры на теорему Гёделя в паранаучных дискурсах доказывается всё, что угодно: от существования Бога до отсутствия интеллекта. Как мы уже многократно упоминали и демонстрировали ранее – на основе понятий с высоким уровнем абстракции и при привлечении «для помощи» феномена самореференции, воспроизведение парадоксов представляется не особенно сложной задачей. Мы здесь будем говорить о первой теореме Гёделя – о противоречивости.
Собственно, зачастую её смысл обобщается до уровня «любая теория ложна», что, кстати, чуть более соответствует смыслу теоремы Тарского о невыразимости истины о формальной системе средствами этой же системы, при определённых условиях, конечно [c]. Само собой разумеется, что из теоремы Тарского ничего подобного не следует – она говорит только о том, что доказывается в её рамках и не более того. Смысл его теоремы действительно более общий, а теоремы Гёделя – очень конкретный и детализированный.
Однако обе теоремы (Тарского о «невыразимости истины» и Гёделя о противоречивости) подразумевают необходимость «присущести» системе некоторых условий. Условий же этих, в самом общем смысле, всего два, но – минимум два: возможность дуального формата представления данных и наличие рекурсивного потенциала, что обеспечивает основу для самореференции в рамках системы и валидность чего, в общем то, как раз и обеспечивается дуальностью системы. Понятно, что мы предоставляем не совсем то определение, которое обычно подразумевают в подобных случаях. Традиционно говорят о том, что формальная система, ввиду смысла теоремы Гёделя, является либо противоречивой, либо неполной в том случае, если она как минимум равна арифметике по «функционалу» и специфике, то есть если она использует операции сложения и умножения натуральных (как минимум) чисел; является консистентной, что подразумевает невозможность доказать в рамках аксиоматики системы одновременно некое утверждение и его отрицание; подразумевает возможность формировать утверждения о натуральных числах, что подразумевает наличие рекурсивных функций; является замкнутой, то есть все утверждения в рамках системы могут быть доказаны или опровергнуты. Мы же подходим к вопросу в несколько более общем смысле (сохраняя постоянную опору на строгие положения теоремы Гёделя), и полагаем, что это имеет свой конструктив. 
Конечно речь идёт именно о формальных системах, а не просто о любой, сколь угодно неспецифицированной системе. Под формальной системой обычно подразумевается множества 5-ти множеств: алфавита, последовательностей символов этого алфавита, правильно построенных высказываний (подмножество предыдущего пункта), формул (подмножество предыдущего пункта), аксиом (подмножество предыдущего пункта) и правил вывода, то есть взаимоотношений между высказываниями. Если отсутствует хоть что-то из названного – речь уже идёт о чём угодно, но не о формальной системе.
И дополнительно, для того, чтобы подпадать под юрисдикцию теоремы Гёделя (да и Тарского также) формальная система должна обладать теми ключевыми характеристиками, которые были обозначены нами выше. Иначе – теорема Гёделя не будет к ней применима. Допустим, теория типов Рассела – является ли формальной системой [c]? Вне всякого сомнения – является. Можем ли мы представить эту систему в дуальном формате, к примеру, применив Гёделевскую нумерацию ко всем её элементам [c]? И это также можно сделать (но уже с нюансами). А можем ли мы применить самореференцию в чистом виде? А вот этого мы уже не можем – иерархия типов Рассела не позволяет этого сделать. И отсюда мы можем вывести, что теория типов Рассела не подпадает под юрисдикцию теоремы Гёделя (и Тарского, соответственно). В контексте системы Рассела невозможны классические самореферентные парадоксы, но у неё другие минусы, которые не остались незамеченными ни самим Расселом, ни его многочисленными коллегами – одна аксиома сводимости вызвала вопросов чуть ли не столько же, сколько аксиома выбора [c на акс сводимости]. И с момента публикации первого издания «Оснований математики», Рассел всю оставшуюся жизнь был занят «латанием дыр» и ответами на обширную критику. По всей видимости, пытаться достичь функционала, сравнимого с рекурсией, без самой рекурсии и без циклов – не особо продуктивно. Приходится выбирать – или достойный уровень возможностей и противоречивость, или же – отсутствие парадоксов, но огромное количество искусственных «костылей».
Фундаментальные причины тотальной противоречивости в контексте любой «более или менее функциональной» формальной системы будут нами обсуждены несколько позднее. Теперь же – непосредственно теорема Гёделя о противоречивости, её представление и доказательства.

4.1. ПРЕДСТАВЛЕНИЕ ДОКАЗАТЕЛЬСТВА ТЕОРЕМЫ ГЁДЕЛЯ О ПРОТИВОРЕЧИВОСТИ И ЕГО СПЕЦИФИКА

Вообще говоря, до того, как Гёдель доказал свою теорему, о которой кстати, уже сформировано огромное количество осмыслений, мнений и соображений, ещё была надежда на то, что кризис оснований математики разрешим конструктивным образом, хотя бы потенциально и может быть в будущем. Однако, с момента выступления Гёделя на конгрессе в Кёнигсберге в 1930-ом году, стало понятно, что создать основания математики такими, какими их хотел видеть Гильберт (считавшийся на тот момент мировым лидером математиков), не представляется возможным. Причина этого нами уже озвучивалась ранее: согласно теореме Гёделя, доказательство которой он как раз и предоставил на конгрессе, любая достаточно богатая формальная система либо противоречива, либо неполна. К слову сказать, согласно второй его теореме – о неполноте, набросок доказательства которой предоставил Гёдель, в любой достаточно богатой формальной системе существует не выводимая формула, содержательно утверждающая непротиворечивость этой самой системы.
Просто для примера, очень упрощённый вариант того, что имеется ввиду под обеими теоремами Гёделя.

Бог Гёделя

«Если человек есть творение Бога по образу его и подобию, то и высказывания человека есть подобие высказываний Бога, а соответственно – они истинны также, как истинны высказывания Бога (примерная логика Лейбница, кстати), ведь утверждается, что Бог – есть Истина. Значит, если человек говорит, что «Бога не существует» – это высказывание истинно потому, что истинны сами высказывания Бога. А это значит, что аксиоматика концепции Бога – противоречива, так как в её рамках высказывание способно отрицать наличие источника высказывания, что можно представить примерно так «2 + 3 = 5 в том и только в том случае, если 2 = 0». Если же признать это высказывание не выводимым по причине неправомерности наших логических шагов, то значит система высказываний Бога – неполна, так как мы неспособны вывести высказывание, несомненно являющееся истинным. А если высказывания Бога либо противоречивы, либо неполны – вопрос о его всемогуществе можно также считать имеющим свой закономерный ответ.
P.S. А теперь очень тонкий момент. Из постулата о всемогуществе Бога мы можем логическим путём вывести то, что Бог «определил» потенциальную возможность противоречивости тех или иных аспектов Мироздания только для того, чтобы проверить силу веры людей непосредственно в него. То есть, мы можем вывести, что Бог предстаёт противоречивым и не-всемогущим для достижения неких определённых целей, но на самом деле, истинно – это и есть доказательство его абсолютной непротиворечивости и полного всемогущества. Приблизительно, как в постулате Тертуллиана – «credo in absurdo»».

Собственно, крайне «колючий» пример, но отдалённо он отражает смысл теорем, который в общем случае сводится к тому, что любая достаточно богатая формальная система либо противоречива, либо неполна. Наш последний, в контексте «Бога Гёделя», логический вывод, как мы покажем по ходу изложения, использует в качестве основы один из тех механизмов, которые далее в нашей работе мы подробно охарактеризуем. Теперь лишь скажем, что этот механизм можно нестрого определить в качестве «латентного удвоения». Конечно, к самому примеру можно придраться практически на каждом его логическом шаге, но повторим, что он – просто тривиальная демонстрация с использованием общеизвестного понятия, а также, при некоторой аксиоматике – он всё же правомерен.
А вот что касается того, как сам Гёдель осуществил эту самую «демонстрацию», которая в его случае была, собственно, строгим доказательством (мы сейчас о первой теореме), то об этом мы скажем далее. Но, также укажем на то, что строгого доказательства теоремы Гёделя мы предоставлять не будем, ограничившись общим наброском – специфика работы всё же не строго математическая.
Говоря о деталях того, что же именно доказал Гёдель, лучше начать несколько издалека, дабы было, сколь это возможно, яснее и понятнее.
Во-первых – дуальный формат представления данных, который в данном случае будет определяться, как Гёделевская нумерация. В данном контексте дуальный формат представления данных не означает их транспиляцию в бинарную систему счисления, а подразумевает возможность представления данных в виде двух равнозначных символьных систем. Проще всего пояснить это на примере индексированного множества натуральных чисел или, апеллируя к программированию – массива (списка, вектора). В рамках этой структуры данных у нас есть некоторое некое множество элементов и равномощное множество индексов: одному элементу соответствует один индекс. Элемент, который содержится в структуре данных (массиве) и позиция массива по индексу – равнозначны в том смысле, что мы можем заменить один другим при выполнении любых возможных операций и результат этих операций не изменится.
Покажем на примере. Допустим, у нас есть массив Х – [1, 2, 3, 4, 5]. Массив содержит 5 элементов, которые являются натуральными числами. В соответствии с принятой в программировании спецификой индексирования, начало полагается с 0. То есть мы можем поставить во взаимно-однозначное соответствие массиву Х массив натуральных чисел Y – [0, 1, 2, 3, 4]. И теперь, записанные на любом языке программирования, логические высказывания «число 1», «первый элемент массива Х», «элемент массива Х с индексом 0» – эквивалентны. Что это означает? Также покажем на примере. Арифметические операции «2 + 3», «второй элемент массива Х + третий элемент массива Х», «элемент массива Х с индексом 1 + элемент массива Х с индексом 2» – это одни и те же операции.
Что это всё значит для нас в контексте теоремы Гёделя? Это очень приближённый пример того, как работает Гёделевская нумерация. Можем представить, что у нас есть не просто 5 натуральных чисел, а некий алфавит. Русский, английский, латинский, случайно сгенерированный – в данном случае совершенно не важно. Главное, что есть некоторый алфавит фиксированного, то есть конечного, размера. Размер определяется, соответственно, количеством букв в алфавите (33 для русского, 26 для английского и так далее). Мы можем пронумеровать все символы алфавита таким же образом, каким нумеровали выше числа в массиве. Нам представляется, что уже должен быть вполне понятен исход этого действа, но на всякий случай покажем на примере двух массивов. Напомним, что индексация начинается с 0. Первый массив будет выглядеть так [м, а, м, а], а второй – [«символ русского алфавита с индексом 13», «символ русского алфавита с индексом 0», «символ русского алфавита с индексом 13», «символ русского алфавита с индексом 0»]. Эти массивы – одинаковы, с точки зрения значения наличествующих в них данных, а различаются они только лишь представлением этих данных.
Итак, здесь зафиксируем первое необходимое наличие – алфавит формальной системы. Однако Гёделевская нумерация совершенно не останавливается на отдельно взятых символах алфавита. И поэтому данный момент теоремы Гёделя обычно обосновывается не на общем понятии биекции, а на специфическом случае этой биекции – изоморфизме. Изоморфизм подразумевает, наряду с возможностью установления взаимно-однозначного соответствия между некими множествами, ещё и структурный аспект, а именно – общность строения структур данных и сохранение в них порядка при устанавливании биекции.
Вообще говоря, определения понятий биекции и изоморфизма, как и разницы между ними, в классических случаях представляются весьма смутными, с точки зрения их восприятия теми, кто не является математиком. Поэтому мы покажем эту разницу на примере. Допустим, у нас есть три множества: Х – {1,2,3}, Y – {3,2,1} и Z – {1,2,3}. Мы можем сказать, что все три множества взаимно-однозначны и равномощны, то есть – биективны. Но мы не можем сказать, что они все изоморфны. Однако множества Х и Z – являются не только биективными, но также и изоморфными, так как одинаковы не только элементы, содержащиеся в этих множествах, но и их порядок. Это один из наиболее простых случаев, но он довольно показателен. Предоставим пример чуть сложнее. Тоже три множества: Х – {1,2,3}, Y – {5, 1, 22} и Z – {44, 59, 100}.  И вновь они все биективны и равномощны, но только множества Х и Z – изоморфны. И изоморфны они потому, что оба упорядочены одинаковым образом – по возрастанию. То есть, все изоморфные структуры также является и биективными, но не все биективные структуры также и изоморфны. Изоморфизм – более узкое и специфичное понятие, являющееся частным случаем биекции. Резюмируем этот фрагмент замечанием о том, что изоморфизм алфавита формальной системы и равномощного множества натуральных чисел в данном случае необходим для того, чтобы два представления одних и тех же данных были одинаково упорядоченными друг относительно друга. И это правило должно соблюдаться.
Как мы сказали выше, Гёделевская нумерация не останавливается на одном только алфавите. Ведь при помощи отдельно взятых символов алфавита можно строить различные высказывания. Но, чтобы их строить, нужно знать и понимать, как именно это делать, то есть нужны некие принципы комбинирования алфавита формальной системы. Собственно говоря, этот момент является достаточно тонким, так как мы не можем однозначно сформулировать эти самые принципы при помощи «языка» непосредственно формальной системы, но это правило имеет исключения для систем, не являющихся строго формальными. К примеру, правила русского языка мы можем записать на русском языке, но они будут понятны только тем, кто уже знает русский язык – «ареол обитания» самореферентных парадоксов, по всей видимости, шире, чем казалось изначально. Мы можем сформулировать правила для русского языка на латыни, но тогда сразу же возникнет следующий вопрос: как мы поймём, каким именно образом их формулировать на латыни? То есть бесконечность и цикличность также всегда «поджидают за углом» и получается, что уже нужны правила следующего языка, за ним следующего и так далее. В этом случае однозначного решения, по крайней мере общепризнанного, для возникшей ситуации не существует. Поэтому обычно просто считается, что принципы комбинирования символов для естественных языков сформировались как бы сами собой «по историческим причинам». Но формальные системы – это феномены искусственные, непосредственно созданные людьми. Поэтому для них подобных проблем не существует, так как принципы комбинирования можно просто сформулировать на языке естественном, а затем, при необходимости, символизировать их на языке формальной системы (как логические кванторы, например). Но обычно этот вопрос решается несколько более формально, при помощи всё той же теории множеств.
С этого момента несколько проще. Мы просто принимаем, как данность, что у нас имеется некое множество последовательностей символов нашего алфавита. Данное множество может быть либо конечным, либо бесконечным, но счётным. Это множество являет собой второе необходимое наличие. На основе него мы уже можем сформировать третье необходимое наличие – множество правильно построенных высказываний или выражений, которые также можно традиционно определить, как формулы. Это множество представляет собой некое подмножество множества всех конечных последовательностей символов. Также не заставит себя ждать и наличие четвёртое – множество аксиом формальной системы, которое, в свою очередь, представляет собой некое подмножество множества формул. О том, что такое аксиомы, зачем они нужны и каково их значение для формальных систем мы уже подробно говорили ранее. Далее, примем как данность и пятое необходимое наличие – правила вывода формальной системы, которые определяют отношения между нашими формулами.
Если же мы в действительности, по какой-либо причине, задумаем создать некую искусственную формальную систему, сформируем для неё произвольный алфавит, и изложим при этом на языке только самой этой системы, критерии того, что такое правильно построенные высказывания, то есть формулы, опишем только на внутреннем языке аксиомы, а также правила вывода системы – то мы просто получим систему, с которой никто не сможет работать. В каком-то смысле, примерно таким стал известный «Кодекс Гигас». Мы не утверждаем, что там изложена некая формальная система, но говорим только о том, что даже просто какой-либо произвольный язык, который не пересекается с чем-либо известным нам, будет совершенно непереводим – и это отнюдь не шифр, так как любой шифр как раз-таки имеет опору в виде реально существующего и используемого языка. Это мы, собственно, к тому, что полностью замкнутой система быть не может до тех пор, пока специфика её организации не будет транслирована на понятном языке и не объяснена соответствующим образом. То есть любая замкнутая система – всегда до известной степени искусственно замкнута (приоткрыта на самом деле), так как изначально она полностью замкнутой быть не может.
В этом контексте интересно то, что в рамках теоремы Тарского говорится о том, что невозможно выразить истину о формальной системе средствами самой системы, также, как и во второй теореме Гёделя, указывается, что в любой достаточно богатой формальной системе существует не выводимая формула, содержательно утверждающая непротиворечивость этой системы. То есть как для разрешения противоречий, так и для утверждения истины о самой системе – нам необходимо выходить на некий мета-уровень по отношению к самой системе, как бы постулировать и/или выводить извне, но в то же самое время на языке самой формальной системы. Обе теоремы доказаны логическим путём. Как утверждал Брауэр, в контексте апологии интуиционизма, диаметрально противореча Расселу с его логицизмом – не математика выводима из логики, а наоборот – логика из математики, а сама же математика, по мнению Брауэра, есть продукт чистой интуиции. Вот мы как раз-таки и предоставили не логическое, а интуитивное доказательство того, что любая формальная система, которая сформирована искусственно, а не «сама собой по историческим причинам», не неполна или противоречива, а, собственно говоря – будучи полностью замкнутой, не имеет никакого внешнего смысла, а напротив – имея хоть какой-то внешний смысл замкнутой быть не может априори. То есть, в целом мы к тому что, формируя систему изначально открытой для взаимодействия с системами иного плана – с естественными языками, например, а затем, уже после того, как формируемой формальной системе был поставлен в соответствие некий смысл, привнесённый извне, требовать от неё того, чтобы она могла сама же формировать смысл о самой же себе – всё же несколько «нелогично». Метафорически выражаясь, это примерно, как если бы мы набрали ведро воды, затем подождали, пока эта вода из него испарится и потом удивлялись тому, что вода в этом ведре не возникает сама – «мы же показали, что вода может быть в ведре», а дальше «оно уже должно само».
Таким образом, резюмируя наше небольшое отступление, мы сформируем некую «интуитивную» (то есть представляющуюся «сама собой понятной») теорему полноты о том, что любая полностью замкнутая формальная система Х – полна и непротиворечива, а также в её рамках выводимо высказывание, содержательно утверждающее истинность и непротиворечивость самой системы Х. Под полностью замкнутой системой Х мы здесь подразумеваем такую систему, для которой во всех высказываниях и операциях, допустимых в рамках Х, отсутствуют любые смыслы и значения, непосредственно не «принадлежащие» Х и не детерминированные её же контекстом. Ещё раз для, хоть и не вполне точного, но всё же примера – «Кодекс Гигас» (по крайней мере – на данный момент так). Собственно, предложенную нами в контексте теоремы полноты формальную систему можно ещё охарактеризовать как обладающую «смыслом Шрёдингера» (по аналогии с «котом Шрёдингера», конечно). И это так потому, что в данном случае внутренний смысл формальной системы находится как бы в суперпозиции – непонятно есть он или нет, но как только он оказывается привнесённым извне в ходе осуществления попытки доказать или опровергнуть нашу теорему полноты – система тут же приобретает некий внешний смысл и перестаёт быть полностью замкнутой, а соответственно, и подпадать под критерий нашей теоремы. В этом смысле теорему полноты невозможно ни доказать, ни опровергнуть.
Вернёмся к сформированным нами ранее множествам формальной системы: алфавит, множество конечных последовательностей символов алфавита, подмножество предыдущего множества в виде множества правильно построенных высказываний, то есть формул, подмножество множества формул, интерпретируемых как аксиомы и множество правил вывода формальной системы. Об отношении между множеством всех конечных последовательностей символов и множеством правильно построенных высказываний мы уже говорили выше, но говорили, имея ввиду любой язык в принципе, а для формальных же систем можно сказать только, что демаркация последовательностей символов на корректные и некорректные каким-то определённым образом подразумевается для формальной системы.
Далее, также важный момент, элементы множества формул, в свою очередь, подразделяются на выводимые из аксиом при помощи правил вывода и не выводимые из аксиом. То есть, в любой формальной системе существуют формулы, которые мы можем вывести из аксиом, последовательно применяя правила вывода к формулам. А существуют такие, к которым мы прийти тем же самым путём, соответственно, не можем – совокупность аксиом и правил вывода нам этого не позволят.
Следующий важный момент: любая формула в рамках формальной системы может иметь некий истинностный статус, то есть быть или истинной, или ложной. Здесь уточним, что мы сейчас рассматриваем те формальные системы, которые подходят под теорему Гёделя о неполноте и, соответственно, организованы по законам бинарной логики. То есть, формальные системы, «функционирующие» на тернарной логике, например, мы здесь не учитываем. Поэтому, любая формула в контексте формальной системы или истинна, или ложна, согласно закону исключённого третьего. Все аксиомы полагаются истинными априори. Почему это так, мы также уже объясняли ранее. Для натуральных чисел, наличествующих в контексте любой рассматриваемой нами формальной системы, применяется стандартная аксиоматика Пеано. Если мы начинаем с некоторых аксиом и осуществляем процесс перехода от одних формул к другим и в конечном счёте получаем некоторую формулу, которая не противоречит аксиомам – она будет считаться истинной, в противном же случае – ложной. Формула, которая выведена из аксиом и которая им не противоречит, то есть полагается истинной, будет определяться как теорема. Таким образом мы можем сформировать ещё одно множество, а именно множество теорем формальной системы.
Исходя из двух предыдущих пунктов, можно вывести определение противоречивой формальной системы. Противоречивая формальная система, это система, в контексте которой, начав с её аксиом и следуя её правилам вывода, мы можем прийти как к некоей определённой формуле, так и к её отрицанию. Система же, в которой подобное невозможно – не противоречива.
Также можно дать определение не выводимого, но истинного высказывания – всё той же формулы. Это формула, к которой невозможно прийти, начав с аксиом и применяя правила вывода к последовательным высказываниям, но которая, тем не менее, является истинной. И здесь мы определяем неполную формальную систему, как систему, в которой существуют высказывания, которые являются истинными, но не выводимыми из аксиом при помощи правил вывода. В полной же формальной системе, соответственно, все истинные высказывания выводимы из аксиом.
Ещё, следуя тому, что мы ранее говорили о требованиях к аксиомам системы, если мы можем, начав с аксиом и применяя правила вывода к последовательным высказываниям, прийти к формуле, которая противоречит аксиомам – аксиоматика системы внутренне противоречива, то есть нарушает единственное значимое требование к аксиоматике. То есть такую аксиоматику формальной системы нужно перерабатывать.
Далее стоит заметить, что подразумевается под «наличествующими в контексте формальной системы» натуральными числами. То есть система же может вовсе и не работать с числами, а создаваться для совершенно иных целей. Но здесь следует вспомнить о том, что мы говорили выше об изоморфизме. Таким образом, если мы можем, на языке нашей формальной системы, представить все аксиомы арифметики натуральных чисел, то наша система будет считаться «достаточно богатой» в контексте теоремы Гёделя и обладать функционалом сравнимым с арифметикой. Здесь можно выразиться более формально: если некоторое семейство подмножеств множеств формальной системы изоморфно семейству множеств формальной системы арифметики натуральных чисел – система «достаточно богата». Менее формально: если мы, на языке формальной системы, можем выразить любое высказывание о натуральных числах, которое может быть построено в контексте арифметики натуральных чисел – система «достаточно богата». То есть для «менее богатых», чем арифметика формальных систем – теорема Гёделя ничего не значит. Колоссальное же значение этой теоремы обусловлено тем, что формальных систем не имеющим изоморфизма с арифметикой в контексте математики и логики практически не существует. Однако исключения всё же имеются, такие как, к примеру пропозициональная логика. Этот достаточно сложно дифференцируемый момент на самом деле является ключевым. Поэтому, к примеру, наша вышеприведённая теорема о полноте не противоречит теореме Гёделя, а в действительности просто не пересекается с ней.
Теперь, более или менее очертив границы формальных систем и первично познакомившись с Гёделевской нумерацией, можем приступить непосредственно к самому действу, а точнее – к нестрогому наброску доказательства теоремы Гёделя о противоречивости.
Итак, для начала сформируем алфавит. Мы будем строить его на основе некоторых логических кванторов, арифметических символов и букв английского алфавита, которые будут означать переменные. Сначала будет в кавычках представляться символ алфавита нашей формальной системы, а затем пояснения к нему и его Гёделевский номер.
•	«⌐» – отрицание. 1.
•	«V» – дизьюнкция (логическое или). 2.
•	«→» – импликация («если, то …»). 3.
•	«∃» – квантор существования. 4.
•	«=» – знак равенства. 5
•	«<» – знак меньше. 6.
•	«>» – знак больше. 7.
•	«0» – ноль. 8.
•	«s» – последователь числа. 9.
•	«(» – открывающая скобка. 10.
•	«)» – закрывающая скобка. 11.
•	«,» – запятая. 12.
•	«+» – знак сложения. 13.
•	«*» – знак умножения. 14.
•	 «x» – переменная. 17.
•	«y» – переменная. 19.
•	«z» – переменная. 23.
•	«∈» – знак принадлежности. 29.
Алфавит нашей формальной системы состоит из 18 символов, каждому из которых присвоен соответствующий Гёделевский номер. Номера эти, как видно, совершенно необязательно должны строго соответствовать натуральным числам, так как главное для нас – соответствующая функциональность. И эта функциональность в нашем примере обеспечивается простыми числами, которые поставлены в соответствие переменным алфавита (и знаку принадлежности). Почему именно простые числа – будет показано далее.
Алфавит у нас теперь есть. Далее мы можем допустить существование множество конечных последовательностей символов, которые мы по понятным причинам буквально представлять не станем. На основе этого множества мы можем сформировать множество правильно построенных высказываний, то есть формул. Какими правилами при этом мы будем руководствоваться? Всё очень просто: мы возьмём за основу формулы арифметики натуральных чисел. То есть, те высказывания, которые будут считаться корректными в контексте арифметики, будут также корректны и в нашей формальной системе. Для нас пока не особенно важны конкретные содержания наших множеств: формул, аксиом, теорем и так далее. Важно показать смысл Гёделевской нумерации, а потому в качестве смысловой основы будем использовать арифметику.
Сформулируем высказывание: «∃х(х<s0)». Это высказывание переводится как «существует такой х, что этот х является меньше, чем последователь нуля». А теперь – самое главное. Как мы уже говорили ранее, Гёделевская нумерация не ограничивается символами алфавита, а распространяется вообще на все элементы всех множеств формальной системы. Предложенное нами высказывание является корректно сформулированным и значит – у него должен быть некий Гёделевский номер. Каким образом его узнать? Для начала мы просто токенизируем высказывание. Затем мы могли бы присвоить каждому элементу соответствующий ему Гёделевский номер и выполнить операцию умножения, но есть один нюанс. Как мы можем гарантировать, что полученное нами число не будет уже занято каким-либо иным элементом (формулой, аксиомой, теоремой). А это гарантирует основная теорема арифметики, которая утверждает, что каждое натуральное число большее 1 может быть представлено в виде произведения простых чисел одним единственным способом.
Попробуем сделать это для нашего высказывания: возьмём первые 8 простых чисел (по количеству токенов в нашей формуле), возведём их в степени соответствующих им Гёделевских номеров и выполним операцию умножения. У нас получится следующее выражение (операцию возведения в степень мы будем для краткости обозначать в виде двойного знака умножения) – «2**1*3**17*5**10*7**17*11**6*13**9*17**8*19**11». Теперь непосредственно выполним операции возведения в степень и умножения. Соответственно, Гёделевский номер нашего высказывания будет равен вот такому, соответственно, числу – 8957468976985414232522031657220278495749776574014493683197555996093750. Само собой, это совсем немаленькое число, однако именно оно и будет являть собой уникальный «идентификатор» нашего высказывания. Также вполне ясно, что, в контексте нашей системы, мы можем присвоить подобный номер любому высказыванию. Примерно так работает Гёделевская нумерация, как специфический метод индексирования.
Теперь же наконец-то перейдём к нашему второму пункту – самореферентность. Выше говорилось о том, что мы можем формировать некие утверждения о самой формальной системе, причём – на языке этой самой системы. Попробуем сделать что-то подобное в нашем случае. Сформулируем высказывание N – «х=х», с Гёделевским номером нашей формальной системы, соответственно – 24300000000000000000. А теперь сформулируем высказывание Z о высказывании N, заявив, что «х ∈ N», что переводится как «х является частью высказывания N» или, если короче и буквальнее, «х принадлежит N». Теперь переведём высказывание Z на язык арифметики и получим примерно следующее «х является множителем N» или, если ещё точнее «17 является простым множителем 24300000000000000000». Гёделевский номер высказывания Z будет рассчитан по формуле – «2**17*3**29*5**24300000000000000000» и, по понятным уже причинам, осуществлять вычисление этого номера мы не станем. Достаточно указать на то, что он существует и является уникальным идентификатором высказывания Z. Для чисто арифметического доказательства мы могли бы осуществить факторизацию Гёделевского номера высказывания N и подтвердить истинность высказывания Z, но пока важнее другое.
Описанную нами выше зависимость между двумя высказываниями, подобную той что была показана на примере высказываний N и Z, мы бы хотели символически обозначить в качестве отдельного наличия, потому что эта связь является значимой в контексте формальной системы, так как позволяет в краткой форме выражать зависимости между высказываниями, чётко идентифицируя их по Гёделевским номерам. Так что эту связь мы представим в виде функции F – F(a, b). Эта функция будет означать такую связь между формулами «а» и «в», которая означает, что формула «а» является иллюстрированием доказательства формулы «в». На прошлом примере это бы выглядело как F(N, Z) или, по Гёделевской нумерации – F(24300000000000000000, «2**17*3**29*5**24300000000000000000»).
Дадим необходимые пояснения по поводу использования натуральных чисел в контексте высказываний на языке нашей формальной системы. В предыдущем примере мы передали в функцию зависимости F конкретный Гёделевский номер, однако мы никак не кодировали числа Гёделевскими номерами при построении алфавита формальной системы. Однако, стоит заметить, что в рамках алфавита наличествуют следующие 2 элемента – число 0 и последователь числа, обозначаемый как s. Поэтому, допустим число 2, в нашей формальной системе будет записываться как ss0, что означает «число, следующее за числом, которое следует за 0» число 4 будет обозначаться как ssss0, что означает «число, следующее за числом, следующее за числом, следующее за числом, которое следует за 0» и так далее. Отсюда видно, что в рамках нашей формальной системы мы можем конструктивно определить любое натуральное число.
Теперь представим следующее высказывание «∃х(х=sy)», что переводится как «существует такой х, который равен последователю y». Мы можем вычислить Гёделевский номер этого высказывания, так как у всех представленных символов выражения имеются свои соответствующие идентификаторы, но условимся просто называть его «q». То есть q – это некое число, которое является Гёделевским номером вышеприведённого высказывания. Теперь мы можем, на основе сформулированного высказывания, ввести ещё одно высказывание, которое будет следующим ««∃х(х=sq)», что примерно переводится как «существует такой х, который является последователем высказывания с Гёделевским номером q». То есть мы взяли наше высказывание «q» и осуществили в нём замену одного символа на сам Гёделевский номер высказывания «q». И Гёделевский номер этого, уже нового высказывания, мы также можем вычислить соответствующим образом. Для этого нам понадобится произвести замену символа «q» на соответствующий ему идентификатор. Таким образом, мы можем определить номер нашего второго высказывания, как «w».
Далее, мы можем определить функцию – D, сигнатура которой будет выглядеть как D(h, 19, h). Эта функция будет работать примерно таким же образом, каким выше мы «вручную» заменяли один символ в некоем высказывании на иной. То есть, функция D берёт высказывание с Гёделевским номером «h» и заменяет число 19, которое имеется в высказывании с номером «h», на сам идентификатор высказывания «h». То есть мы можем применить функцию D следующим образом «D(q, 19, q)» и получить по итогу высказывание с Гёделевским номером w, которое выше было нами сформировано «вручную». Именно число 19 взято просто в качестве примера – можно взять любое соответствующее. На данном этапе уже появляется настоящая рекурсия в своём аутентичном виде.
Теперь попробуем повторить шаги самого Гёделя. Построим следующую формулу «(∃х)F(x, z)». На всякий случай напомним, что F – это обозначение функции отношения между формулами, которую мы ввели ранее. Эта формула переводится так: «существует такая формула с Гёделевским номером х, что эта формула является доказательством формулы с Гёделевским номером z, соответственно, z является выводимой формулой». Также запишем отрицание этой формулы «⌐(∃х)F(x, z)», что будет, соответственно, переводится как : «не существует такой формулы с Гёделевским номером х, что эта формула является доказательством формулы с Гёделевским номером z, соответственно, z не является выводимой формулой».
Попробуем с этим поработать. Построим формулу «⌐(∃х) F(x, D(y, 19, y))». Напомним, что D – это функция, которая заменяет в формуле под Гёделевским номером «y» все элементы с номером 19 на элементы с номером «y». Приведённая формула означает, что «не существует такой формулы с Гёделевским номером х, которая является доказательством формулы D(y, 19, y), соответственно, D(y, 19, y) – не выводима». Само собой разумеется, что для этой формулы также существует некоторый Гёделевский номер, который мы можем обозначить, как N. И таким образом мы получаем ещё одну формулу, которую представим под Гёделевским номером G – «⌐(∃х) F(x, D(N, 19, N))», которая говорит о том, что «внутренняя» формула «D(N, 19, N)» – не выводима.
И вот здесь наступает интересный момент. У «внутренней» формулы ведь также должен быть свой Гёделевский номер. Чему он должен будет соответствовать? Если мы внимательно разберём формулу, то заметим, что, как для формулы под Гёделевским номером N, так и для «внутренней» формулы, ввиду рекурсивности последней, подразумеваются ровно одни и те же действия. Получается, что и Гёделевские номера у них будут, соответственно, одни и те же. А это значит, что наша изначальная формула G теперь по сути говорит о том, что «неверно, что формула G выводима».
То есть получается, что в контексте формальной системы у нас есть формула, которая утверждает, что она же сама не является выводимой. Таким образом уже можно сделать первичный вывод о том, что система явно противоречива.
И, собственно, таким же рекурсивным образом затем выводятся и формула, и её отрицание и так далее. Но нам здесь хватит и уже приведённого, дабы ещё глубже не проникать в дебри формульных построений. Мы показали лишь один фрагмент доказательства Гёделя, и то – в виде нестрогого наброска. Однако полагаем, что даже таким образом сумели продемонстрировать его специфику и общий смысл. Главное, что необходимо зафиксировать – это рекурсивная специфика доказательства. То есть основание доказательства заключается именно в самореферентности и мы это только что продемонстрировали на достаточном уровне детализации. Теперь наши дальнейшие умозаключения уже не будут выглядеть голословными.

ВЫВОДЫ ПО ГЛАВЕ

Здесь скажем, что в арифметике натуральных чисел весь смысловой спектр теоремы Гёделя резюмируется в одном уравнении – «х=1-х». Это уравнение не имеет решения в системе натуральных чисел. Конечно, уже в системе чисел рациональных решение очевидно, но вот в натуральных – уравнение нерешаемо. И после ознакомления со всем тем, что мы продемонстрировали в данной части нашей работы, это уравнение действительно кажется неплохой иллюстрацией.
Мы здесь говорили о теоремах Гёделя о неполноте и противоречивости. Было указано, что на том самом конгрессе, на котором мир впервые услышал о теоремах Гёделя, к теореме о непротиворечивости было предоставлено строгое доказательство, а о неполноте – набросок. Мы же в данной части нашей работы предоставили ещё более общий набросок, но не одной из теорем Гёделя, а скорее их несколько расширенного смысла. Нам здесь было важно не доказать что-то, а именно объяснить и как можно детальнее проиллюстрировать специфику доказательства Гёделя, сделав особый акцент на неотъемлемость таких ключевых характеристик, как дуальный формат представления данных и самореферентность (рекурсия).
Тем не менее, даже в столь общем смысле, мы достаточно подробно прорисовали некоторое множество нюансов и мелких деталей. Достаточное для того, чтобы действительно реализовать один из шагов доказательства Гёделя и показать, как одно и то же высказывание может утверждать диаметрально противоположные вещи.
Изначально мы показали это на метафоре «Бог Гёделя». В общем то, на этом можно было бы и остановиться – смысл продемонстрирован и достаточно определённо очерчен. Но мы хотели показать не только «гуманитарную» сторону вопроса, но и «точно-научную» также. Поэтому занялись формированием основы для подобной демонстрации.
Изначально мы указали на необходимые наличия для вообще формальной системы: алфавит, множество конечных последовательностей символов, формул, аксиом и правил вывода. Также было указано на общие необходимости для системы: дуальный формат представления данных и возможность воспроизведения рекурсии.
Было показано, что дуальный формат воспроизведения данных в этом случае будет обеспечиваться Гёделевской нумерацией, смысл которой нами был подробно раскрыт и показан на примерах. Далее мы сформировали алфавит формальной системы, взяв за основу некоторые логические кванторы, математические символы и переменные. Алфавиту было поставлено во взаимно-однозначное соответствие множество натуральных чисел в контексте осуществления Гёделевской нумерации. Затем мы, путём составления различных формул пришли к финальной формуле, которая была самореферентной, и, как оказалось, противоречит сама себе, эквивалентно – противоречит своему Гёделевскому номеру, если совсем точно. Тем самым мы выполнили свою задачу в этой части, пройдя «путём Гёделя».
Ещё мы, в качестве небольшого отступления, предложили теорему полноты, которая гласит, что любая полностью замкнутая формальная система – полна и непротиворечива, а также в её рамках выводимо высказывание, содержательно утверждающее истинность и непротиворечивость самой этой системы. Под полностью замкнутой системой мы же имели ввиду такую систему, для которой во всех высказываниях и операциях, допустимых в её рамках, отсутствуют любые смыслы и значения, непосредственно не «принадлежащие» самой системе и не детерминированные её же контекстом. Мы охарактеризовали полностью замкнутую формальную систему, как обладающую «смыслом Шрёдингера». Это было объяснено тем, что в данном случае внутренний смысл формальной системы находится как бы в суперпозиции – непонятно есть он или нет, но как только он оказывается привнесённым извне в ходе осуществления попытки доказать или опровергнуть нашу теорему полноты – система тут же приобретает некий внешний смысл и перестаёт быть полностью замкнутой, а соответственно, и подпадать под критерий нашей теоремы. Было указано, что в этом смысле теорему полноты невозможно ни доказать, ни опровергнуть.
 
ГЛАВА 5. ФИЛОСОФСКИЕ ТЕОРЕМЫ: АКСИОМАТИКА ТЕОРИИ МНОЖЕСТВ, ЧИСЛА, ДОКАЗАТЕЛЬСТВА, РЕКУРСИЯ

ПРЕДИСЛОВИЕ

В данной части нашего исследования мы представим наши собственные соображения, касаемо большинства тех ключевых аспектов, специфических особенностей и некоторых неоднозначных моментов, которые были охарактеризованы ранее. В частности, будут исследованы некоторые наиболее значимые свойства и характерные составляющие теории множеств. В особенности же мы будем заниматься понятием бесконечности и её использованием в рамках данного раздела математики; ну и конечно, так как сама система вещественных чисел имеет в качестве своего основания теорию множеств – вещественным числам в контексте данной теории мы выделим достойную часть нашего времени. Также мы уделим немалое внимание математической логике и тем её особенностям, которые были эксплицированы в контексте доказательства теоремы Курта Гёделя о неполноте. Мы покажем, что в области логики наличествуют некоторые механизмы, которые не только способны «помочь» в формировании парадоксов, но также и представляют собой нечто гораздо большее – а именно фундаментальные принципы функционирования бытия вообще. Этому моменту – генеалогии и базису возникновения как чисел и множеств, так и гораздо более сложных феноменов – мы также уделим достойную долю осмысления.
Ещё мы хотим сказать о том, что актуальная часть изложения будет нами представлена в не совсем классическом виде. Учитывая тематику текущего исследования и вообще контекст нашей работы, нам представляется наиболее целесообразным предоставить материал в несколько формализованном виде, а именно – в виде философских теорем. Мы, конечно же, имеем представление о том, что эта форма не является стандартной для философского исследования, исключая, возможно, «Этику» Бенедикта Спинозы. Также мы в курсе, что самого понятия «философской теоремы» ранее не наличествовало в контексте научного познания. Однако то, что собираемся продемонстрировать мы – непосредственно этим и является – философскими теоремами, по крайней мере именно это определение представляется наиболее корректным для репрезентируемой нами формы изложения частей данного раздела. Лишь последняя часть раздела будет приведена в более свободном, но всё же не совсем классическом виде.
Отличие же философской теоремы от непосредственно классического понятия теоремы заключается в более мягких требованиях к логической генеалогии (выводимости) и, собственно, форме. А точнее в том, что философская теорема не обязательно является прямым следствием применения правил вывода формальной системы к аксиомам этой системы. Она может как быть непосредственным выводом из аксиом, так и не быть им, то есть может или выводиться, или постулироваться. Также не подразумевается обязательным высочайший уровень математической строгости – теоремы всё-таки именно философские. В остальном особенных отличий нет. Форма достаточно вариативна, но всё же обязательно подразумевает следующие пункты: введение в конкретную проблему; основания, на которых строится логическое рассуждение и доказательства; сама непосредственно теорема, как некоторое высказывание, которое доказывается или опровергается; доказательство, как логическая цепочка последовательных высказываний; выводы по итогу доказательства; заключение, как подведение общего итога для всей философской теоремы. Ещё может подразумеваться необязательный пункт – дополнения, которые мы будем, при необходимости, предоставлять под символическим обозначением «P.S.», и в которые могут входить некоторые соображения, пояснения и умозаключения, также имеющие отношение к теме конкретной теоремы, но не вошедшие в основную часть. Не является противоречием, если вдруг в контексте дополнений обнаруживаются некоторые аспекты, которые могут, в сущности, опровергать предыдущие выводы и ставить на их место выводы уже новые – повторим ещё раз, что теоремы именно философские и порой могут демонстрировать некий желаемый смысл одной лишь своей формой, с косвенной опорой на содержание. С подобной формой мы уже сталкивались ранее, в разделе о понятии бесконечности, а конкретно в отрывке «О времени и бесконечности», а также в последующем осмыслении апории Зенона о стреле. Так что особенных неожиданностей возникнуть не должно.
Ещё мы видим целесообразность в том, чтобы предварительно уточнить то, что здесь мы не претендуем на окончательное и строгое математическое обоснование тех выводов, которые будут нами приведены. Философская теорема, согласно нашему видению, подразумевает несколько более широкий и существенно менее строгий формат в отличие от теорем непосредственно математических. Однако всё же, учитывая и логико-математическую тематику, и способ подачи материала в виде философских теорем, считаем, что те заключения, которые будут нами приведены, имеют право на рассмотрение не только с философской, но также и с математической точки зрения. Пусть даже и без, как мы и сказали, претензии на абсолютную строгость и убедительную завершённость. Примерно таким нам и видится предназначение философских теорем – они потенциально способны формировать основу для дальнейшего построения теорем уже сугубо математических и логических.
С философской же точки зрения, мы «дополнительно» верифицируем те логико-математические основы, которые и без того являются, в соответствии с нашим видением ситуации, достаточно «зыбкими» и ставим вопросы в тех местах, которые нам представляются менее всего для этого «подготовленными». Подобный подход призван не только вскрыть тот самый «лес за деревьями», который нам представляется невозбранно наличествующим, но также и обеспечить достаточно надёжную логическую и философско-методологическую основу для построения здания той концепции, к формированию которой ведёт общий дискурс нашего исследования.
Дополнительно скажем, что фрагменты данной части нашей работы не обязательно будут логически связаны друг с другом, если только мы не предоставим на это дополнительных указаний. Более того, они будут до известной степени изолированы в концептуальном плане и то, что выводится в одном фрагменте, может прямо противоречить тому, что выводится в другом просто за счёт вывода «от противного». Эта особенность обусловлена спецификой отправной точки для построения логических цепочек. 

5.1. ТЕОРЕМА ОБ УПОРЯДОЧЕННОСТИ И АКСИОМЕ ВЫБОРА

Описание. В данной теореме мы рассмотрим выбор элементов из различных множеств и некоторые затруднения, которые могут быть связаны с этой операцией.
Напомним определение аксиомы выбора: для любого семейства W непустых множеств q может быть определена функция выбора, которая ставит в соответствие каждому q элемент z, такой, что z -> q. Мы рассматривали эту аксиому и её положение в теории множеств ранее, в частности указав на то, что применение этой аксиомы сопряжено с некоторыми затруднениями в контексте определения функции выбора на бесконечных несчётных множествах. Однако, в рамках данной теоремы мы хотим показать ещё одно затруднение (которое представляется нам совершенно очевидным) в определении функции выбора на множествах с размером от 2-х элементов и выше – то есть даже на конечных множествах.
Здесь же скажем немного о порядке вообще и о том, что под ним подразумевается в контексте теории множеств. Порядок в теории множеств – это способ организации элементов множества, который позволяет установить между ними бинарные отношения: больше, меньше или равно. Частичный порядок означает, что отношение порядка может быть определено для некоторых элементов множества, но не для всех. А полный порядок, который зачастую определяется как «вполне упорядоченность», подразумевает определение бинарного отношения порядка для всех элементов множества. Для удобства определим отношение порядка как R (собственно, таким же образом оно определяется и в классическом случае).
Отношение порядка отличается некоторыми специфическими качествами или же их противоположностями, такими как: рефлексивность – способность отношения порядка быть однозначно определенным на некоем элементе по отношению его к самому же себе, то есть хRх (некоторые объекты им не обладают, к примеру, NaN в языке программирования JavаScript); симметричность – отношение порядка, при котором для двух элементов х и у является одинаковым отношение хRу и уRх; транзитивность – отношение порядка для множеств от 3-х и более элементов, для которых может быть определена логическая импликация «хRу и уRz -> хRz».
Для данного случая нам также представляется целесообразным привести свои определения порядка и упорядоченных множеств.
Итак, множество размером 2 и более элемента X может называться частично упорядоченным, если соблюдаются следующие условия: для любого х, такого что х -> X, существует хотя бы один у, такой что у -> X и для элементов х и у определено отношение хRу. То есть, для любого элемента, входящего в множество, существует хотя бы один элемент этого же множества, для которого мы можем точно сказать, больше, меньше или равен он нашему исходному элементу. Наше отношение порядка подразумевает рефлексивность, антисиммметричность и транзитивность.
Можно также сказать, что для вполне упорядоченных множеств также подразумевается ещё и сравнимость любых двух элементов, что превращает наше изначальное определение в следующее. Множество X может называться вполне упорядоченным, если соблюдаются следующие условия: для любых х и у, таких что х -> X и у -> X определено отношение хRу.
Здесь нам представляется целесообразным ввести ещё одно определение, которое в отличие от двух предыдущих, уже совсем не пересекается с классическими подходами к упорядоченности вообще – как философскому понятию и к отношению порядка для множеств – в частности. Мы говорим о понятии «случайного порядка». Само понятие несёт на себе яркую печать оксюморона и напоминает «горячий снег», служа «красной тряпкой» для дискуссии, и в первую очередь – дискуссии философского характера. Причина в том, что случайность, как манифестация хаоса, представляет собой антипод упорядоченности. И чтобы сводить эти противоположности в единое определение нужны очень веские основания. Мы полагаем, что они у нас как раз-таки имеются.
Приведём достаточно тривиальное утверждение: не существует способа однозначно отличить случайное от упорядоченного. Это утверждение логически следует из того факта, что согласно теории вероятности, вероятность (извиняемся за тафтологию) генерирования случайным образом упорядоченной последовательности не является нулевой. То есть мы можем, при помощи генератора случайных чисел, получить упорядоченную последовательность. Из этого следует, что наше изначальное высказывание правомерно. Допустим, последовательность чисел 1, 2, 3, 4, 5 – среднестатистический человек признает упорядоченной и имеющей смысл. В смысле же последовательности Трибоначчи (одна из модификаций последовательности Фибоначчи) 1, 1, 1, 3, 5, 7, 9, 17, 31, 57, 105, 193, 355, 693 – быстро разберётся далеко не каждый, а последовательность «счастливых чисел Эйлера» 41, 42, 47, 53, 61, 71, 83 и так далее – по началу кажется и вовсе лишённой всякого смысла. И любая из представленных последовательностей потенциально способна быть полученной совершенно случайным образом. Если пойти далее по этой логической цепочке, то, минуя смысл «10-ти тысяч обезьян с пишущими машинками», можно довольно быстро прийти к тому, что последовательность представляется упорядоченной тогда, когда известен способ её упорядочивания/генерирования. Это в свою очередь приводит нас к понятию «Колмогоровской сложности», принуждает провести аналогию между сложностью и случайностью с одной стороны, и упорядоченностью с «простотой» – с другой. Отсюда хорошо виден и следующий логический шаг, который заключается в определении упорядоченности, как свойства феномена быть описанным полиномиально, то есть в виде некоего уравнения или функции; а неупорядоченности и случайности – как невозможности феномена иметь полиномиальное описание.
Однако, так как случайно сгенерированный феномен также может иметь полиномиальное описание, то этот критерий демаркации между случайностью и упорядоченностью – не вполне корректен. Более того, при попытке определить некий строгий критерий чистого порядка, который не может быть получен случайным образом, мы вынуждены будем очень скоро прийти к тому, что такого критерия не существует в принципе. Если бы не это, то философские и теологические дискуссии о том, случаен или закономерен человек во Вселенной и вообще сама Вселенная – давно бы были завершены. Но, ввиду отсутствия чёткого критерия – тема очень даже живая.
Исходя из вышесказанного, мы утверждаем то, что высказывание о том, что случайно сгенерированная последовательность или случайным образом сформированное множество – являются упорядоченными случайным образом. То есть, чистый хаос, как трансцендентальный потенциал, при соприкосновении с реальностью перестаёт быть, собственно, самим собой и становится в известной мере упорядоченным. Мы можем посчитать вероятности того или иного события или последовательности событий, но результаты подсчёта «истинны» только для актуально бесконечных данностей – для бытия mundus, но не для нашей реальности. Так как в контексте как раз-таки нашей реальности – результаты любого события или последовательности событий являют собой оформленную реальность, то есть «хаос, редуцированный к упорядоченному» или «случайность, сложившаяся в порядок». И это так в абсолютно любом случае – даже, если феномен представляется имеющим совершенно стохастическую природу – если он реален, то есть существует, то он упорядочен каким-то конкретным образом.
Поэтому мы полагаем наше понятие случайного порядка – валидным и непротиворечивым. Для наглядности приведём пример. Допустим, у нас есть естественным образом упорядоченное множество натуральных чисел на закрытом диапазоне [1, 3] – то есть множество, которому принадлежат натуральные числа 1, 2 и 3. Изначально заявляется, что оно упорядочено естественным образом, то есть – по возрастанию и, следовательно – оно выглядит так – {1, 2, 3}. Переупорядочим его случайным образом, и мы получим: например, такое множество {2, 3, 1}; или такое {3, 1, 2}; а, возможно такое {1, 2, 3}; может быть и такое {3, 2, 1}; и так далее. Но – строго какое-то конкретное. И вот именно конкретный его вид в некоторой отдельно взятой ситуации и будет представлять собой результат случайного упорядочивания. Но неупорядоченным оно представлено быть не может.

Формулы (высказывания). Сформулируем два высказывания, которые являются корректными с точки зрения аксиоматики теории множеств Цермело-Френкеля с аксиомой выбора. И, кстати, в данном случае высказывания будут корректны для аксиоматики даже в случае её рассмотрения без аксиомы выбора, так как её отсутствие не означает того, что мы вообще ничего выбрать не способны, а лишь свидетельствует о некоторых ограничениях в случае работы с бесконечными несчётными множествами.
Формула 1. Для любого непустого конечного частично упорядоченного множества Х размером 2 и более может быть определена функция выбора F, которая сопоставляет множеству X элемент х, такой, что x -> X.
Формула 2. Для любого непустого конечного частично упорядоченного множества X размером 2 и более есть хотя бы 2 элемента y и z, таких что y -> X и z -> X, для которых не определено отношение порядка. Если это не так, то множество будет вполне упорядоченным.

Утверждение 1. Хотя бы одна из формул – неверна.
Утверждение 2. Если неверно утверждение 1, то функция выбора может быть определена для любого множества, в том числе несчётного и бесконечного, а значит – аксиома выбора правомерна для любого множества.
Утверждение 3. Хотя бы одно из утверждений 1 и 2 – верно.
Утверждение 4. Выбор элемента из любого множества подразумевает предварительное упорядочивание этого множества.

Доказательство. Так, у нас есть два конечных множества размером более 2: вполне упорядоченное множество А, частично упорядоченное множество В.
Для простоты и наглядности можем предположить, что для любых двух элементов x и y, таких что х -> В и y -> В, не определено отношение порядка.
Мы хотим определить функцию выбора F для множеств А и В. Для множества А функция F выбирает наименьший, наибольший или любым иным образом определённый отношением порядка элемент из А. Теперь с множеством В. Мы могли бы попробовать выбрать наименьший или наибольший элемент множества В, но там нет ни наименьшего, ни наибольшего; мы могли бы попробовать выбрать первый или последний элемент множества В, но там нет ни «первого», ни «последнего» элементов. И так далее – по любым бинарным отношениям.
И мы не можем выбрать ни одного элемента, если не определено отношение порядка для элементов множества В. А значит – неверна формула 1.
Тогда попробуем «просто выбрать» «случайный» элемент. Но «просто случайно» выбрать мы тоже не можем. Так как случайность подразумевает некоторое распределение вероятностей – абсолютно равное в данном случае, так как для множества В не определено отношение порядка, соответственно – актуализирован принцип безразличия. А если распределение вероятностей равное – мы не можем выбрать ни одного элемента из множества В и вновь неверна формула 1.
Если же выбор случится, то значит, хотя бы один элемент был «оценён» выше или ниже любого другого элемента, следовательно, отношение порядка также было определено, и определено оно было на основе вероятности, а значит, что множество было частично упорядочено перед осуществлением выбора, следовательно, неверна формула 2.
Для наглядности приведём конкретный пример: множество А – {1, 2, 3} и множество В – {{}, «авс», 57}. Из множества А мы можем выбрать наименьший – 1 или наибольший – 5 элемент, так как на множестве определено «естественное» отношение порядка по возрастанию натурального ряда. Также нам совсем не обязательно выбирать элементы именно по этому критерию – мы можем выбрать и случайно тоже, но тогда на множестве должно стать определённым отношение «случайного порядка», то есть оно должно быть упорядочено случайным образом – так тоже можно.
Теперь, можем ли мы выбрать элемент из множества В? А если можем, то какой? Наименьший, наибольший? Может быть средний? На самом деле здесь представлена ситуация «Буриданова осла», которую можно резюмировать следующим образом: если абсолютно всё равно куда идти, то никуда идти и не получится. Соответственно, для того, чтобы сделать выбор – нужно расставить приоритеты. А для расстановки приоритетов необходим некий чёткий критерий. Для данного случая это означает унификацию элементов множества по какому-либо признаку (хотя бы по случайному распределению) с последующим определением порядка по этому самому признаку. Допустим, первым выберем из множества В элемент {}. Это, просто для примера, будет означать унификацию элементов множества В по мощности и выбор наименьшего элемента. Далее представим, что первым мы выбрали элемент «авс», что в качестве возможных примеров может означать определение порядка по: сумме индексов символов в алфавите, сумме значений символов в формате ASCII и так далее с выбором наибольшего элемента множества В. Конечно же возможен и случайный выбор, но тогда за каждым элементом окажется закреплена вероятность его выбора, выраженная количественно, то есть – упорядоченность также будет осуществлена.
Таким образом, повторим ещё раз – из множества размером 2 и более могут быть выбраны только те элементы, для которых определено отношение порядка. Более того – изначально неупорядоченное множество упорядочивается в процессе осуществления выбора из него элементов по одному за раз.  То есть, если множество вовсе не упорядочено, то выбор первого элемента может быть осуществлён согласно отношению «случайного порядка», но, как только получен хоть один элемент, мы можем либо выбрать следующий больший или меньший по отношению к нему элемент, руководствуясь при этом любым возможным критерием сравнения, либо – при актуальном отсутствии такого элемента по отношению к случайно выбранному, просто продолжить упорядочивать множество в случайном порядке.
Но порядок будет в любом случае, если подразумевается выбор вообще – неупорядоченным может являться только такое множество, для которого не подразумевается истинной аксиома выбора и которое, в сущности, не может быть представлено ни в каком виде вообще.

Вывод 1. Утверждение 1 – верно.
Вывод 2. Если же, несмотря на очевидную логическую противоречивость подобного «выбора», подразумевать возможность «просто выбрать любой элемент», то значит, утверждение 1 – неверно, а утверждение 2 – верно.

Заключение. Утверждения 3 и 4 верны, что и требовалось доказать.

P.S. Покажем следствия нашего доказательства для основных 5 аксиом из системы аксиом Цермело-Френкеля. Предварительно будет приводить определения каждой из аксиом, хоть они уже и приводились ранее.
Аксиома экстенсиональности. Для любых 2-х множеств, если каждый элемент 1-ого принадлежит 2-му, а также каждый элемент 2-ого принадлежит 1-му, то 1-е множество идентично 2-му.
Очевидно, что ввиду нашего доказательства невозможности выбора элементов из неупорядоченного множества аксиома экстенсиональности будет истинна только для вполне упорядоченных множеств, причём как во внутреннем смысле, так и во внешнем. То есть, если у нас есть 2 различных множества А и В, то для того, чтобы мы в принципе могли выбрать хотя бы 1 элемент, например, из А, то А должно быть вполне упорядочено. Но допустим, что множество А упорядоченно, а В – не упорядочено. Из этого следует, что мы не можем их сравнить, так как не можем выбрать для сравнения элемент из множества В. Если же элементы множеств равны, то на данном этапе это не создаёт проблемы.
Аксиома объединения. Из любого семейства Х множеств Y можно образовать такое множество Z, каждый элемент z которого принадлежит хотя бы одному множеству Y семейства множеств Х.
Вновь очевидно, что для объединения множеств А и В каждое из них должно быть вполне упорядоченным. Однако, в данном случае наличествует ещё один аспект. А именно: если есть хотя бы один элемент a множества А и, хотя бы один элемент b множества В, такие что на множестве {a, b} не может быть определена функция упорядочивания элементов друг относительно друга, то множества А и В – не могут быть объединены.
Аксиома степени (аксиома булеана). Для любого множества Х существует множество Y, которое является множеством всех подмножеств Х.
Представляется закономерным, что аксиома истинна только для вполне упорядоченных множеств.
Аксиома регулярности. Для любого непустого множества Х существует х, такой что х -> X, и пересечение х с Х равно пустому множеству.
Также видно, что верификация истинности для этой аксиомы может быть осуществлена только для вполне упорядоченных множеств.
Аксиома бесконечности. Существует множество Х, такое что для каждого х -> X, объединение х с х – также принадлежит Х. Эта аксиома позволяет строить бесконечные множества на основе пустого множества. И для последовательного добавления элементов по одному за раз проблем, даже ввиду нашего доказательства, не возникает. При попытке добавить элемент, который уже содержится в множестве, функция выбора легко может расставить приоритеты. Но тут же следует, что добавить элемент, для которого не определено отношение «следует за» или «следует перед» – мы не можем, так как для него не определена «ячейка». Само собой разумеется, что эта проблема легко решаема в том случае, если при невозможности определения отношения порядка для следующего элемента с хотя бы одним элементом из уже наличествующих во множестве, будет определено отношение «по очерёдности добавления». Однако тогда будет получено множество, упорядоченное сразу как минимум 2-мя разными способами. Однако, если для построения любого множества «просто» заключить, что конкретный порядок не важен и определить отношение порядка «по очерёдности добавления» или «вероятностно», то проблема просто снимается. Но вот сам порядок, в том или ином виде – наличествует всегда.

P.S.2. По итогу получается так: либо аксиома выбора верна и неупорядоченных множеств не существует, либо неупорядоченные множества существуют, не могут быть представлены ни в каком возможном виде и аксиома выбора ошибочна. Наша теорема, в некотором роде, существенным образом сближает теорию множеств с конструктивистским подходом в математике, но, как это ни странно – только теорию множеств, основанную на аксиоматике Цермело-Френкеля с аксиомой выбора. Это потому, что, основываясь на нашей теореме, определение функции выбора эквивалентно упорядочиванию «универсума» (множества всех множеств вообще и каждого его элемента – в частности) и (ключевой момент) утверждению существования любого множества в виде конструирования через упорядочивание.

P.S.3. Выше мы ни разу не упомянули о теореме Цермело. Теперь же считаем целесообразным это сделать. В сущности, теорема Цермело, согласно которой на любом множестве можно ввести такое отношение порядка, что множество станет вполне упорядоченным, говорит ровно о том же самом, о чём и мы в рамках нашей теоремы. Интересно, что именно теорема Цермело послужила стартовой точкой для дальнейшей аксиоматизации теории множеств. Другое дело, что сам Цермело при доказательстве своей теоремы впервые явным образом использовал аксиому выбора. На данный момент теорема Цермело признаётся эквивалентной аксиоме выбора – теорема может быть доказана (и была доказана) в рамках аксиоматики теории множеств Цермело-Френкеля с аксиомой выбора. То есть, по большому счёту, мы особо ничего нового не внесли в общий контекст – контекст аксиоматической теории множеств с аксиомой выбора. Однако, мы сформировали свою собственную аргументацию не просто так, а с неким стратегическим замыслом. И смысловая интенция здесь следующая: любое существующее множество является упорядоченным. То есть здесь идёт речь о том, что только лишь в рамках применения аксиомы выбора мы оперируем структурами, которые могут быть признаны существующими, а без неё – с эфемерными, нечёткими и не имеющими отражения в «реальности» объектами. Обычно в математике полагается ровно наоборот, но мы говорим именно о конструктивном определении функции выбора на любом множестве, то есть – о введении порядка и конструировании через упорядочивание.

5.2. ТЕОРЕМА О НЕПРЕРЫВНОЙ СЛУЧАЙНОЙ ВЕЛИЧИНЕ

Описание. Данная наша теорема очевидным образом пересекается с «теоремой об упорядоченности», которая была представлена предыдущей. Непрерывная случайная величина – это некоторая величина (здесь не совсем корректно употреблять понятие «число»), которая может принимать любое значение (вот здесь – именно число) из определенного диапазона на числовой оси. В отличие от дискретной, то есть прерывной, случайной величины, которая может принимать, соответственно, точные и конкретные значения, непрерывные случайные величины могут иметь бесконечное множество значений в пределах некоторого диапазона. В случае с непрерывной случайной величиной несколько стирается грань между счётными и несчётными бесконечными множествами, то есть в данном контексте нет особой разницы, с чем именно имеется дело – с множеством всех рациональных чисел или множеством всех вещественных, ведь и то и другое множества подразумеваются актуально бесконечными и именно это тут является определяющим фактором.

Формула 1. Согласно принципу безразличия, при недостаточном основании для выбора какого-либо определённого элемента, вероятности выбора отдельно взятого элемента для каждого элемента множества строго равны.
Формула 2. Для непрерывной случайной величины вероятность того, что она примет какое-либо конкретное значение из диапазона равна нулю.
Формула 3. Вероятность того, что непрерывная случайная величина, при актуализации принципа безразличия, примет какое-либо конкретное значение из числового диапазона является обратно пропорциональной мощности множества чисел, входящих в диапазон – то есть она подразумевается равной нулю только в том случае, если мощность множества чисел, входящих в диапазон – как минимум равна алеф 0 (мощность актуально бесконечного множества натуральных чисел).
Формула 4. При потенциально бесконечном множестве чисел на диапазоне (и соблюдении принципа безразличия) вероятность того, что непрерывная случайная величина примет конкретное значение, будет считаться стремящейся к нулю, но не равной ему; при актуальной же бесконечности множества – вероятность полагается именно равной нулю.

Утверждение 1. Для непрерывной случайной величины, при условии определения функции выбора на некотором множестве, вероятность выбора конкретного значения из множества чисел на любом диапазоне никогда не будет равна нулю.
Утверждение 2. При определении функции выбора на любом множестве среди элементов этого множества должен быть хотя бы один элемент, вероятность выбора которого равна 1.

Доказательство. Согласно нашей теореме об упорядоченности и понятию «случайного порядка», определение функции выбора на любом множестве эквивалентно нарушению принципа безразличия и автоматическому упорядочиванию множества. Тем самым происходит некоторое перераспределение вероятностей выбора с определением одного «актуально доминирующего» элемента, который и будет соответствующим образом выбран.

Вывод. Утверждения верны, что и требовалось доказать.

Заключение. Здесь, конечно, можно сказать, что если мы не будем определять функцию выбора на актуально бесконечном множестве, то вероятность выбора какого-либо конкретного значения, при условии соблюдения принципа безразличия, как раз-таки будет равна нулю. На это мы скажем, что это высказывание может казаться верным только до тех пор, пока мы не захотим его верифицировать и оно тут же перестанет быть верным при любой проверке. И это так потому, что истинность высказывания о равенстве нулю вероятности выбора конкретного значения для непрерывной случайной величины основывается строго на двух факторах: 1. Актуальная бесконечность (которая задаётся свойством непрерывности, то есть – континуальности); 2. Принцип безразличия. И мы можем даже оставить в покое и временно «не трогать» обоснованность понятия актуальной бесконечности и непрерывности континуума (с этим мы разберёмся позже), так как нам достаточно того, что при попытке осуществить хотя бы какую-то операцию со множеством чисел на диапазоне – нам изначально придётся определить функцию выбора, тем самым нарушив принцип безразличия и, соответственно, нивелировать равенство вероятности нулю. Более того, определение функции выбора эквивалентно фактическому осуществлению выбора одного конкретного элемента, затем следующего, следующего и так далее – до тех пор, пока множество не станет пустым.

P.S. Также, если пойти несколько дальше и внимательно присмотреться к высказыванию «при отсутствии определения функции выбора на актуально бесконечном множестве, вероятность выбора какого-либо конкретного значения, при условии соблюдения принципа безразличия, будет равна нулю», то можно заметить некоторые интересные аспекты. В рамках данного высказывания говорится о применимости понятия выбора при отсутствии определённой функции выбора, что уже само по себе является тафтологией. Это примерно и отдалённо эквивалентно высказыванию, в рамках которого мы будем утверждать, что «объект красным быть не может, так как он синий» или «если не красить объект в зелёный цвет, то при покраске его в чёрный цвет вероятность его озеленения равна нулю». То есть, если мы подразумеваем хоть какой-то выбор элементов из какого-либо множества или вообще любую операцию с ним, то мы априори имеем дело с упорядоченным множеством. Но, если мы говорим, что ничего выбирать не будем и потому ничего выбрано и не будет, то это по сути является повторением одного и того же два раза. Если не выбирать, то вероятность выбора любого элемента равна нулю, но это же «и так понятно». Однако, если мы выбор всё же подразумеваем, то элемент будет выбран с вероятностью равной 1. То есть, в общем-то, непрерывная случайная величина, в её классическом виде – это как раз-таки про отсутствие попытки сделать выбор вообще и именно потому вероятность выбора нулевая. А при попытке сделать выбор – он сделан будет гарантированно на любом множестве, при условии того, что множество вообще существует и не является пустым – для несуществующего множества вероятность выбора хотя бы одного из его элементов конечно же равна нулю, точно также, как и для множества пустого.

5.3. ТЕОРЕМА О НЕУПОРЯДОЧЕННОМ МНОЖЕСТВЕ

Описание. Данная теорема логически пролонгирует две предыдущие. И в этом случае мы попробуем, что называется, пойти от противного. Предположим, что неупорядоченное множество существует и попробуем выяснить его специфику и охарактеризовать особенности.
Предварительно уточним определение понятия статичности в математике – здесь это нам пригодится. Формального определения, собственно, нет. Возможно потому, что оно подразумевается «и так понятным». Однако, введём хоть какую-то формализацию – через свойства отображения. Итак, под статичным объектом мы будем понимать такой объект, который является неизменным или имеет возможность быть представленным в некоем одном и только одном состоянии с сохранением своих характеристических особенностей – то есть, по статичному отображению объекта мы должны иметь возможность понять, с каким именно объектом мы имеем дело.
Также напомним определение аксиомы экстенсиональности: для любых 2-х множеств, если каждый элемент 1-ого принадлежит 2-му, а также каждый элемент 2-ого принадлежит 1-му, то 1-е множество идентично 2-му.

Формула. Согласно аксиоме экстенсиональности, любое множество равно самому себе.

Утверждение 1. Осуществление любой математической операции возможно только на статичных объектах, следовательно – определение отношений порядка и/или равенства возможно только на статичных объектах.
Утверждение 2. Неупорядоченное множество не является статичным объектом, следовательно – неупорядоченное множество не равно самому себе, а значит – неупорядоченное множество не может существовать.

Доказательство. Итак, представим неупорядоченное множество Х. Нам даны следующие его характеристики: мощность и диапазон возможных значений. Мощность множества Х равна 5, а диапазон значений – от 1 до 100. То есть, мы знаем, что нашему множеству Х принадлежит 5 элементов q, таких, что q -> Q, где Q – множество натуральных чисел на закрытом диапазоне [1, 100]. Допустим также, что множество Q – вполне упорядочено по возрастанию.
Можем ли мы сравнить множество Х с самим собой? Кажется, что можем. Но как именно мы должны это сделать? Ведь под наше определение множества Х подходят как, к примеру множество – {1, 9, 22, 54, 2}, так и множество – {3, 78, 99, 2, 5}. То есть сравнение в виде запроса «равно ли это множество – {1, 9, 22, 54, 2} вот этому – {3, 78, 99, 2, 5}?» может считаться корректной попыткой сравнить Х с Х? Очевидно, что нет – это подсказывает сама интуиция, причём без особенного напряжения. Но, хорошо – тогда как нам сравнить Х с самим собой? 
Очевидно, что для начала нам нужно хотя бы уточнить какие именно элементы принадлежат нашему множеству. Вообще, традиционно полагается, что для операции сравнения порядок элементов во множестве не важен – это следует из самой аксиомы экстенсиональности. Поэтому нам, чтобы осуществить корректное сравнение множества Х с самим собой, необходимо просто каким-то образом понять, что за элементы ему принадлежат. Каким образом мы можем это сделать?
На основе имеющейся у нас информации о диапазоне натуральных чисел, которые потенциально могут принадлежать множеству Х, мы можем определить функцию, которая выполнит операцию пересечения множества Q, содержащего все натуральные числа от 1 до 100 (оба включительно) и нашего множества Х, которое содержит 5 элементов, также принадлежащих и множеству Q. Хорошо, но каким именно образом нам выполнить операцию пересечения этих двух множеств?
Мы можем осуществить итерирование по множеству Q и сформировать на основе него и множества Х новое множество – W, такое что W будет представлять собой результат пересечения множеств Х и Q. То есть, мы будем «пробегать» по множеству Q и при помощи индикаторной функции для его актуального элемента q осуществлять проверку на предмет принадлежности q множеству Х. Если значение, возвращаемое индикаторной функцией равно 1 (индикаторная функция будет использовать нотацию Айверсона), то q помещается в множество W, а если нет, то ничего не делаем.
Помним, что наше множество натуральных чисел Q – вполне упорядочено по возрастанию. Мы также помним о том, что ранее мы говорили о невозможности определения индикаторной функции на неупорядоченном множестве – и мы именно так и полагаем, но на данный момент всё же попробуем и посмотрим, что получится. Итак, мы «пробежали» по множеству Q и сформировали множество W, которое, как мы помним, являет собой результат пересечения множеств Q и Х, и которое выглядит вот так {2, 6, 24, 35, 77}. Теперь сравним его с самим собой. Как мы можем это сделать?
Мы знаем, что его мощность равна 5 и мы можем установить некий счётчик С равным 0, далее определив индикаторную функцию для множества Х. И при каждом возврате значения индикаторной функции равному 1 – мы будем увеличивать наш счётчик С также на единицу. И если по итогу «пробега» по всему множеству Х значение счётчика С будет равно 5 – то множество Х будет равно самому себе, а иначе – нет. Здесь очевидно, что всё пройдёт успешно и значение счётчика С будет равно 5 и, следовательно, само множество Х тоже – будет равным самому себе. Так выходит, что неупорядоченное множество Х – всё-таки равно самому себе?
Давайте внимательно посмотрим на наше множество Х. Здесь же вспомним, что множество натуральных чисел Q, на основе которого мы осуществляли проверку на принадлежность элементов множеству Х, является упорядоченным по возрастанию. А не является ли теперь и множество Х, непосредственно упорядоченным в соответствии с порядком, определённым на множестве Q? Для наглядности представим, что множество Q упорядочено не как изначально – по возрастанию, а наоборот – по убыванию. Тогда, соответственно, множество Х выглядело бы так – {77, 35, 24, 6, 2}. И оно вновь было бы упорядочено в соответствии с порядком, определённым на множестве Q. В сущности, у нас 120 вариантов комбинаций множества из 5 элементов – и каждая из них является упорядоченной. Упорядоченные же они именно потому, что ни для одного варианта из 120-ти мы не можем вывести валидное утверждение о том, что оно не упорядочено – по нему этого «не видно». И как бы мы ни старались хоть что-нибудь сделать с нашим неупорядоченным множеством Х – мы в самом начале будем определять некую точку отсчёта, в соответствии с которой и будет осуществляться всё дальнейшее взаимодействие с множеством Х. И, следовательно, множество будет упорядочиваться в процессе осуществления с ним взаимодействия и в соответствии со спецификой этого взаимодействия. То есть – неупорядоченное множество в принципе существовать не может, и мы это сейчас, хоть и не строго формально, но всё же доказали.
Но вот именно «представить» себе исконно неупорядоченное множество и именно в таком виде мы ведь можем? Примерно таким же образом, каким представляет себе актуально бесконечное множество всех чётных чисел, например. Попробуем немного уточнить его характеристики. Итак, мы помним, что в нём 5 элементов. Но, для простоты предположим, что эти самые 5 элементов принадлежат диапазону чисел от 1 до 5 – то есть в нашем множестве Х точно есть 1, 2, 3, 4, 5 и ими оно ограничено. Но как именно расположены эти элементы?
В данном моменте реализация принципа безразличия имеет интересные последствия. Каждый элемент множества Х может находится в каждой позиции этого множества. То есть вероятность того, что 1 будет условно первым элементом множества Х – равна рациональному числу 1/5. И такова же вероятность для каждого из чисел. Но мы не можем сказать, что, например, число 3 находится в такой-то конкретной позиции или, допустим, что число 5 является наибольшим (или максимальным – здесь не особо важно) элементом множества, так как для этого нам придётся упорядочить его, как в нашем предыдущем примере. Ни один из элементов такого множества мы не можем определённо поставить ни на одну из возможных позиций этого множества, следовательно – оно не является статичным, а значит – мы не можем сравнить его с самим собой.

Вывод. Оба утверждения верны, что и требовалось доказать.

Заключение. Так как неупорядоченное множество не является статичным объектом – оно не может быть представлено ни в каком из возможных отображений таким образом, чтобы было понятно, что множество не является упорядоченным.

P.S. По сути, неупорядоченное множество представляет собой междисциплинарный объект, на субстрате которого сходятся математика и квантовая механика. То есть – каждый элемент неупорядоченного множества находится как бы в суперпозиции и вероятность его «воплощения» в той или иной «локации» множества обратно пропорциональна мощности множества. «Эффект наблюдателя» также присутствует и тоже полностью эквивалентен такому же эффекту в квантовой механике: как только мы пытаемся хоть что-нибудь «сделать» с множеством – оно «автоматически» и как бы «само собой» упорядочивается.
К слову сказать, именно в этом самом моменте и ему подобных вспыхивает красками один из древнейших и сложнейших вопросов философии математики (и не только математики): о том, открываются уже «где-то там» существующие математические объекты или же всё-таки они непосредственно создаются при помощи когнитивного функционала человека? И если внимательно присмотреться, то краски как раз и складываются в метафорический ответ: неупорядоченное множество мощностью в 5 элементов, продуцирующее 120 вариантов упорядоченных множеств – потенциально, но не актуально, существует в принципе во Вселенной (без уточнения «где именно»), но вот конкретное множество – существующее актуально и упорядоченное одним из 120-ти вариантов – совершенно точно создаётся, формируется и непосредственно конструируется при помощи когнитивного функционала человека.
Заодно мы также очертили чёткую разницу между потенциальным и актуальным. И на основе этой разницы актуально бесконечные множества стали выглядеть несколько странно, хоть пока это и не создаёт особой проблемы.

5.4. ТЕОРЕМА О ХАОСЕ И ПОРЯДКЕ

Описание. Содержание данной теоремы подразумевается существенно более философским, чем в большинстве прочих, представленных в данной части.
Здесь мы хотим детальнее рассмотреть и продемонстрировать разницу между хаосом и порядком – тем, что подразумевается под этими фундаментальными феноменами. Собственно, хаос и порядок – философские категории, которые, в некотором смысле, пронизывают собой все остальные. Причина/следствие, часть/целое, бесконечное/конечное, форма/содержание, количество/качество и прочее – всё обусловлено «переходом», но не количественного в качественное, а именно хаотичного в упорядоченное – и этот переход несколько фундаментальнее прочих.
На основе наших предыдущих теорем мы уловили некие специфические особенности хаоса и его взаимодействия с упорядоченностью. И природа хаоса поистине красочно проявляется на примере неупорядоченных множеств. На практике, при рассмотрении какой-либо совокупности множеств, зачастую просто подразумевают их существование, чётко не определяя и не конструируя их – это само по себе не создаёт ни противоречий, ни вообще проблем. То есть, мы можем сказать «допустим W – неупорядоченное множество натуральных чисел на диапазоне от 1 до миллиарда, а В – булеан W, то есть множество всех его подмножеств». Контекст высказывания подразумевает, что мы пока не собираемся ни формировать отображение множества W, ни вообще хоть как-то его представлять; и то же самое и с множеством В (тем более с ним). Так, в сущности, можно – проблем это, как мы уже сказали выше, не создаст. Позже это нам пригодится.
В области научного знания хаос обычно рассматривается в теории хаоса, теории динамических систем и так далее. Под хаосом понимается такое поведение системы, которое не может быть спрогнозировано линейными средствами. Наиболее простыми и понятными примерами хаотического поведения системы служат генераторы случайных чисел: в зависимости от диапазона возможных чисел, каждое число в последовательности выбирается вероятностным образом – то есть на основе некоторого распределения вероятностей по диапазону. Полученная последовательность традиционно представляется примером хаотического поведения системы. Вероятности, как правило, сохраняются вне зависимости от осуществлённых ранее актов выбора, то есть выбор не удаляет число из множества чисел на диапазоне, а та последовательность, которая формируется при помощи генератора случайных чисел, может по итогу представлять собой не множество, а мультимножество – то есть, в рамках одной последовательности числа могут повторяться сколь угодно много раз и может случится, что вся последовательность представлена повторением одного единственного числа.
Сложность в подобных случаях, как уже было упомянуто, заключается в том, что мы не можем точно сказать, какое именно число будет выбрано следующим – отсутствует возможность составления строго определённого прогноза и всё, что имеется – вероятности того или иного числа быть выбранным. Как мы показывали в предыдущих наших теоремах – невозможно представить множество в неупорядоченном виде. Это должно означать, что и последовательность, которая формируется при помощи случайного выбора, также должна быть каким-то образом упорядочена. Однако мы не можем указать способ этого упорядочивания и, на его основе, осуществить соответствующее прогнозирование. Почему же так происходит? Попробуем разобраться с этим вопросом на примере множества натуральных чисел и формируемой на его основе числовой последовательности.

Формула 1. Определение функции выбора на любом множестве гарантирует упорядоченность этого множества.

Утверждение 1. Невозможность прогнозирования следующего элемента последовательности обусловлена не неупорядоченностью исходного множества, а его отдельным «переупорядочиванием» для каждого элемента формируемой последовательности.
Утверждение 2. Ни самого хаоса в чистом виде, ни хоть каких-либо его проявлений – не может конструктивно существовать, они могут быть только «предположены» существующими, но даже не «представлены».

Доказательство. Итак, предположим существование неупорядоченного множества натуральных чисел на закрытом диапазоне от 1 до 100 – [1, 100]. Обозначим его как Х. Здесь же заметим, что именно «предположить» существование подобного множества мы можем, но вот непосредственно его представить – уже нет, так как само формирование совершенно любого представления упорядочит множество, что мы, собственно, и показали в нашей предыдущей теореме. Теперь попробуем на его основе сформировать последовательность случайных чисел. Причём формировать её будем следующим способом. Мы случайным образом выбираем число из множества и помещаем его в последовательность. Далее есть два варианта: 1. Число при выборе не удаляется из множества, 2. Число при выборе удаляется из множества. Разница здесь будет в том, что во втором варианте мы со стопроцентной гарантией сможем предсказать последнее число последовательности (ведь диапазон нам известен и последовательности мы тоже будем «видеть»). То есть разница будет в распределении вероятностей по числовому диапазону, но в остальном – ситуации не будут отличаться. Мы предпочтём первый вариант, так как на нём проще продемонстрировать принцип.
Итак, выберем число и поместим его в последовательность. Мы знаем, что для того, чтобы осуществить выбор элемента, мы изначально должны упорядочить множество в соответствии с некоторым отношением порядка, пусть даже и случайного. Порядок этот будет отражаться в распределении вероятностей по всему множеству, то есть, буквально – каждому числу будет поставлено в биективное соответствие некоторое другое число – случайное, в этой ситуации. Но, как мы можем вообще выбрать хотя бы один элемент? Ранее мы уже упоминали, что выбор одного единственного элемента из некоторого множества эквивалентен упорядочиванию множества. Но этот момент всё ещё может представляться не вполне очевидным. На самом же деле мы по сути формируем ещё одно множество – множество случайных чисел, в котором каждому числу исходного множества соответствует строго один элемент. Обозначим это «дополнительное» множество как Y. И на основе этого соответствия, с опорой на «случайное число» из множества Y, и будет осуществлён выбор из исходного множества Х. Нам же нужен только один элемент – нам не нужно всё множество сразу. Поэтому определив Y для Х и выбрав одно единственное число – актуально доминирующее в множестве Y и, на основе него – актуально доминирующее в множестве Х, мы просто помещаем это число из множества Х в нашу последовательность.
Попробуем выбрать следующий элемент. Мы просто вновь будем формировать множество Y, только уже каждый раз новое, и затем выбирать тот элемент, который актуально доминирует в множестве Х, на основе соответствующего ему элемента из множества Y, который уже в нём является актуально доминирующим. То есть мы по сути упорядочиваем всё множество, «берём» один его элемент, и в дальнейшем просто повторяем эту процедуру – каждый раз переупорядочиваем множество «с нуля» для выбора одного из его элементов.
Собственно, примерно так и функционирует то, что обычно имеется ввиду под хаосом. Но, на основе нашего примера, видно, что этот самый хаос просто «складывается» из последовательности упорядочиваний за счёт операции «частичного выбора».
Поясним понятие частичного выбора. На основе того элемента, который является актуально доминирующим в множестве, мы могли бы найти следующий за ним по «уровню доминирования», затем следующий и так далее – множество уже упорядочено. Но мы этого не делаем, а просто, взяв «самый доминирующий» элемент, вновь переупорядочиваем множество уже другим способом. Потому последовательность и выглядит «хаотичной» – за счёт того, что каждый очередной элемент являет собой итог отдельно взятого порядка и порядки это разные, хоть и потенциально могут совпадать. При таком подходе хаос представляется просто совокупностью «порядков», причём не полностью, а частично представленных.
Однако наше представление о хаосе несколько глубже. А именно – хаос в принципе не может быть представлен никаким образом, так как любое представление, как мы уже неоднократно убеждались – являет собой версию порядка. Хаос не может быть представлен так, чтобы было «видно» и «понятно», что то, что представлено – есть хаос потому, что то, что представлено любым возможным образом и способом – всегда есть порядок.

Вывод. Утверждения верны, что и требовалось доказать.

Заключение. Наслаивание друг на друга различных версий порядка создаёт впечатление, что система хаотична. Но, в действительности мы имеем дело просто с различными версиями порядка и каждое последующее состояние системы представляет собой «верхушку айсберга» в виде актуально доминирующего элемента всего множества возможных вариантов упорядочивания – каждый элемент случайной последовательности несёт «за собой» полностью упорядоченное множество.

P.S. В довершение темы о невозможности хаоса существовать, можно упомянуть о высказывании «мыслить вне рамок». Под этим высказыванием традиционно имеется в виду осуществлять процесс рассуждения и формулирование высказываний без опоры на «актуально доминирующие» в текущей парадигме догмы. Но, если попробовать мыслить вовсе «без рамок», то очень скоро станет понятно, что это возможным не представляется в принципе – в любом случае имеются некие «опорные точки» рассуждения, как бы «неявные аксиомы» той модели формальной системы, в контексте которой осуществляется формулирование высказываний. То есть сама попытка мыслить «вне рамок» уже создаёт рамки. И позже мы вернёмся к этому моменту.


5.5. ТЕОРЕМА О ДИАГОНАЛЬНОМ МЕТОДЕ КАНТОРА И ВЕЧНОМ ЦИКЛЕ

Описание. В контексте данной теоремы мы рассмотрим доказательство Георга Кантора о том, что мощность континуума, то есть множества всех вещественных чисел, больше мощности множества всех натуральных чисел. Это доказательство строится на диагональном аргументе. Вообще говоря, Кантор, в ходе «апологии» теории множеств вообще и несчётности континнума – в частности, за свою жизнь привёл не одно доказательство, а несколько. Но именно диагональный метод традиционно признаётся наиболее ярким, простым, понятным и непротиворечивым.
Кратко приведём метод Кантора. Представим себе множество всех вещественных чисел R. Подчеркнём, что множество R –  актуально бесконечно, на что указывает применение к нему притяжательного местоимения «всех». Почему не потенциально бесконечно? Нам нужны именно «все» вещественные числа для доказательства, а потому – множество R актуально бесконечно. Запишем все элементы этого множества в столбик и приведём их к двоичному формату (именно двоичный формат выбран для простоты, можно выбрать любой). Выпишем все цифры по главной (опять же, можно и по второстепенной – не важно) диагонали и применим к каждой из них операцию «1 – х», где х – «диагональная» цифра каждого числа множества. Таким образом, мы получаем число Х, которое не совпадает ни с одним числом из множества R конкретно по диагональной цифре, то есть оно отсутствует в множестве R, что и требовалось, собственно, доказать. Х можно назвать «дополнительным» числом.
Тем самым доказывается, что мощность бесконечного множества всех вещественных чисел больше мощности бесконечного множества всех натуральных чисел, так как корректно применить диагональный метод для множества всех натуральных чисел мы не можем. «Дополнительных» чисел также подразумевается бесконечное множество, так как после получения «диагонального числа», подразумевается возможность добавить его в множество R, после чего получить следующее «диагональное число» и так далее – до бесконечности.
Само собой разумеется, что «сделать» хоть что-то подобное в реальности, реализовать физически хоть один шаг доказательства – возможным не представляется. Но вот именно «представить» это, смоделировать на субстрате mundus – мы теоретически способны. Потому доказательство Кантора принимается и является неоспоримым уже более полутора веков.

Формула об актуально бесконечных числах. Количество цифр в записи (при представлении числа в определённой системе счисления) отдельно взятого актуально бесконечного вещественного числа не может увеличиться, так как оно «уже дано» бесконечным.
Формула об актуально бесконечных множествах. Количество элементов в актуально бесконечном множестве не может быть увеличено, так как множество «уже дано» бесконечным.

Утверждение 1. Если аксиомы верны, то количество «дополнительных» чисел не является бесконечным, а равно количеству символов в избранной для записи системе счисления.
Утверждение 2. Если верна теорема 1, то множество вещественных чисел перечислимо и размер его «дополнительного» множества равен x, где х – количество символов в системе счисления.
Утверждение 3. Если не верна теорема 1, то актуальная бесконечность не является правомерным понятием, так как под её «определением» находится бесконечность потенциальная, и, как следствие, диагональный метод ошибочен ввиду неправомерного использования понятий – по сути простой «подмены».
Утверждение 4. Хотя бы одна из теорем 2 и 3 верна.

Доказательство. Повторим диагональный метод Кантора. Итак, мы уже нашли дополнительное «диагональное» число. Назовём это число Х. Но тут же становится очевидно, что из-за первой аксиомы добавить его в наше множество R мы не можем – множество R актуально бесконечно, а соответственно не может стать ещё больше, так как это бы означало, что оно как раз не было актуально бесконечным, что противоречит изначальным условиям.
Сформируем множество «дополнительных» чисел RD, добавим в него число X и попробует таким путём «спасти» доказательство Кантора, иначе нам бы пришлось просто признать метод неправомерным по причине несоответствия действий, подразумеваемых методом, условиям ситуации его применения. Напомним, что число Х отличается от всех чисел в множестве R по диагональной цифре. Теперь применим ко множеству R диагональный метод ещё раз, но последнюю цифру возьмём из числа Х – ведь нам доподлинно известно, что именно оно условно «последнее». И мы получим число Y. Число Y совпадает со всеми, кроме Х, элементами R по диагональной цифре, но отличается от Х также по диагональной цифре – в данном случае по «последней». Добавим Y в RD. Теперь попробуем найти следующее «дополнительное» число Z. Мы применяем диагональный метод ещё раз и получаем число Z. Это число отличается от всех элементов R, теперь уже включая Y, по диагональной цифре. Ещё раз: число Z отличается от всех элементов R, включая Y, по диагональной цифре. То есть – это то же число Х, так как Х = Z. То есть, мы можем добавить в множество RD только 2 числа – Х и Y, а после они «зациклятся». Для десятичной системы счисления они «зациклятся» после 10, для шестнадцатеричной – после 16 и так далее.

Вывод. Если мы принимаем аргументацию представленного нами доказательства, то верны утверждения 1 и 2, соответственно – континуум является счётным; если не принимаем – верным является утверждение 3 и доказательство Кантора неправомерно, так как получается, что в контексте диагонального метода смешиваются 2 разных бесконечности – актуальная и потенциальная, причём первая используется для построения множества R, а вторая – для увеличения количества цифр в «дополнительном» числе и увеличении самого множества Rс, а подобный подход – противоречив, так как под видом актуальной бесконечности используется потенциальная.

Заключение. Утверждение 4 – верно, что и требовалось доказать. Из этого следует наличие глубоких внутренних противоречий в теории множеств, в частности, подвергается сомнению правомерность трансфинитной индукции, несчётности континуума и делается особый акцент на корректности применения понятия актуальной бесконечности.

P.S. Является очевидным, что парадоксы типа «отеля Гильберта» основываются на использовании точно такой же «операции» неправомерного смешивания понятий актуальной и потенциальной бесконечностей и умелом, но совершенно не корректном, маневрировании между ними в зависимости от актуальных нужд формулировки, включённой в контекст доказательства. Здесь некоего «совокупного математика» можно метафорически представить в виде молодого человека, который девушкам на свиданиях рассказывает, что он очень много зарабатывает, а в налоговой инспекции клянётся, что очень мало.

5.6. ТЕОРЕМА О МОЩНОСТИ МНОЖЕСТВ

Описание. Согласно доказанной теореме Кантора, множество X имеет мощность меньшую, чем множество всех его подмножеств Y. Для конечных подмножеств это правило всегда соблюдается. Можно представить множество Х – {1,2,3} и множество всех его подмножеств Y – {{}, {1}, {2}, {3}, {1,2}, {1,3}, {2,3}, {1,2,3}}. Очевидно, что мощность множества меньше мощности подмножества, так как 3 < 8. Также проблем в подобном сравнении не возникнет даже в том случае, если принять возможность актуально бесконечного множества натуральных чисел и равномощных ему множеств. А вот в случае с множеством всех вещественных чисел, то есть континуумом, ситуация несколько иная, и выводы об этой ситуации основываются на одной из ключевых концепций теории множеств – трансфинитной индукции. Трансфинитная индукция – это, повторим тезисно выражаясь, экстраполяция на более высокие значения тех качеств и свойств, которые были выявлены при работе со значениями более низкими. То есть принцип следующий – что верно для 1, то верно и актуальной бесконечности. Подобный подход должен вызывать и, конечно же, вызывает различные проблемы и противоречия. Это мы и покажем.

Формула. Равномощность множеств Х и Y определяется по возможности установления между ними биективного, то есть взаимно-однозначного соответствия: для каждого х -> X должен существовать ровно один уникальный y -> Y, и обратно, соответственно, также – для каждого y -> Y должен существовать ровно один уникальный х -> X.

Утверждение 1. Мощность множества всех вещественных чисел равна мощности всех его подмножеств.
Утверждение 2. Если верна теорема 1, то теорема Кантора – ошибочна.
Утверждение 3. Если неверна теорема 2, то диагональный аргумент Кантора – неправомерен.
Утверждение 4. Хотя бы одна из теорем 2 и 3 – верна.

Доказательство. Представим актуально бесконечное множество всех вещественных чисел Х и сформированное на его основе (как в нашем примере) также актуально бесконечное множество всех его подмножеств Y. Также у нас есть функция F, которая сопоставляет один уникальный х, такой что х -> X с одним уникальным y, таким что y -> Y. Также у нас есть функция D, которая служит для воспроизведения диагонального аргумента Кантора в том случае, если для функции F не хватает элемента х для установления биекции с элементом y. То есть в том случае, когда элементы в Х заканчиваются и функция F не может «взять» следующий элемент из Х для установления биекции с Y, то функция D воспроизводит диагональный аргумент Кантора и добавляет полученное число в множество Х. Таким образом, можно сказать, что для любого элемента множества Y всегда может быть найден взаимно-однозначный уникальный элемент в множестве Х.

Вывод. Утверждение 1 – верно, что и требовалось доказать. Если же по какой-либо причине указать, что подобное противоречиво и неправомерно, то из этого следует также противоречивость и неправомерность самого диагонального аргумента Кантора. Так как если его можно использовать в одном случае для некоторой операции, то можно и в другом случае с той же операцией.

Заключение. Утверждение 4 – верно, что и требовалось доказать.

P.S. В данном случае мы не акцентировали внимание на том аспекте, что, собственно, к актуальной бесконечности совершенно нечего добавить, что мы уже неоднократно показывали и обосновывали. Здесь мы, как раз в угоду теории множеств и полностью, как, собственно, и сам Кантор, не «отвлекаясь на мелочи», поступили ровно так же, как и он, и вскрыли противоречие уже в ином месте.
Здесь скорее целесообразна контраргументация о том, что если мы применяли диагональный метод для самого множества всех вещественных чисел, то его с тем же успехом можно рекурсивно применить и для всех подмножеств множества всех вещественных чисел – и тогда мощность множеств вновь не должна совпасть. То есть, более формально: при увеличении мощности множества мощность его булеана возрастает соответствующим образом. Мы добавляем элемент в множество и на множестве всех его подмножество это также сказывается – это понятно. То есть, при добавлении «диагональных чисел» в множество всех вещественных чисел мощность множества всех его подмножеств также «начнёт учитывать» эти самые дополнительные числа и тут, вроде бы, нет вариантов. Но, на самом деле, не совсем так. Если мы способны показать потенциал к «энергичному расширению» множества, даже при условии того, что оно актуально бесконечно, то мы имеем дело (понятно, что с потенциальной бесконечностью, но сейчас не об этом) с парадоксом Тристама Шенди – Кантор сам указал эту «лазейку». А таким образом мнение Рассела о том, что «часть не меньше целого» – становится правомерным и в данном случае также. То есть, если и целое, и часть целого одновременно обладают потенциалом для бесконечного расширения, то часть не меньше целого.
На это ещё можно ответить тривиальным образом указав, что перманентное увеличение мощности, как самого множества, так и рекурсивно множества всех его подмножеств, уже настолько явно провоцирует к входу в контекст понятия потенциальной бесконечности, что сопротивляться этому мы и вовсе не в силах. И тогда просто в очередной раз постулируем полную несостоятельность понятия актуальной бесконечности в контексте теории множеств по всем показателям, кроме одного – мощности, которую мы предлагаем заменить на более корректное и более соответствующее понятие индифферентности по какому-либо показателю (мощности, опять же).
К слову сказать, вопрос о мощности или индифферентности потенциально бесконечных множеств и их подмножеств, по понятным причинам, лишён всякого смысла.
Резюмируем же сказанное:
•	если к актуальной бесконечности нечего добавить, то неправомерен диагональный аргумент;
•	если есть что добавить, то бесконечность не актуальная, а потенциальная;
•	если и добавить можно, и в то же время этим ничего нельзя изменить в самом множестве, то вновь неправомерен диагональный аргумент Кантора;
•	если и добавить можем и изменить имеем право всё, что угодно, за исключением мощности, то мы имеем дело уже и вовсе не с бесконечностью, а с индифферентностью по показателю мощности;
•	а вот, наконец, если мы и добавить можем, когда нам надо и не можем, когда нам, соответственно, не надо и изменить можем, что угодно, когда захотим и в то же время – не можем, если нам именно в актуальном контексте это необходимо для доказательства, то – мы и вовсе имеем дело с безосновательной теорией, требующей существенной переработки.

P.S.2. Здесь считаем целесообразным сделать акцент на том, что мы прекрасно осознаём то, что именно актуальная наша теорема, а точнее специфика формирования нашего доказательства данной теоремы может выглядеть как будто бы «не вполне серьёзно». То есть, мы, как представляется, берём некие разрозненные фрагменты теории множеств, комбинируем их и получаем достаточно несуразное «нечто». И здесь это действительно так. В сущности, мы не совсем склонны поступать подобным образом при формировании философских теорем, однако эта теорема находится на своём месте по одной простой причине: мы убеждены в том, что при попытке обосновать неправомерность нашего доказательства, эти самые попытки обоснования в любом случае будут выглядеть куда более несуразно. То есть – «так» всё же «можно».

5.7. ТЕОРЕМА О БЕСКОНЕЧНОМ МНОЖЕСТВЕ НАТУРАЛЬНЫХ ЧИСЕЛ

Описание. В продолжение темы прошлой теоремы – попробуем представить ровно тот же самый смысл, но на гораздо более интуитивно понятном примере.
Парадокс Бурали-Форти свидетельствует о противоречивости таких понятий теории множеств, как множество всех порядковых чисел или максимальное порядковое число. В случае с противоречиями подобного рода не вполне понятно, зачем же «ходить» так далеко и рассматривать именно порядковые числа или кардинальные числа. Допустим, множество всех натуральных чисел является счётным, однако его булеан – множество всех его подмножеств, уже счётным не является. На этот момент, собственно, обычно не обращается никакого внимания – что может быть парадоксального в натуральных числах? Напомним, что под счётностью множества подразумевается возможность установления взаимно-однозначного соответствия всех элементов некоего множества и равномощного множества натуральных чисел. Также укажем на то, что согласно математическим представлениям о природе натуральных чисел – максимального натурального числа не существует, так как для любого натурального числа Х всегда существует такое число, которое равно Х + 1.

Формула 1. Согласно аксиоматике Пеано, не существует максимального натурального числа, так как для любого натурального числа существует его последователь, также являющийся натуральным числом.
Формула 2. Согласно теореме Кантора, мощность множества всех подмножеств множества натуральных всех натуральных чисел равна 2 в степени алеф 0, то есть мощности множества всех вещественных чисел и, соответственно, данное множество не является счётным.

Утверждение 1. Существует максимальное натуральное число.
Утверждение 2. Множество всех подмножеств множества всех натуральных чисел – является счётным.
Утверждение 3. Существование максимального натурального числа – противоречиво, так как его теоретическое существование противоречит аксиомам арифметики натуральных чисел.
Утверждение 4. Существование актуально бесконечного множества всех натуральных чисел – противоречиво, так как его теоретическое существование приводит к логическим парадоксам, а также противоречит аксиомам арифметики натуральных чисел.
Утверждение 5. Либо теоремы 1 и 2 – верны и противоречива аксиоматика арифметики натуральных чисел и аксиоматика теории множеств, либо теоремы 3 и 4 – верны и противоречива аксиоматика теории множеств.

Доказательство. Попробуем представить себе актуально бесконечное множество всех натуральных чисел - N. Представили и получили множество с мощностью равной алеф 0. Теперь проиндексируем все его элементы этими же натуральными числами. Проиндексировали и у нас всё получилось. Соответственно – множество всех натуральных чисел является счётным. Теперь же попробуем сформировать булеан – множество всех подмножеств Z, для нашего N. Мощность этого Z должна быть равна 2 в степени мощности N – 2 ^ алеф 0. Сформировали такое множество. Напомним, что, согласно аксиоматике теории множеств Цермело-Френкеля, оно уже не является счётным. Но вот только почему? На данный момент ответ на этот вопрос, который использует в качестве опоры диагональный аргумент Кантора – не имеет смысла, так как противоречивость (в общем смысле слова) метода Кантора мы уже показали ранее. Тем более что, для данного конкретного случая это так не потому, что метод Кантора противоречив, а потому, что даже приняв его и подразумевая, что мы всегда можем сформировать ещё одно «диагональное» множество в качестве «дополнительного» элемента нашего множества Z – мы всё равно придём к одному закономерному моменту.
Допустим, мы не интересуемся возможностями Канторовской диагонализации. Мы просто хотим проиндексировать Z, соответственно, натуральными числами. И вот вопрос: у нас это получится? То есть, мы можем попробовать это сделать. Вот, мы начали этот процесс. Все те натуральные числа, которыми мы индексируем множество Z, мы последовательно помещаем в некоторое множество Q. Теперь представим, что мы проиндексировали очередной элемент множества Z, поместили взаимно-однозначное некоторому элементу Z натуральное число в множество Q и мощность Q стала равна алеф 0 – то есть мощности N, которое, как мы помним, содержит в себе ВСЕ натуральные числа. Вопрос: разве мы не сможем проиндексировать следующий элемент Z? То есть, разве мы не сможем добавить число 1 к предыдущему, только что нами определённому, индексу очередного элемента множества Z?
Тут есть всего 2 варианта.
Вариант первый: мы всё-таки сможем проиндексировать следующий элемент множества Z, добавив число 1 к предыдущему индексу, следовательно, мы сможем проиндексировать вообще все элементы множества Z, в том числе даже «дополнительные диагональные», так как на любой «дополнительный диагональный» элемент у нас также найдётся натуральное число, равное натуральному числу предыдущего индекса + 1 – как и следует из аксиоматики арифметики натуральных чисел. Из этого следует, что множество Z будет являться счётным, так как мы всё же сможем поставить ему во взаимно-однозначное соответствие соответствующее множество натуральных чисел, что полностью эквивалентно нашей операции индексирования, а значит само множество N – невозможно конструктивно определить, так как само его существование является противоречивым, ввиду того, что мы не смогли найти явных причин невозможности индексирования элемента множества Z, которому должен взаимно-однозначно соответствовать индекс, равный натуральному числу равному алеф 0 + 1, что как раз и следует из аксиом арифметики натуральных чисел.
И второй вариант: мы не можем проиндексировать следующий элемент множества Z потому, что у нас «закончились» натуральные числа, так как они «все» уже находятся в множестве N, а мы только что «исчерпали» его возможности, следовательно, существует максимальное (актуально бесконечное или «индифферентно» большое) натуральное число, добавление 1 к которому – «не меняет» его в сторону количественного увеличения.

Вывод. Утверждение 5 – верно, что и требовалось доказать.

Заключение. Получается, что, либо существует максимальное натуральное число, либо не существует множества всех натуральных чисел. Интуитивно гораздо «понятнее», конечно, именно второй вариант. Однако, это также будет означать и «низвержение» всей иерархии алефов, так как она, что следует из нашей теоремы, выводится из противоречивых понятий, одним из которых является множество всех натуральных чисел.
Но возможно, стоит обратить внимание также и на вариант альтернативный – на максимальное натуральное число, как гипотетическую возможность. То есть, у нас по-прежнему два варианта, которые, при их некотором логическом продолжении, будут выглядеть так: либо противоречива арифметика натуральных чисел, либо – основания теории множеств.
Но тут есть ещё один интересный момент, который указывает на один гипотетико-логический путь, которым мы просто для примера немного пройдём. Попробуем, так сказать, пойти от противного и предположить, что максимальное натуральное число – существует. Само существование максимального натурального числа сразу же делает множество всех натуральных чисел – конечным. А если оно конечно – у него должен быть конкретный размер, а значит – у 2 в степени этого размера – тоже должно быть конкретное количественное определение (хотя бы в гипотетической системе счисления), а следовательно – множество всех подмножеств множества всех натуральных чисел, при условии существования максимального натурального числа – также будет конечным, а значит – счётным, так как из аксиоматики теории множеств следует, что любое конечное множество – счётно.
Из вышесказанного следует, что, если существует максимальное натуральное число и, как следствие, множество всех натуральных чисел конечно, а следовательно – счётно, то множество всех подмножеств множества всех натуральных чисел несчётно, ведь мы не можем поставить ему во взаимно-однозначное соответствие равномощное множество натуральных чисел.
Но, так как само множество всех натуральных чисел является конечным, то из этого следует, что множество всех его подмножеств – также является конечным, следовательно – оно счётно, что следует из того, что любое конечное множество – счётно. Таким образом, получается, что множество всех подмножеств множества всех натуральных чисел несчётно, а следовательно – оно счётно, а также оно счётно, а следовательно – оно несчётно.
Очевидно, что на логическом пути, который следует за предположением о существовании максимального натурального числа, наличествуют явные логические противоречия. На основе чего можно предположить, что не существует максимального натурального числа и, соответственно, актуально бесконечного множества всех натуральных чисел – не может существовать, так как не существует такого натурального числа Х, для которого мы не смогли бы найти натурального числа, равного Х + 1, что и следует из аксиом арифметики натуральных чисел.
Ну и резюмируя, заметим, что основания арифметики натуральных чисел представляются гораздо более «прочными» по той причине, что в контексте арифметики аксиоматизируется потенциальная бесконечность – для любого Х, если Х – натуральное число, существует такое натуральное число, которое равно Х + 1. Это и есть потенциальная бесконечность. А вот попытка представить множество ВСЕХ – чего угодно, но именно «всех», подразумевает уже бесконечность актуальную, а потому и сразу же ведёт к противоречиям. Но, изъять из теории множеств актуальную бесконечность, означает лишить её не просто одного из предсердий, а вообще всей сердечно-сосудистой и нервной систем.

P.S. С логической точки зрения эта теорема – одна из самых сложных, так как содержит «замыкание» и даже не одно. Также здесь дадим определение этого самого замыкания, которое мы и ранее, к слову, уже неоднократно демонстрировали. Логическое замыкание – это ситуация, возникающая при формировании цепочки высказываний, когда какое-либо высказывание противоречит одному или нескольким высказываниям, стоящим в цепочке ранее, при соблюдении двух условий: правомерности каждых двух последовательных высказываний из цепочки, взятых по отдельности; и восстановлении «правомерности» цепочки высказываний при возврате к тому высказыванию, которому противоречит последнее. На примере (неизоморфном) натуральных чисел это выглядит как: 1 -> 2 -> 3 -> 4 -> -3. Нечто подобное мы уже демонстрировали ранее на примере связи «Стрелы Зенона» и «Момента Зенона».
Отличие от классического противоречия в контексте логических систем заключается в том, что обычно, при формировании доказательства, останавливаются несколько раньше, чем будет достигнуто замыкание. И именно это мы и продемонстрировали на примере работы с апорией Зенона – мы просто продолжили далее его же логику и пришли к тому, что дальнейшее развитие аргументации опровергает сделанные ранее, как бы «преждевременно», выводы.
Замыкание же для данного конкретного случая, формируется на основе противоречивости определения понятий счётности и несчётности множеств. То есть, с одной стороны счётное множество – это множество, которому мы можем поставить в биективное соответствие множество равномощное множество натуральных чисел, а с другой стороны – конечное множество всегда является счётным. Ну вот мы и привели пример конечного множества, которому невозможно поставить в биективное соответствие равномощное множество натуральных чисел. Оно счётно? И счётен ли его булеан? На оба вопроса: с одной стороны – да, с другой – нет. Ну так вот именно такого и не должно происходить в формальных системах. Если мы имеем право рассматривать оба высказывания в качестве истинных «о возможности добавления к любому натуральному числу 1» и о «множестве ВСЕХ натуральных чисел», то это ведёт к противоречиям между арифметикой и теорией множеств и одно из этих противоречий мы здесь и продемонстрировали – о невозможности однозначного ответа на вопрос о счётности или несчётности булеана множества всех натуральных чисел.
Ситуация примерно похожа на один из моментов истории математики, второй половины 19-ого века, когда, вопреки всем актуальным на тот момент представлениям, Вейерштрасс (учитель Кантора) представил математическому миру непрерывную функцию, нигде не имеющую производной, что ранее считалось невозможным.

5.8. ТЕОРЕМА О МАКСИМАЛЬНОМ ЭЛЕМЕНТЕ МНОЖЕСТВА И НЕДОКАЗУЕМОСТИ БЕСКОНЕЧНОСТИ КОНТИНУУМА

Описание. Может ли быть увеличено максимальное число актуально бесконечного множества вещественных чисел? Если нет, то множество действительно можно считаться актуально бесконечным. Если же да, то оно не может считаться актуально бесконечным, а является всего лишь потенциально бесконечным.

Формула. Добавление чего-либо к актуальной бесконечности не способно изменить свойства актуальной бесконечности.

Утверждение. Если максимальное число актуально бесконечного множества вещественных чисел X можно увеличить, то множество X не является актуально бесконечным.

Доказательство. Рассмотрим актуально бесконечное множество вещественных чисел на закрытом диапазоне [0.0, 1.0] – X. Максимальное число Х – 1.0. Теперь представим ещё одно такое же множество, но теперь на интервале [1.0, 2.0] – Y. Максимальное число Y – 2.0. Оба этих множества считаются актуально бесконечными. Теперь же преобразуем X путём добавления к нему Y. Максимальное число Х стало 2.0.

Вывод. Так как максимальное число актуально бесконечного множества X может быть увеличено, то множество X должно считаться не актуально, а потенциально бесконечным.

Заключение. Утверждение – верно, что и требовалось доказать.

P.S. Вышесказанное ставит вопрос о специфике и природе понятия актуальной бесконечности, корректности употребления этого понятия и пределах его применимости.
То есть, можно возразить, что добавление в бесконечное множество нового числа не делает изначальное множество «не бесконечным», так как бесконечной подразумевается именно мощность этого множества, которая не изменилась – а она действительно не изменилась. Само собой разумеется, что с нашей точки зрения это является крайне слабой аргументацией, которая просто «обесцвечивает» силу и юрисдикцию самого понятия бесконечности. В таком случае правомернее было бы и вовсе не говорить о бесконечности как таковой. Вместо этого, если мы заменим высказывание «множество, мощность которого равна бесконечности» на «множество, индифферентное к увеличению», то смысл останется тем же, и бесконечность будет представлять собой просто «бинарную оппозицию нуля».
Если добавление нуля к сумме любой последовательности ничего не прибавляет к сумме последовательности, то ровно наоборот – добавление чего угодно к актуальной бесконечности ничего не добавляет к актуальной бесконечности. Из этого следует, что вполне правомерно можно определить актуальную бесконечность, как «обратную сторону нуля».
Таким образом, если мы в контексте формирования высказываний можем без всякого ущерба для смысла заменить бесконечность на индифферентность, такая бесконечность не имеет даже той понятийной силы, коей обладает бесконечность потенциальная и, соответственно, не является, в полной мере и должным образом, концептом, отражающим специфику незавершённости любой «процессуальной данности» в контексте Вселенной вообще.
В этом смысле мы и полагаем целесообразным заменить понятие актуальной бесконечности на понятие индифферентности. Это имеет свой конструктив, так как именно к мощности множеств в контексте теории множеств – как раз-таки претензий никаких нет. Но, как только вводится конкретно актуальная бесконечность – тут же начинаются проблемы, противоречия и парадоксы. В обоснование нашего тезиса о замене понятий: может ли мощность континуума быть увеличена или уменьшена? Не может. Даже если мы добавим к одному отдельному континууму ещё дополнительно континуум континуумов – мощность будет прежней, если отнимем почти всё (вопрос начёт мощность континнума минус мощность континнума не имеет в математике однозначного ответа) – мощность будет прежней. Это и называется индифферентностью – нечувствительность к воздействиям.
Если же, как ещё один вариант, строить контраргументацию на том, что добавление элемента к актуально бесконечному множеству никак не должно изменить это множество вообще, а соответственно мы и «не имеем права» как захотим добавлять в него элементы, так как оно, в соответствии с нашим же предложением, индифферентно к воздействиям, или, говоря языком теории множеств – актуально бесконечно, то рассмотрим ещё раз диагональный аргумент Кантора, на котором, напомним, стоит всё обоснование несчётности континуума. Кантор именно «добавляет» элементы к актуально бесконечному множеству всех вещественных чисел, показывая его «потенциально бесконечную» несчётность. Мы же ранее показали, что таким образом вовсе неправомерно «добавлять» элементы, так как сам этот факт является убедительным доказательством сугубо потенциальной бесконечности множества, но никак не актуальной. В противовес этому, мы, в отчаянной и самоотверженной попытке «спасти» доказательство Кантора, формировали «дополнительное» множество «диагональных чисел» и на примере двоичной системы счисления продемонстрировали, что мощность этого «дополнительного» множества будет равна двум.
Таким образом резюмируем, что:
•	если утверждать, что добавление к множеству «дополнительных» элементов не правомерно, то неправомерен сам диагональный метод Кантора, а следовательно – континнум является счётным;
•	если же сказать, что мы имеем право добавлять дополнительные элементы, то актуально бесконечное множество всех вещественных чисел (то есть континуум) является потенциально, а не актуально бесконечным;
•	ну и наконец, если обосновывать неправомерность наших выводов на том, что неизменной является только лишь мощность континуума, а всё остальное – изменяемо, но не имеет особого значения, то мы полагаем, что большей целесообразностью, по отношению к множествам такого типа, будет обладать понятие индифферетности по какому-либо критерию (мощности в данном случае), как нечувствительности к изменениям конкретно по этому критерию (сколько мёртвое не пинай, «живее» оно не станет).

P.S.2. Эта наша теорема, в отличие от предыдущей, является одной из наиболее «простых» и даже несколько «наивных». Однако она верна в том смысле, что дополнительно и точечно акцентирует внимание на корректности применения философских понятий в контексте иных дисциплин – математики и теории множеств в данном случае. Они со всей очевидностью некорректно используют «наши» понятия.
Однако, мы всё же хотим именно здесь, внутри данной теоремы, представить небольшую «под-теорему» «о недоказуемости бесконечности континуума» R.
Попробуем обосновать бесконечность континуума тем, что для любого х, такого, что x -> R, существует такой y, что y -> R и у = x / 2.
Смоделируем ситуацию следующим образом. У нас есть 2 персонажа: «Адвокат бесконечности» и «Начальник вечности». Первый утверждает, что множество всех вещественных чисел – актуально бесконечно, а второй убеждён в том, что оно бесконечно лишь потенциально. Доказательства второго строятся на предоставлении некоторого числа Q, меньше которого, по его мнению, в этом множестве быть уже не может. Первый же каждый раз предоставляет в качестве контраргумента число W, такое что W = Q / 2. Затем второй берёт число W, определяет его в качестве своего нового Q и всё продолжается.
Представим их аргументацию в виде актуально бесконечных множеств Х и Y. Вопрос: мощность какого множества будет больше? Со всей очевидностью они будут равны. А для того, чтобы доказать, что континуум актуально бесконечен, необходимо, чтобы мощность множества Х была хотя бы на единицу больше мощности множества Y. А так как они равны – бесконечность континуума недоказуема.
Справедливости ради заметим, что она также и неопровержима. Но вот это как раз уже не совсем хорошо, так как, согласно критерию Карла Поппера о фальсифицируемости, то знание, которое мы хотим определить в качестве непосредственно научного – должно быть опровержимо хотя бы потенциально. Сойдёмся на том, что бесконечность континуума неопровержима конкретно в контексте нашего доказательства недоказуемости бесконечности континуума.
Конечно же здесь можно сказать, что наличие актуально бесконечных множеств аргументов как в защиту бесконечности континуума, так и в защиту постулата о недоказуемости этой бесконечности – уже само по себе является доказательством бесконечности континуума. С одной стороны – конечно же является. Но вот с другой – разве доказательство завершено? Нет, оно не завершено и, более того, обладает бесконечным потенциалом для дальнейшего «развёртывания» своих аргументов. Но множество доказательств никогда не станет больше, чем множество опровержений – они просто обладают потенциалом к дальнейшему расширению. Это уже должно что-то смутно напоминать.
Посмотрим внимательней. На каждый аргумент о конечности континуума, находится аргумент о его бесконечности и этот процесс может продолжаться сколь угодно долго. Таким образом – мы со всей очевидностью имеем дело именно с потенциальной бесконечностью. То есть, ещё раз, если на любое число, предъявляемое «Начальником Вечности» в качестве наименьшего числа континуума, у «Адвоката Бесконечности» найдётся число, меньшее в два раза и это может продолжаться бесконечно – то это потенциальная бесконечность в чистом виде. Таким образом, получается, что из того факта, что множество доказательств бесконечности континуума является бесконечным, следует то, что он бесконечен потенциально, но вот именно актуальная бесконечность континуума – недоказуема.

5.9. ТЕОРЕМА О ДОКАЗАТЕЛЬСТВАХ. ФЕНОМЕН ДЖИМИНАЦИИ

Geminos – двойняшки (лат).

Джиминация (от geminos) – это «смысловая» рекурсия некоего рекурсивного объекта х с изменением хотя бы одного свойства принадлежащего этому же х на каждом рекурсивном шаге.

Описание. Парадокс лжеца, брадобрея, парадокс Рассела, доказательство Алана Тьюринга о неразрешимости проблемы остановки, доказательство Альфреда Тарского о невыразимости истины, доказательство Куртом Гёделем теоремы о неполноте – что у них общего? А общего у них – самореферентность или, как ещё иногда определяют, – диагонализация аргументов. С одной стороны, мы уже показали, что диагональный метод Кантора неправомерен, но мы показали это на совсем ином примере – Кантор не использовал самореферентность в том смысле, в котором её использовал, например, Гёдель. Кантор просто неправомерно смешал понятия актуальной и потенциальной бесконечности в контексте своего доказательства, но по поводу самореферентности – к его доказательству претензий нет. А вот что касается остальных, мы покажем на примере доказательства теоремы Гёделя о противоречивости.
В некотором роде, вся правомерность всех самореферентных как парадоксов, так и доказательств резюмируется в вопросе: «Если я напишу в отчёте, что я писал в отчёте, то будет ли отчёт валидным?». И это один из случаев в ходе нашего изложения, когда введение нечёткой или темпоральной логики позволяет полностью решить проблему, хоть этим путём мы и не пойдём.
Напомним теорему Гёделя о противоречивости в очень простом виде. Представим себе достаточно богатую формальную систему, то есть как минимум систему с «возможностями» арифметики. Назовём эту систему S. Мы можем пронумеровать все элементы S (символы алфавита, формулы, совокупности формул и так далее) натуральными числами. Построим по правилам вывода системы S высказывание G, которое говорит о том, что «высказывание с номером n – недоказуемо», где n – номер высказывания G. И получаем соответствующий результат: если высказывание G недоказуемо, то оно истинно; если же G доказуемо, то оно ложно. То есть система в любом случае оказывается противоречивой.

Формула. Одно из основных правил рекурсивного проектирования гласит: сущности на любом уровне вложенности должны обладать равными возможностями. То есть, вне зависимости от актуальной «глубины» рекурсии – рекурсивный объект должен оставаться одним и тем же.

Утверждение. Доказательство Гёделя строится с нарушением правил рекурсии.

Доказательство. В качестве доказательства приведём старую математическую задачу «100 рублей и 2 шоколадки».
«Допустим, Х взял 100 рублей у Y. Далее Х пошёл в магазин и по дороге потерял 100 рублей. После чего он встретил Z и взял у него 50 рублей. Потом Х купил 2 шоколадки по 10 рублей. У него осталось 30 рублей. Он отдал эти 30 рублей Y и остался должен ему 70 рублей. Плюс Z – 50. То есть всего 120. Плюс у Х 2 шоколадки по 10 рублей. Итого 140 рублей. Вопрос: где ещё 10 рублей?»
При всей тривиальности задачи и, собственно, даже отсутствии в ней явной самореферентности, в ней используется тот же самый «манёвр», что и в доказательстве Гёделя. То есть, если бы вместо того, чтобы отдать Y 30 рублей, Х бы отдал также и 2 шоколадки по 10, то он бы остался должен 50 рублей Y плюс 50 рублей Z, то есть должен ровно столько, сколько потерял. Но за счёт реализации феномена джиминации шоколадок, то есть ввиду того, что их посчитали 2 раза в разном качестве (в качестве долга и в качестве материального ресурса Х), мы получаем парадокс и, собственно, абсурд.
Теперь вернёмся к доказательству Гёделя. Проанализируем высказывание G. «Высказывание с номером n – недоказуемо», где n – номер высказывания G. Выше мы уже постулировали то, что высказывание G и истинно, и недоказуемо. Однако заметим, что высказывание G не просто рекурсивно, а оно обращается к самому себе, но уже в другом качестве. То есть «высказывание G» и «высказывание с номером n» – различные субъекты высказывания, а не один и тот же. Происходит та же самая джиминация причём без соблюдения правила рекурсивного проектирования, так как одно высказывание предстаёт в разных ипостасях, тем самым «удваиваясь» и приобретая на новом «месте» свойства этого места.
«Брадобрей бреет всех тех, кто не бреется сам. Бреет ли он себя?» – 2 раза в разных качествах брадобрей; «Житель острова Крит утверждал, что все жители Крита лгут. Он лжёт?» – 2 раза в разных качествах житель Крита; «Парадокс Рассела» – 2 раза в разных качествах одно множество; доказательство Тьюрингом проблемы остановки – 2 раза в разных контекстах один алгоритм; доказательство Тарского – 2 раза в разных контекстах одно высказывание, точно так же, как и у Гёделя, и так далее.

Дополнение к доказательству. Для того, чтобы наглядно продемонстрировать то, что мы имеем ввиду под джиминацией, мы приведём пример с использованием рекурсии в компьютерной программе, написанной на интерпретируемом языке программирования. Допустим, у нас есть файл с расширением Х. Именно этот файл мы и будем «запускать» в интерактивном интерпретаторе. В этом файле у нас определена функция f. Функция будет использовать хвостовую рекурсию. Конкретней: она проверяет стек вызовов и, если она вызвана ровно 1 раз (что будет значить «внешний» вызов, который по смыслу эквивалентен формированию высказывания на языке формальной системы), то она вызывает сама себя (что по смыслу эквивалентно высказыванию о самой себе). В том же случае, если она вызвана иное количество раз, то функция печатает строку «Самореферентные доказательства – истинны». Далее, вызвав себя внутри себя же она обращается через каталог к нашему файлу Х, и сама переписывает свой код, заменяя 1 на 2, затем на 3 и так далее. Данный процесс очевидным образом просто переполнит стек вызовов и приведёт к выбросу исключения переполнения стека.
А всё потому, что мы рассматриваем одну и ту же функцию f 2 раза в разных качествах: 1 – как функционирующий программный объект, 2 – как совокупность строк кода.
И точно так же, с учётом соответствующей специфики контекста, происходит при любом самореферентном доказательстве и парадоксе, то есть – при джиминации. Пример, возможно, не очень хорош, но тем не менее, он предоставляет хоть какое-то наглядное основание.

Вывод. В приведённых случаях в целом и в доказательстве Гёделя, в частности, нарушено правило рекурсивного проектирования за счёт реализации джиминации, и разные объекты рассматриваются под видом одного.

Заключение. Утверждение – верно, что и требовалось доказать.

P.S. Также заметим, что в данном случае наше доказательство делает неправомерным только само непосредственно доказательство Гёделя, а не доказывает то, что формальные системы могут быть полными и непротиворечивыми.
В сущности, наше доказательство как раз подчёркивает правоту Гёделя и демонстрирует это именно в экспликации джиминации, показывая, что даже в контексте строгих правил логического вывода могут как бы «сами собой» формироваться «сущности» и приобретаться «свойства», которых там не предполагалось самими правилами. Также добавим, что многократная джиминация лежит в основе всех апорий Зенона о движении.

P.S.2. Также здесь вспомним про наш пример о «Боге Гёделя». А конкретнее – про один из наших логических шагов в его контексте. Там мы формулировали высказывание о том, что Бог допускает потенциальную возможность тех или иных аспектов Мироздания, в том числе самого себя, быть или казаться (здесь несущественно что именно из этого) противоречивыми, что на самом деле подтверждает всемогущество и непротиворечивость самого Бога. Это можно сформулировать в виде некоего «яркого» высказывания, например, «Бог настолько непротиворечив, что его кажущаяся противоречивость лишь доказывает его непротиворечивость». На основе представленных нами здесь умозаключений, уже несложно догадаться, что мы здесь имеем дело всё с тем же самым феноменом джиминации и «двумя Богами», один из которых непротиворечив, а второй – противоречив. Здесь нам важно именно проиллюстрировать разницу, не сосредотачиваясь на том, чем же именно являются 2 наличия, после «разтождествления». Однако, можем указать, чем они могут, в некотором отдельном случае, являться по мнению Канта. Кант, в «Критике чистого разума» говорит следующее: «В самом деле, бесплодная попытка подменить логическую возможность понятия (поскольку понятие не противоречит само себе) трансцендентальной возможностью вещей (поскольку понятию соответствует предмет) может обмануть и удовлетворить разве только неискушенного человека» [с]. Здесь речь ведётся о подмене понятия «вещью», или означающего означаемым.
И да – это один из примеров, причём далеко не самый сложный. А вот в некоторых иных случаях, когда одно понятие подменяется самим же собой, но из иного контекста – разобраться бывает практически невозможно. И тем не менее, если очень внимательно присмотреться, то можно заметить, как при переходе от одного понятия к самому же себе, но в «другом месте», это изначальное понятие претерпевает изменения, порой несовместимые с собой же самим на «прежнем месте» и, по факту – становится совершенно иным – «вторым» понятием. Это и есть джиминация, которую мы так понятийно определили потому, что оба понятия, как и близнецы, на первый взгляд могут казаться одним и тем же человеком, но при внимательном рассмотрении будут обнаружены различия, по наличию которых можно будет утверждать, что перед нами на самом деле 2 разных человека. Грубо говоря, в достаточно богатой формально системе непротиворечивы и полны каждая из двух систем по отдельности – сами множества формальной системы с одной стороны и изоморфное им множество натуральных чисел, представляющих Гёделевские номера – с другой. А одной из лучших метафор здесь представляется: дерущиеся друг с другом близнецы.

5.10. ТЕОРЕМА О САМОРЕФЕРЕНТНОМ ДОКАЗАТЕЛЬСТВЕ

Описание. Здесь мы на примере покажем, на что способна рекурсия на аппарате формальной логики – и даже без построения сложных систем с гёделевской нумерацией.

Утверждение 1. В качестве доказательства нашей «Теоремы о самореферентном доказательстве» мы не предоставим никакого доказательства.
Утверждение 2. Если верно утверждение 1, то отсутствие доказательств делает теорему доказанной.

Вывод. Утверждение 2 – верно, что и требовалось доказать. Следовательно, отсутствие доказательства доказывает теорему.

Заключение. Укажем на то, что теперь в контексте логики любую теорему можно доказать по следующей схеме: «X истинно, так как Y», где Х – любое высказывание, Y – наша «Теорема о самореферентном доказательстве».

P.S. Если же попытаться хоть как-нибудь опровергнуть нашу теорему, то, само собой, придётся обращаться к мета-уровню по отношению к формальной логике и смотреть на ситуацию, как бы со стороны, что, собственно, и было доказано как Гёделем, так и Тарским. Можно сказать, что наша «Теорема о самореферентном доказательстве», хоть и доказана, но доказывает только сама себя, как бы рекурсивно. И, соответственно, отсутствие доказательств делает доказанной только нашу «Теорему о самореферентном доказательстве», но не «вообще» любую теорему – то есть можно попытаться упрекнуть нас в неправомерной индукции.
На это мы скажем, что, во-первых, точно таким же образом теорема Гёделя доказывает не неполноту и противоречивость «всей» формальной системы, а только лишь неполноту и противоречивость той самой самореферентной формулы, которая, собственно, и должна была доказать неполноту и противоречивость «всей» формальной системы.
Ну, а во-вторых: в принципе пытаться опровергнуть нашу теорему можно было только одним способом – вовсе её не читая. А теперь все попытки её опровержения – противоречат законам логики. И это потому, что, будучи доказанной в контексте некоторой формальной системы, теорема становится неотъемлемой частью «истинного» аппарата самой формальной системы и теперь на неё можно опираться при доказательстве последующих теорем, формулировке выводов и так далее.
То есть, на данный момент, пытаться вообще опровергнуть нашу теорему эквивалентно попытке утверждать, что «А» не равно «А». А кстати, давайте это докажем (почему нет?): «А не равно А, потому, что «Теорема о самореферентном доказательстве»». Вот, теперь высказывание «А не равно А» – истинно, ведь в качестве доказательства мы предоставили доказанную уже теорему о том, что отсутствие доказательства доказывает теорему.
Так. Но ведь если мы предоставили в качестве доказательства нашу теорему, то есть отсутствие доказательства, то получается, что мы всё-таки предоставили доказательство в виде его отсутствия, а значит уже не факт, что теорема доказана, так как в случае предоставления доказательства необходимо рассматривать уже само доказательство на предмет его корректности и непротиворечивости. С другой стороны, в качестве доказательства в рамках самой «Теоремы о самореферентном доказательстве» – отсутствие доказательства. Но ведь сам факт отсутствия доказательства в качестве доказательства – как отдельный феномен –  можно тоже считать доказательством? Или нет? Тогда получается, что даже наша теорема не доказана, так как там всё же есть доказательство в виде отсутствия доказательства.
С этой стороны, если там всё же не было доказательства, так как там «есть» «отсутствие доказательства», то наша «Теорема о самореферентном доказательстве» всё-таки доказана, а значит – истинна. А значит и теорема «А не равно А» – ничем не доказана, потому что мы не предоставили никакого доказательства, а значит она доказана и потому истинна, ведь в качестве доказательства…
Или всё-таки мы предоставили доказательства? С этой точки зрения, если отсутствие доказательства равно доказательству, то для чего вообще предоставлять доказательства хоть какой-нибудь теоремы – ведь отсутствие доказательства равно доказательству, а значит всё же «А не равно А»? Но ведь если отсутствие доказательства равно доказательству, то доказательство всё же имело место быть, а значит «Теорема о доказательстве» опять не доказана?
Но тогда получается, что если наша «Теорема о самореферентном доказательстве» не доказана потому, что в ней всё же имело место быть доказательство, то в контекст «истинного» аппарата формальной логики можно непротиворечиво ввести уже априори доказанную теорему в виде высказывания «наличие доказательства опровергает теорему». И после этого доказанными могут считаться только те теоремы, к которым не прилагалось никакого доказательства. Вот, например, к нашей «Теореме о самореферентном доказательстве» – не прилагалось. А значит она всё же доказана и «А не равно А». Или нет?
В общем получается нечто следующее: доказательства теоремы Гёделя о неполноте, Тарского о невыразимости истины, Тьюринга о неразрешимости проблемы остановки, парадокс Рассела, парадокс Бурали-Форти и прочие – правомерны и «А не равно А, так как «Теорема о самореферентном доказательстве»».

P.S.2. Конечно, данная теорема представляется несколько «игровой», но тем не менее – логически правомерной она как раз-таки является. И это несмотря на то, что она по форме и смыслу похожа на: высказывание о том, «в этом высказывании пять слов»; или на логическую цепочку «Аристотель не признавал возможности существования больших языковых моделей и их реализации на основе архитектуры фон Неймана, то есть, согласно закону исключённого третьего – отрицал такую возможность. А это неправомерно, так как эта возможность реализована и это нам известно, соответственно утверждение Аристотеля неправомерно, а так как все утверждения Аристотеля равны сами себе, то следовательно – все утверждения Аристотеля – ложны».
И это рекурсия на основе формальной системы логики. И «так можно».


P.S.3. В сущности, проблема доказательства является одной из наиболее фундаментальных в таких дисциплинах, как логика, математика и вообще естественные научные направления исследований. К примеру, в философии грань между обоснованием и доказательством достаточно размыта – достаточно обоснованное может априори считаться доказанным. Но в математике это не так – обоснование теоремы, в виде некоторых следствий из аксиоматики, представляет собой стартовую «стартовую точку» доказательства. По сути, доказательство – это процесс формирования модели, структурно-функциональные особенности которой не противоречат аксиоматике формальной системы. Затем, на основе модели воспроизводится ситуация, в рамках которой изначальные предположения «сближаются» с аксиомами в том смысле, что предпосылки «представляются» прямыми следствиями «развития» аксиом формальной системы согласно правилам её вывода. То есть чётких критериев и формализации доказательства, как и однозначной формализации алгоритма, не существует. Это всегда игра в моделирование.
Правила вывода также порой могут приводить к интересным последствиям. В общем то, большинство исследователей согласится с тем, что изначально в основе любой научной дисциплины лежат эмпирические данные. Но вот какие выводы мы имеем право делать на основе этих самых эмпирических данных? Фундаментом и базисом любых правил вывода для любой формальной системы (за возможным исключением совершенно нетипичных случаев) будут методы индукции и дедукции. Со времён Аристотеля повелось, что именно на дедуктивных выводах должно строится обоснование всякой теории. И это так неспроста, а потому что индукция, как метод научного познания, не способна обуславливать общие выводы и заключения. Но ведь эмпирические выводы и заключения всегда индуктивны.
Допустим, если мы исследовали миллион треугольников и пришли к выводу о том, что все они являются равнобедренными, а затем «на основе проведённого исследования» вывели, что «любой треугольник является равнобедренным», то правомерен ли наш вывод? Само собой разумеется, что нет – это просто пример ошибочной индукции на основе некоторой выборки эмпирических данных. И увеличение количества треугольников – ничего нам не даст, так как выводы могут продолжать быть строго неверными.
Вот потому дедукция и признаётся «лучше индукции», но, в свою очередь, посмотрим на её основания. От общего к частному – это понятно, но вот именно это общее – где оно «находится» и что из себя представляет? Что означает «дедуктивный вывод о треугольнике»? То есть, где существует такой треугольник, на основе которого мы можем судить обо всех треугольниках? Ответ здесь может быть только один – в аксиоматике и только в аксиоматике. «По-нашему» говоря – в mundus, той самой «математической реальности». И, если там указано, что «треугольник представляет собой геометрическую фигуру с тремя сторонами», то мы однозначно можем сказать, что-то вроде «существует три и только три прямые, которыми можно соединить точки на вершинах треугольника» – и это уже будет дедуктивно. На основе этого дедуктивного вывода мы можем построить дальнейшую импликацию: «если существует только три прямые, которыми можно соединить точки на вершинах фигуры, то из этого следует, что фигура является треугольником». Примерно так это работает.
Посмотрим чуть внимательнее. О физической реальности «треугольника», «поля», «частицы» или «волны» – речь здесь не идёт. Мы говорим именно о «аксиоматической метафоре», аспектами и особенностями которой относительно удобно оперировать. Как мы уже говорили – аксиомы вовсе не обязательно непременно «истинны» и не из-за «истинности» они не требуют доказательств, а потому, что сами есть основа некой модели «реальности» и специфическая «истина» этой реальности. Конечно, у математической модели имеются очень серьёзные претензии на описание реальности непосредственно объективной, так что при экспликации каких-либо противоречий с уже эмпирическими данными – аксиоматика пересматривалась. То есть теоремы и выводы – это «мазки», которыми дорисовывается «образ» модели, или же – что с нашей точки зрения эквивалентно – «сколы», за которыми понемногу проступает идеальная «скульптура». Первое или второе здесь происходит – не особенно важно, так как в любом случае ни то, ни другое – недоказуемо и неопровержимо одновременно.
Мы же здесь просто хотим сделать акцент на том, что дедуктивные доказательства в любом случае не имеют прямого отношения к реальному миру, а подпадают под юрисдикцию только лишь «аксиоматической модели», а индукция, как мы уже показали выше, не имеет полномочий должного уровня правомерности для экстраполяции полученных выводов на всю совокупность исследуемых объектов.
То есть, в любом случае, доказательство – это «игра со смыслами», потому что «Теорема о самореферентном доказательстве».

5.11. АКСИОМА РЕГУЛЯРНОСТИ И «МНОЖЕСТВЕ РАССЕЛА»

Описание. Для начала напомним определение аксиомы регулярности или, как ещё иногда её называют – аксиомы фон Неймана. Для любого непустого множества Х существует х, такой что х -> X, и пересечение х с Х равно пустому множеству. Это означает, что для любого множества должен существовать такой элемент, пересечение которого с этим исходным множеством по итогу «даёт» пустое множество. Также напомним для чего вообще понадобилась эта аксиома. Как предполагалось изначально, на основе этой аксиомы должна формироваться невозможность возникновения парадоксов, подобных парадоксу Рассела, то есть – отсутствие даже потенциальной возможности формирования циклов в контексте множеств. Укажем также на определение пустого множества – как множества, не содержащего ни одного элемента; пустое множество является подмножеством самого себя, причём единственным подмножеством (2 ^ 0 = 1), но не своим собственным элементом. То есть, определение указывает на то, что пустое множество не может принадлежать самому себе, так оно пусто по этому самому определению. Операция же пересечения двух множеств создаёт третье множество, которое содержит в себе только те элементы, которые принадлежат одновременно обоим исходным множествам. То есть, множество Х, являющееся пересечением множеств Y и Z, содержит только такие элементы х, для каждого из которых верно, что x -> Y и x -> Z.

Утверждение. Из аксиоматики Цермело-Френкеля в целом и аксиомы регулярности – в частности, не следует невозможности возникновения циклов и парадоксов, так как на её основе невозможно строго вывести того, что некое множество Х не может являться своим же собственным элементом. Следовательно, аксиоматика теории множеств Цермело-Френкеля – противоречива, так как допускает самореферентные парадоксы.

Доказательство. Сформируем множество Х, такое что Х = {{}, Х}. Выполняется ли аксиома регулярности? Множество Х содержит элемент, пересечение которого с самим Х равняется пустому множеству. Значит – аксиома выполняется. Но также верно и то, что множество Х – содержит само себя в качестве своего же собственного элемента и не нарушает этим самым ни аксиому регулярности, ни вообще хоть какой-либо пункт аксиоматики Цермело-Френкеля (с аксиомой выбора или без неё – в данном случае не особенно существенно), а даже буквально следует указаниям, содержащимся в аксиоме бесконечности. Мы здесь утверждаем, что аксиоматика Цермело-Френкеля не содержит явных указаний на невозможность построения подобного множества и из неё этого не следует явным образом, а значит – не следует вообще, раз уж мы придерживаемся должного уровня строгости.

Вывод. Утверждение – верно, что и требовалось доказать, соответственно, аксиоматика Цермело-Френкеля – противоречива.

Заключение. На основе того, что было нами показано в контексте доказательства, можно заключить, что аксиоматика Цермело-Френкеля внутренне противоречива, так как позволяет формировать циклы, а соответственно и парадоксы, подобные парадоксу Рассела и прочим из этой же категории.

P.S. Итак, на основе вышесказанного, убедившись, что аксиоматика Цермело-Френкеля нам этого точно не запрещает, представим себе несколько иное множество Х, где Х – Универсум Кантора, то есть множество всех возможных множеств. Ну и – здесь вопрос: содержит ли Х само себя? И неважно каков именно ответ на этот вопрос. А что здесь важно, так это то, что аксиоматика Цермело-Френкеля не предотвращает правомерности подобного вопроса и, как следствие, вся современная теория множеств превращается в «наивную теорию множеств 2.0». Ведь аксиоматика этого не запрещает.

P.S.2. Здесь мы предоставим нашу теорему в более строгом виде.
Теорема. Аксиоматика теории множеств Цермело-Френкеля противоречива, так как допускает существование множества, которое содержит само себя в качестве элемента.
Доказательство.
Аксиома регулярности: Для любого непустого множества (X) существует элемент (x), такой что (x ∈ X) и пересечение (x) с (X) равно пустому множеству, т.е. (x ∩ X = {}).
Определение множества. Рассмотрим множество (X = {{}, X}).
Проверка аксиомы регулярности.
Элемент (x = {}) принадлежит множеству (X) (т.е. ({} ∈ X)).
Пересечение (x) с (X) будет равно: [{} ∩ X = {}]
Таким образом, аксиома регулярности выполняется.
Содержит ли (X) само себя?
Мы видим, что (X) содержит элемент (X) (т.е. (X ∈ X)).
Вывод. Мы показали, что множество (X) удовлетворяет аксиоме регулярности и одновременно содержит само себя в качестве элемента. Это противоречит утверждению, что никакое множество не может содержать само себя в качестве элемента.
Следовательно, аксиоматика теории множеств Цермело-Френкеля противоречива.

P.S.3. Собственно говоря, а с чего это аксиоматика Цермело-Френкеля противоречива, если мы этого однозначно не доказали? Ну да, мы построили множество, содержащее само себя, но, если это не противоречит аксиоматике, то она и не является противоречивой, а если бы это противоречило аксиоматике, то мы и не смогли бы его построить. Напомним же, для чего именно и по какой причине вообще создавалась аксиоматика Цермело-Френкеля. Она создавалась для чёткого и непротиворечивого определения того, что же такое множество и каковы его ключевые характеристики. Так как Цермело считал, что причина парадоксов теории множеств заключается в том, что само понятие множества не является чётко определённым. Задачей аксиоматики было просто дать строгое определение множеству таким образом, чтобы парадоксы не смоги возникнуть, то есть, чтобы понятие множества стало непротиворечивым. И за всё время существования аксиоматики на её основе ни разу не было найдено ни одного парадокса – Цермело лично утверждал невозможность подобного.
Таким образом, получается, что наша теорема – неверна и аксиоматика не содержит внутренних противоречий? И это так потому, что наше вышеприведённое множество Х – не опровергает аксиоматику, так как оно не противоречит ни одному из её пунктов?
А вот здесь зададим вопрос: выполнила ли аксиоматика Цермело-Френкеля то, ради чего, собственно, она и создавалась? Если Х – множество всех множеств, то следуя аксиоматике Цермело-Френкеля, мы не можем утверждать того, что Х не может принадлежать самому себе, а значит – Х может принадлежать самому себе и наше вышеприведённое множество Х – также возможно. Таким образом, если Х – множество всех множеств и оно принадлежит самому себе, следовательно, Х – не может быть множеством всех множеств, так как если оно принадлежит самому себе, то это означает, что оно не равно само себе и так далее. Это и есть парадокс Рассела. На основе этого сразу же возникают давно забытые вопросы о множествах оридиналов и кардиналов. То есть – парадоксы «наивной теории множеств», плавно перетекающие в парадоксы «наивной теории множеств 2.0». Это потому, что аксиоматика Цермело-Френкеля не позволяет строго и чётко определить понятие множества, а потому – она противоречива и наша теорема на самом деле – верна, так как определение множества Х противоречит самому себе, так как получается, что множество Х, как множество всех множеств – не равно самому себе в любом случае.

P.S.4. И последнее. Если попытаться опровергнуть существование нашего множества Х с точки зрения его несоответствия аксиоме степени, так как оно может не позволить корректно сформировать множество всех подмножеств для множества Х, то это будет эквивалентно непризнанию аксиомы выбора и возможности получения упорядоченных множеств – в частности и вообще актуально бесконечных множеств – в целом. Посмотрим, на основе чего можно выстроить подобную аргументацию – сделаем это сами.
Для тех, кто не является сильно искушённым в вопросах сущности рекурсии, мы ниже предоставим пояснения. Посмотрим ещё раз на наше множество Х. Оно выглядит следующим образом –{{}, Х}. И в данном случае Х – это не просто символ, а именно то, что под этим самым символом подразумевается – множество. Теперь давайте попробуем применить к нашему множеству аксиому степени и, соответственно, сформировать множество Y, которое будет представлять множество всех подмножество нашего множества Х. Само собой, мы не можем знать заранее, с каким именно множеством мы имеем дело, так что применять мы будем, конечно, рекурсивный алгоритм. Изначально наше множество Y будет пустым – {}, но по мере рекурсивного обхода исходного множества Х оно должно будет заполняться элементами. Заранее скажем, что, так как множество всех подмножеств для любого множества должно быть равно 2, возведённой в степень размера исходного множества, то наше формируемое множество Y должно будет иметь размер равный 4, так как исходное множество Х имеет размер – 2, а 2 в степени 2 = 4. И кстати сказать, вот теперь уже аксиома выбора становится важной и мы определяем функцию выбора на нашем множестве Х. Итак, начнём рекурсивный обход множества Х. Изначально мы «берём» пустое множество – {} и помещаем его в наше множество Y. Теперь оно выглядит так {{}}. Затем мы переходим к следующему элементу множества Х, который, собственно, и представлен самим множеством Х. Просто «взять» его – мы не можем, так как, согласно аксиоме объединения, мы должны извлечь элементы и поместить их в новое множество, которое мы здесь и формируем – множество Y. Если элементом является пустое множество, то мы его же и «берём», но, если элементом является множество, и оно не пустое – мы должны предварительно разобрать его на составляющие. Тогда рекурсивно «войдём» в этот элемент. Далее вновь «возьмём» первый элемент этого элемента, то есть пустое множество – {} и поместим его в множество Y, которое теперь будет выглядеть {{}, {}}. Собственно, этот его вид не вполне корректен, так как мы всё же имеем дело с множеством, а не с мультимножеством – а принадлежать множеству могут только уникальные элементы. Значит, корректный вид множества Y для нашего случая – {{}}. Затем перейдём к последующему элементу, который вновь представлен самим множеством Х. Значит вновь рекурсивно «войдём» в него. «Возьмём» первый элемент актуально рассматриваемого множества – {}. Поместим его в конструируемое множество, и оно, если бы было мультимножеством, стало бы выглядеть так – {{}, {}, {}}. Но, так как мы формируем именно множество – его внешний вид не изменится вообще и останется – {{}}. Ну и, собственно, так до бесконечности – мощность множества Y останется равна 1.
В ответ на подобную возможную критику мы укажем, что почему-то ни в каком ином случае бесконечность не мешала формировать множества – какие заблагорассудится. Если есть некое множество Х, то мы всегда просто можем сказать, что «допустим Y – булеан Х». То есть, если актуальная бесконечность служит ключевым фактором доказательства несчётности некоторых множеств и вообще – краеугольным камнем теории множеств, то мы можем просто определить множество Y, как множество всех подмножеств нашего множества Х и сказать, что оно – существует, хоть и является актуально бесконечным. Это не будет нарушать аксиому степени вовсе, так как она не задаёт конкретного способа формирования множества всех подмножеств, а просто утверждает, что его можно сформировать. В этом смысле аксиома степени эквивалентна аксиоме выбора, которая также не предоставляет никакого конструктивного способа выбора элемента из множества. И вот в нашем случае – можно ровно так же, правда множество всех подмножеств множества Х будет существовать только при условии наличия актуальной бесконечности, как, собственно, и вообще сама теория множеств целиком.
Если же сказать, что актуальная бесконечность «работает» только тогда, когда мы потенциально способны успешно завершить процесс, то это приведёт уже к совсем иным последствиям (глобального, собственно, масштаба), которые будут нами рассмотрены далее.
Также считаем нужным добавить, что в некотором роде сама аксиоматика подразумевает возможность дуального смысла возникающих в её рамках противоречий. То есть, если мы выводим из аксиом некоторую теорему (или же постулируем, если речь о философской теореме), затем развиваем её логически, с непосредственной опорой на правила вывода формальной системы, и приходим к некоему противоречию с самими аксиомами, то это можно толковать двояко.
С одной стороны, мы можем утверждать, что аксиоматика противоречива, ведь на ней основываясь и применяя правила вывода, мы пришли к некоему противоречию.
Но с другой стороны, мы можем утверждать то, что аксиомы как раз и служат в качестве локальных «законов физики» формальных систем и потому в аксиоматике системы, при отсутствии прямых противоречий между аксиомами, не может возникать противоречий в принципе. То есть, исходя из второго утверждения, придя к какому-либо противоречию в контексте системы, мы можем его нивелировать ссылкой просто на то, что оно противоречит аксиомам. Ещё раз, даже если противоречие выводится явным образом, но оно не являет собой непосредственное противоречие между аксиомами, то оно может быть признано недействительным просто по определению, и не особенно значимо, в чём именно там будет проблема: в логическом шаге, модели ситуации или совокупности аксиом из которых выводится теорема – это просто «не считается». Таким образом аксиоматика формальной системы может быть признана противоречивой только в одном случае – если одна из аксиом прямо утверждает отрицание того, что утверждается в другой.
При применении второго подхода к теореме Гёделя, можно было бы просто сказать, что в таком случае введём в контекст любой достаточно богатой формальной системы 2 дополнительные аксиомы. Само собой разумеется, что Гёдель показал бесплодность попыток разрешить противоречия за счёт введения дополнительных аксиом. Но мы бы могли ввести совершенно особые – аксиому «полноты» и аксиому «непротиворечивости». После чего указать на то, что наша формальная система, хоть и является достаточно богатой и, соответствует критерию систем, подпадающих по теорему Гёделя, но она – согласно её же аксиоматике, является как полной, так и непротиворечивой. А потому выводы Гёделя неправомерны.
Но по такому пути не пошла даже «чистая математика», иначе полная утрата не только связи с реальностью, но также и вообще хоть какой-то адекватности, не заставили бы себя ждать. Соответственно, наука всё же старается соответствовать тем реалиям, которые задаёт практика. Поэтому нам, как в общем-то и научному сообществу в целом, представляется более адекватным подход именно первый.
Более того, нахождение некоторых противоречий в контексте системы свидетельствует о том, что с системой скорее всего действительно что-то не так. Иначе бы «наивная теория множеств» могла бы отреагировать на парадокс Рассела просто установив в качестве дополнительной аксиомы невозможность множества всех множеств принадлежать самому себе – в качестве своеобразного исключения. Но тогда бы это вылилось в необходимость создавать такой же класс «исключительных» множеств и для кардиналов с ординалами. То есть, получилось бы, что у нас есть некоторое множество «исключительных» множеств. Следовательно – мы сформировали бы ещё более «качественную» основу для того же парадокса Рассела, от которого старались «сбежать».
Исходя из вышесказанного, если некоторое высказывание выводится из теорем и, при своём логическом развитии, противоречит какой-либо из, соответственно, теорем – значит какие-то проблемы именно у самой аксиоматики. С этих позиций критика нашего, ранее приведённого, множества Х, которое не нарушает аксиому регулярности, но потенциально способно вступить в противоречие с аксиомой степени – не корректно.

5.12. ПАРАДОКС ПУСТОГО МНОЖЕСТВА

Описание. Данный фрагмент является логическим и концептуальным продолжением предыдущего. Напомним, что наше множество Х – {{}, X}. Попробуем ещё раз проверить, может ли подобное множество существовать. Но теперь – с иных позиций. Напомним, что в прошлый раз нам не удалось однозначно доказать противоречивость аксиоматики Цермело-Френкеля, так как мы не смогли прийти к строгому выводу о том, может ли существовать наше множество Х.

Основания. Аксиома регулярности: для любого непустого множества Х существует х, такой что х -> X, и пересечение х с Х равно пустому множеству.
Свойство пустого множества: пересечение любого множества с пустым множеством равно пустому множеству.

Утверждение. Аксиоматика Цермело-Френкеля либо не позволяет строго определить понятие пустого множества, либо не позволяет строго определить понятие множества вообще. Это следует из эквивалентности понятия пустого множества и понятия отсутствия пересечений множеств.

Доказательство. Приведём две совершенно эквивалентные в рамках аксиоматики теории множеств Цермело-Френкеля «версии» аксиомы регулярности.
1.	Для любого непустого множества Х существует элемент, пересечение которого с Х равно пустому множеству.
2.	Для любого непустого множества Х существует такой элемент, который не пересекается с Х.
Если мы скажем, что «для любого непустого множества Х существует такой элемент, который не пересекается с Х», то наше вышеприведённое множество Х – не может существовать, так как все элементы Х пересекаются с Х. Посмотрим внимательно на множество Х – {{}, X}. Пересекается ли Х с Х? То есть – является ли Х элементом Х? Да, пересекается и да – является. Пересекается ли Х с пустым множеством? То есть – является ли пустое множество элементом Х? Да, пересекается и да - является. Значит не существует такого элемента множества Х, который не пересекается с Х, следовательно, аксиома не соблюдается и наше множество Х – не может существовать в контексте аксиоматики теории множеств Цермело-Френкеля, так как аксиома регулярности этого не допускает.
Но, если мы скажем, что «для любого непустого множества Х существует такой элемент, пересечение которого с Х равно пустому множеству», то наше множество Х – может существовать, так как у нас есть такой элемент, пересечение которого с Х равно пустому множеству. Посмотрим внимательно на множество Х – {{}, X}. Пересечение Х с Х точно не равно пустому множеству. Чему именно оно равно, разумеется, вопрос уже сам по себе интересный и может также привести к нетривиальным умозаключениям, но пересечение не равно пустому множеству и это точно. А вот чему равно пересечение Х с пустым множеством? Согласно приведённому нами выше свойству пустого множества – оно равно пустому множеству. А это значит, что у нас в множестве Х есть элемент, пересечение которого с Х равно пустому множеству. Следовательно, аксиома регулярности соблюдается и множество Х – может существовать, не нарушая аксиоматику. То есть, аксиоматика Цермело-Френкеля в целом и аксиома регулярности в частности допускают существование множества, которое может принадлежать самому себе.
Итак, однозначно, может ли оно существовать или нет?
Из вышесказанного ещё раз следует, что «отсутствие пересечений» и «пустое множество» – эквивалентные для некоторых случаев в контексте теории множеств формулировки, так как известно, что «отсутствие пересечения» и «пересечение равно пустому множеству» – одно и то же.
Следовательно, аксиоматика требует того, чтобы либо не каждое пересечение было равно пустому множеству, либо утверждает, что пустое множество – не гарантирует отсутствия пересечений.
Таким образом, наше множество Х само по себе являет парадокс в том смысле, что мы не можем однозначно утверждать, нарушает оно аксиому регулярности, а вместе с ней и всю аксиоматику Цермело-Френкеля или же наоборот – находится строго в заданных аксиомами рамках.
Так как, если множество Х нарушает аксиоматику и, соответственно, не существует, то пересечение с пустым множеством не равно пустому множеству для множеств, содержащих пустое множество. А следовательно – понятие пустого множества нуждается в переопределении.
А если множество Х не нарушает аксиоматику и, соответственно, существует, то все циклические множества, включая парадокс Рассела – возможны и, более того – правомерны в рамках аксиоматики. А, следовательно, аксиоматика не выполняет свою задачу в том смысле, что не позволяет чётко определить понятие множества вообще, так как, к примеру, не существует однозначного ответа на вопрос о том, должно ли принадлежать само себе множество всех множеств или на вопрос о том, может ли существовать само наше множество Х.

Вывод. Утверждение – верно, что и требовалось доказать.

Заключение. Здесь мы показали, что именно в пересечении множеств и в том, что пересечение любого множества с пустым множеством равно пустому множеству – и кроется нюанс неопределённости аксиоматики Цермело-Френкеля. То есть либо в её рамках не является чётко определённым понятие пустого множества, либо вообще – понятие множества.

P.S. Вообще говоря, мы здесь и сейчас, хоть и в гораздо менее строгой форме и не особо формальной версии, сделали ровно то же самое, что и Гёдель в своих теоремах. Конечно же, мы имеем ввиду – в широком смысле «то же, что и Гёдель». Оба пункта применены: дуальность и рекурсия. Результат получен тот же – противоречие.
Смысловой краеугольный камень заключается в том, что «вижу ничего» и «ничего не вижу» – в общем-то эквивалентные высказывания естественного языка. Однако именно в контексте формальной системы – аксиоматики теории множеств в данном случае – это и оказывается недопустимым, так как различные высказывания с одним и тем же (вроде бы) смыслом (читай – Гёделевским номером) «запускают» различные логические цепочки и, соответственно, приводят к разным последствиям, которые противоречат друг другу.
В естественных языках за подобное никаких «санкций» не предусмотрено – люди привыкли так высказываться. Но, вообще говоря, данные высказывания не являются строго эквивалентными, хотя и кажутся таковыми. Их эквивалентность обычно подразумевается и в естественном языке и, как видим, в теории множеств также. Но они ведь «говорят» о совершенно разных ситуациях: «ничего не вижу» представляет собой «ссылку» на состояние субъекта, который ничего не способен разглядеть, так как он один в ситуации, так как больше «ничего нет»; а вот «вижу ничего» это уже отсылка к наличию «чего-то» ещё в контексте ситуации, чего-то, что субъект наблюдает, пусть это самое «что-то» и представляет собой «ничто» – порой «ничто» это уже «что-то». Впрочем, об этом моменте мы подробно скажем далее. Здесь же повторим, что в естественных языках подобное не подразумевает противоречий. Но вот в теории множеств это являет собой основу для парадокса, так как отсутствие пересечений множеств имеет смысл в том, что «в множествах отсутствуют равные элементы», а не в том, что «пересечение множеств равно пустому множеству». Но, ввиду того, что эти высказывания эквивалентны в теории множеств – возникает парадокс, который мы описали.
Ну и, в качестве некоей «вишенки на торте», уже можно заметить, что это самое разделение одного и того же аспекта «пересечения с пустым множеством» при рассмотрении нашего множества Х в контексте аксиоматики Цермело-Френкеля, представляет собой пример феномена, о котором мы уже говорили ранее. Речь идёт, конечно, о феномене джиминации, которая в данном случае есть демаркация пустого множества на два различных пустых множества в разных, соответственно, контекстах. И в одном из них оно являет собой гарант отсутствия пересечений множеств, а в другом – элемент множества, для которого нам нужно установить отсутствие пересечений. Даже звучит рекурсивно. И разумеется, что в одном из этих контекстов множество Х может существовать, а в другом нет. В этом и заключается противоречие и основа для найденного нами парадокса.

5.13. ТЕОРЕМА О НЕВОЗМОЖНОСТИ ЧИСЛЕННОГО ПОСТРОЕНИЯ КОНТИНУУМА

Описание. Ранее, в разделе о теории множеств, мы уже говорили о нигде не плотном, совершенном множестве Кантора, которое определяется как Канторово множество. Напомним метод его построения. Возьмём множество вещественных чисел на закрытом диапазоне [0, 1] и удалим из него среднюю треть, то есть множество всех элементов на открытом диапазоне (1/3, 2/3), и получим множество, состоящее из двух множеств на диапазонах [1, 1/3] и [2/3, 1]; затем применим рекурсивно первый шаг к каждому из имеющихся на данный момент множеств и получим разделение каждого ещё на два; и так далее вплоть до «актуальной бесконечности». И в итоге мы получим некую структуру, состоящую из бесконечного количества элементов (вообще точек, но так как мы имеем дело с множеством, то можем правомерно говорить – элементы), но при этом без интервалов, и имеющее нулевую меру Лебега. В данном контексте интересно то, что в общем-то представляется очевидным – сколько ни удаляй среднюю треть – элементы в множестве всё равно останутся и оно никогда не станет пустым. В числовом же представлении, а именно в третичной системе счисления, Канторово множество будет представлено всеми числами, запись которых содержит только 0 и 2 – единиц там не будет.

Утверждение 1. Множество всех вещественных чисел не может быть построено конструктивными численными методами.
Утверждение 2. Любой подход к актуально бесконечному множеству всех вещественных чисел базируется на допущении о том, что «числа уже даны», и на «вере» в то, что «они там есть», то есть не существует доказательства непрерывности континуума.

Доказательство. Реверсируем процесс Кантора с внесением небольших изменений. Итак, у нас есть некоторое «актуальное ничто». Мы хотим построить актуально бесконечное множество всех вещественных чисел на закрытом интервале [0, 1]. Возьмём для начала «элемент 0» и «элемент 1». Расположим их в каких-либо «локациях» и условно проведём между ними отрезок. Теперь разделим этот отрезок на 2 и получим «элемент 1/2». Поместим его в соответствующую «локацию». Теперь у нас есть уже три условных «точки» (условных потому, что вообще «длина» точки равна нулю, мы же для нужд эксперимента предполагаем её длину в виде инфитизимали – бесконечно малой, что вполне корректно с точки зрения нестандартного анализа). Поделим «расстояние» между «элементом 0» и «элементом 1/2» на 2 и получим «элемент 1/4» – поместим этот элемент в соответствующую локацию. Затем поделим «расстояние» между «элементом 1/2» и «элементом 1» и получим «элемент 3/4». Поместим и его в соответствующую позицию. Ну и, собственно, так далее – вплоть до «актуальной бесконечности».
Теперь вспомним ранее нами показанное доказательство бесконечности континуума: для каждого числа Х в континууме есть число Х/2, которое также находится в континууме. Следуя нашему методу, получим ли мы по итогу континуум?
Пока без ответа. Теперь попробуем реверсировать процесс Кантора буквально – может быть это поможет. Итак, у нас также изначально есть «элемент 0» и «элемент 1». Поместим их в соответствующие локации. В противовес Кантору мы будем не удалять, а, само собой, заполнять. Так что, мы находим среднюю треть для наших уже локализованных элементов. Она будет представлена сразу двумя новыми элементами: «элемент 1/3» и «элемент 2/3». Поместим их в соответствующие локации. У нас получилось сразу 3 условных «пустых» отрезка. Для каждого из них применим операцию нахождения и заполнения средней трети. И так вплоть до «актуальной бесконечности». Опять тот же вопрос: получим ли мы по итогу континуум?
Как в прошлый раз, так, соответственно, и в этот, ответ один – очевидно, что нет, не получим, так как в наших актуально бесконечных множествах будут отсутствовать иррациональные числа.

Вывод. Оба утверждения – верны.

Заключение. Так как мы не можем определить операцию для численного конструктивного построения континуума, то и доказать его существование, как именно целого и непрерывного, мы тоже не можем.

P.S. Мы способны определить множество вещественных чисел на диапазоне [0.5, 2] и попытаться его построить за счёт применения какого-либо метода аппроксимации квадратного корня от числа 2, который как известно является иррациональным числом и приблизительно равен 1.4142135623730951. Однако сразу же станет очевидно, что таким путём мы будем некоторое время получать только иррациональные числа, да и то – просто стремящиеся к 1. То есть таким путём мы тоже не сможем построить континуум.
Если же мы здесь сделаем вывод о том, что для построения континуума необходимо «сразу несколько» операций, то от этого тоже «хорошо не станет». Во-первых, потому, что такое построение смотрится настолько искусственным, что просто «допустить» наличие иррациональных чисел и непрерывности континуума и то представляется менее притянутым. Ну, а во-вторых, что куда важнее – подобная постановка вопроса сразу же приводит к постановке уже следующего, возможно, куда более сложного вопроса: какое количество операций (точное их число) может гарантировать построение континуума? Если немного вспомнить историю математики, то можно заметить, что и отрицательные, и иррациональные, и комплексные числа изначально считались «не настоящими» числами. Есть ли у нас гарантия того, что те арифметические операции, которые нами используются на данный момент, могут заполнить континуум так, что там точно больше «ничего не влезет»? Собственно говоря, диагональный аргумент Кантора, если принимать его в качестве истинного доказательства, как раз и показывает несчётность континуума, из чего можно, кроме классического, сделать также вывод и о том, что существуют такие числа, которые мы на данный момент вычислить не способны. Подчеркнём, что мы не пытаемся этого утверждать, а всего лишь указываем на то, что отрицание этой возможности на данный момент не является доказанной теоремой.
Наличие отрезка, соединяющего точки А и В, следует из аксиом Евклида, а вот наличие взаимно-однозначного этому отрезку численного множества – не следует, а просто «допускается».

5.14. ТЕОРЕМА О «РАЗРЫВАХ» В КОНТИНУУМЕ

Описание. Не является секретом, что иррациональные числа представляют собой своеобразные «идентификаторы», которые определяют последовательности чисел, к ним стремящихся, но никогда их не достигающих. По крайней мере, именно таким образом их определил Кантор, а иные определения – являются эквивалентными определению. Собственно говоря, так рассматриваются вообще все вещественные числа, но в прошлом примере мы показали, что сформировать множество рациональных чисел можно очень простыми методами. Поэтому мы в данном случае будем полагать, что такими «идентификаторами» являются иррациональные числа и только они, например, тот же квадратный корень из числа 2, число Пи, число Эйлера и прочие. Разделять их на алгебраические и трансцендентные мы пока не станем – на данный момент это без надобности.
Поясним, что значит идентификатор, и почему мы полагаем корректным именно такое определение. Допустим, возьмём последовательность чисел 1, 1.4, 1.41, 1.414 и так далее. Собственно, её можно продолжать потенциально бесконечно, и она генерируется по методу нахождения квадратных корней чисел, разработанному Ньютоном. Конкретно в нашем случае – это приближение квадратного корня из числа 2, и оно стремится к числу 1.4142135623730951, которое и представляет собой иррациональный идентификатор (более или менее общепринятый). Чтобы было немного яснее, мы повторим и акцентируем внимание на том, что этот идентификатор не является точным квадратным корнем из числа 2, так как его не существует вообще в виде целого или рационального числа. Собственно, мы даже можем его существенно улучшить, если не пожалеем времени и ресурсов – его точность можно задавать и регулировать – и потом, после улучшения, уже сможем говорить, что на самом деле число стремится к некоему Х, где Х – не 1.4142135623730951, а более точное приближение.
Само собой разумеется, что континуум не построишь подобным способом, как мы и показали ранее. Но здесь дело в ином. Для тех случаев, когда мы уверены в том, что делаем, и в том, к чему нас это приведёт, мы можем сказать «…и так до бесконечности», а потом добавить «…и в итоге у нас получилось Х», где Х – «представление» об актуально бесконечном «чём-то». Так говорили мы, когда строили бесконечное множество рациональных чисел. Но тут, в случае с числами иррациональными, ситуация принципиально иная. И связано это именно с понятием актуальной бесконечности и корректным использованием этого понятия для обоснования и конструирования тех или иных математических объектов.
Иррациональное число, повторим в очередной раз, не может быть получено при помощи применения стандартных арифметических операций к числам рациональным. Но оно может быть выведено алгоритмически, на основе других чисел, то есть аппроксимировано. И загвоздка тут в том, что мы никогда не сможем сказать, что «мы наконец-то вычислили ИСТИННЫЙ корень из числа Х», где «Х» – любое число, не являющееся квадратным. И это, повторим, потому, что просто не существует квадратного корня для числа, не являющегося квадратным. Метафорически – не существует «цели», в которую мы хотим выстрелить, а есть лишь «направление», в котором просто можно стрелять.
Собственно, в данном случае нам вовсе нет нужды быть голословными и довольствоваться только логикой и философией – попытки определения иррациональных чисел зачастую прямо свидетельствуют в пользу их отсутствия в конструктивном виде – обратимся за помощью к самим же математикам, причём не только к Кантору. В контексте рассмотрения данной теоремы мы в том числе рассмотрим работы одного из выдающихся немецких математиков 19-ого века Рихарда Дедекинда, ученика Гаусса и одного из пионеров обоснования непрерывности поля вещественных чисел, в частности к его труду «Непрерывность и иррациональные числа» [c].

Утверждение. Континуум не является непрерывным – в нём наличествуют «разрывы».

Доказательство. Допустим, мы примем в качестве валидного метод конструктивного численного построения континуума путём применения вообще всех возможных алгоритмов генерирования последовательностей чисел из нам известных. То есть тот метод, который мы применяли ранее, нахождение квадратных корней, нахождение кубических корней и так далее, и прочее, и прочее. По большому счёту, мы хотим применить метод заполнения континуума, используя метод генерирования рациональных чисел, который был показан нами ранее. Для заполнения же «разрывов» в виде отсутствия иррациональных чисел, то есть условных «пустот» на их «локациях», мы хотим применить любые известные нам методы получения иррациональных чисел.
Мы покажем на примере нахождения квадратного корня по методу Ньютона. Допустим, мы хотим найти квадратный корень для числа 2. Тогда формула для каждой итерации будет выглядеть так «x = (x + 2 / х) / 2», где «х» – число, которое служит «аккумулятором» накопления изменений в ходе приближения. Здесь мы сначала прибавляем к «х» то число, для которого ищем квадратный корень, затем суммируем его с «х» и наконец, делим на 2 (искали бы корень кубический – здесь делили бы на три и так далее). Ну и так, собственно, до бесконечности. По итогу мы получаем некоторое «среднее» приближение.
Собственно, этот алгоритм должен нам что-то отдалённо напоминать. А именно – наш собственный метод заполнения «отрезка» рациональными числами, где мы показали, что невозможно построить континуум, используя метод деления отрезков и заполнения какой-либо их части. По сути, это то же самое, только с незначительными нюансами и деталями. И поэтому здесь, так как мы используем, как уже сказали, тот же метод отрезков, мы никогда не сможем найти точного квадратного корня от числа, не являющегося квадратным, и просто будем бесконечно продолжать «заполнять середины», «заполнять трети» (для приближения кубических корней чисел, не являющихся кубическими) и тому подобное, как мы делали при построении множества рациональных чисел. Повторим – однозначно определённой цели просто не существует и это вовсе не является секретом.
Отсюда следует, что континуум никогда не будет заполнен ни при каких численных методах, а значит, в нём есть «разрывы», которые не могут быть конструктивно заполнены.
И вот примерно в подобной же смысловой «локации» в ходе такого же формирования модели, Дедекинд пишет следующее «… сравнение области рациональных чисел с прямой привело к открытию в первой изъянов (Lückenhaftigkeit), неполноты, или разрывности, между тем как прямой мы приписываем полноту, отсутствие пробелов, непрерывность» [c]. То есть, мы с Дедекиндом пришли к одному и тому же – точка «концептуальной бифуркации» у нас достигнута. Дедекинд, как и мы здесь, попытался «заполнить» континуум и ему это, разумеется, также не удалось. Ну, нам уже на данном этапе вроде как понятно, что «разрыв» останется, так как он конструктивно определён, а значит – существует. Но вот эта биекция с геометрическим отрезком, она, конечно, не даёт покоя и полностью, казалось бы, противоречит нашей интуиции – отрезок «целый», множество эквивалентных ему чисел – нет. Такого быть не должно. Поэтому Дедекинд идёт дальше и, опираясь на «точку отрезка», постулирует следующее: «Если точки прямой распадаются на два класса такого рода, что каждая точка первого класса лежит влево от каждой точки второго класса, то существует одна и только одна точка, которая производит это разделение прямой на два класса, это рассечение прямой на два куска» [c]. Дедекинд сам называет это «принципом непрерывности» и сам, за что ему честь и хвала, делает акцент на том, что вывести и доказать этот принцип не представляется возможным, поэтому – принцип постулируется. Далее он указывает, что «Принятие этого свойства прямой линии есть не что иное, как аксиома, посредством которой мы только и признаем за прямой её непрерывность, мысленно вкладываем непрерывность в прямую» [c].
Добавим, что обоснование Дедекинда было приведено просто для примера – на самом деле все (Дедекинда, Больцано-Вейерштрасса, Коши-Кантора, Гейне-Бореля и прочие) обоснования иррациональных чисел полагаются эквивалентными.
В общем, исходя из вышесказанного, иррациональное число может быть определено и ещё одним способом – аксиоматически. То есть, если реально «найти», «вывести» и «сконструировать» его не получается совсем никак, то мы просто можем сказать, что оно существует потому, что «ну не может же его не существовать» и полагать это высказывание единственным доказательством, которое даже не необходимо – аксиомы в них не нуждаются. Собственно, в данном случае, при взгляде на геометрический отрезок, интуиция подсказывает нам ровно то же самое – не может же его не существовать.
Однако, как показали и мы, и сам, собственно, Дедекинд – именно арифметически его и правда не существует и доказать отрицание этого утверждения мы не способны. Таким образом – континуум невозможно заполнить числами. Для того, чтобы показать это ещё нагляднее мы, ни много ни мало, дадим определение иррационального числа. Итак, иррациональное число – это численный идентификатор некоего конкретного этапа потенциально бесконечного вычислительного процесса, «развёртывающегося» в направлении некоего диапазона между двух рациональных чисел. То есть, то, что «сокрыто» под иррациональным числом, то что, является его «означаемым» – это не «точка» и не число – это скорее «инфинитизималь» Лейбница – это именно потенциально бесконечный вычислительный процесс. Вот в такой репрезентации интуицию «всё устраивает» – процесс также может заполнять отрезок.

Вывод. Утверждение верно, что и требовалось доказать.

Заключение. Таким образом, геометрический отрезок, соединяющий точку «0» с точкой «1», и актуально бесконечное множество всех вещественных чисел на закрытом диапазоне [0, 1] не являются биективными, так как не существует взаимно-однозначного соответствия между точками отрезка и элементами множества. Метафорически говоря, при аппроксимации иррациональных чисел сам этот процесс заведомо искусственно останавливается и наличествующие пустоты в континууме как бы «заклеиваются» достаточно «большим» и «громоздким» для их «истинных размерностей» числом, служащим в виде «заплатки». Таким образом, создаётся внешнее впечатление «заполненности» континуума. На самом же деле под подобной «заплаткой» наличествуют те самые «разрывы», о которых мы говорим, и которые «соответствуют» иррациональным числам.
По сути, множество всех вещественных чисел иньективно геометрическому отрезку. И скажем ещё раз, что не существует конструктивного численного метода построения континуума, если принимать его в классическом непрерывном варианте. Если же следовать нашему доказательству, то континуум не непрерывен, но может быть конструктивно построен численными методами.

P.S. Теперь немного объясним про упоминание нами актуальной бесконечности в данном фрагменте. Ещё раз, если мы вообще принимаем понятие актуальной бесконечности в качестве правомерного (разумеется, крайне дискуссионный аспект), то мы имеем право сказать «…и так до бесконечности», а потом добавить что-то вроде «…по итогу мы получили Х» в тех случаях, когда процесс является закономерным, как в случае генерирования нами рациональных чисел. Дополним, что подобный процесс обязательно является «потенциально успешным», то есть, если ему и правда предоставить бесконечно много времени, то он заполнит всё то, что в принципе способен заполнить, так как с каждым шагом он просто всё лучше и лучше «делает свою работу». Но для тех случаев, когда мы совершенно не представляем, куда движемся – как в случае аппроксимации чисел иррациональных – такого права у нас уже нет. «Если стрелять куда попало, то попадёшь куда-нибудь» – правомерное высказывание, но «Если стрелять куда попало, то попадёшь в Х», где Х представляет собой конкретную и заранее определённую цель – не правомерно, так как не способно гарантировать успех.
И интересной здесь представляется специфика использования понятия актуальной бесконечности. Когда нам концептуально «выгодно», чтобы она неограниченно развивалась, то мы предоставляем ей полный карт-бланш – как, например, в случае с диагональным аргументом Кантора, где мы (уже должно быть понятно, что совершенно не правомерно) представляем множество всех вещественных чисел. Для тех же случаев, в которых мы действительно имеем дело с бесконечным процессом, причём не просто закономерно и бесконечно уменьшающимися величинами, а именно «заведомо неверными» бесконечно уменьшающимися величинами, так как, повторим, мы никогда не получим «истинный» корень из числа 2, то мы просто искусственно «тормозим» весь процесс и постулируем нечто вроде «ну как-то так…». То есть здесь как раз-таки процесс является заведомо «обречённым» на провал, и он не становится с каждым шагом всё лучше и лучше, а просто «движется в никуда». То есть «если цели не существует, то в неё невозможно попасть».
То, что мы говорим о том, что «приближение становится всё более точным» на каждом шаге, это можно было бы считать корректным высказыванием в том случае, если бы мы имели дело с процессом, успешной завершённости которого мешает просто нехватка времени. И мы тогда, как бы «помогаем» такому процессу, говоря об актуальной бесконечности – и это ещё можно понять и принять. Но для априори безуспешных процессов мы так сказать уже не можем, так как в этих случаях время не является проблемой, и поэтому актуальная бесконечность не способна «закрыть вопрос». Для подобных процессов проблемой является как раз-таки принципиальная невозможность успешного завершения ни при каких условиях, поэтому «обматывание» их «плащаницей» актуальной бесконечности ничего не решает и не улучшает – «разрывы» всё равно останутся.
Ещё с одной стороны интересными представляются сами «разрывы» численного континуума. То есть из нашего доказательства дискретности численного представления континуума, если мы вместе с этим примем также и его взаимно-однозначное соответствие с пространственным представлением, следует наличие мельчайших разрывов в самом пространстве. Эти разрывы представляются «истинно бесконечными» и их невозможно «впихнуть» в прокрустово ложе актуальной бесконечности. То есть через эти «разрывы» в континууме как бы «утекает» содержимое, и процесс этот представляется действительно бесконечным. Подобное предположение (подчеркнём: всего лишь предположение) позволяет рассматривать континуум в контексте топологии в виде n-мерной сферы с «разрывами». И вот к этому – мы ещё вернёмся.

P.S.2. Мы хотим дополнить наши предыдущие заключения ещё одним немаловажным аспектом. А именно – мнением одного из идейных соперников Кантора и, в некотором смысле, антиципатора интуиционизма – Леопольда Кронекера. Не является секретом, что Кронекер, как, впрочем, и многие другие математики (Пуанкаре, к примеру), не признавал актуальной бесконечности и, как следствие, тех доказательств, которые были выведены при помощи данного понятия. Так, он не признавал способов определения некоторых иррациональных чисел при помощи актуально бесконечных рядов. В частности, в качестве отзыва на работу о числе π другого современного Кронекеру математика – Фердинанда Линдеманна, Кронекер сказал тому: «Что толку от вашей прекрасной работы о числе π? Стоит ли браться за исследование подобных проблем, если подобные иррациональные числа вообще не существуют?» Для пущей ясности уточним, что здесь мнение Кронекера о «несуществующих числах» относилось к трансцендентным иррациональным числам, однако также любые способы определения вещественных чисел при помощи бесконечных рядов он считал неприемлемыми всё по той же причине – непринятие актуальной бесконечности при определении. Далее мы покажем, что если совсем немного продолжить мысль Кронекера, то можно прийти к весьма интересным аспектам, которые выходят далеко за стандартные границы математики.

5.15. ОБОСНОВАНИЕ ФУНДАМЕНТАЛЬНОСТИ ФЕНОМЕНА ДЖИМИНАЦИИ: ПУСТОЕ МНОЖЕСТВО, НОЛЬ И «НИЧТО»

Описание. Дискурс данной части изложения будет как формально отличаться от предыдущих, так и характеризоваться довольно высоким уровнем абстрактности, так как представляется весьма затруднительным эксплицировать феномены подобного уровня и при этом оставаться стоять «на твёрдой почве» сугубо «чувственного» изложения.
Существование пустого множества постулировано Кантором в 1880-ом году. Оно в каком-то смысле эквивалентно нулю из арифметики натуральных (для тех подходов, где принят отсчёт с нуля) чисел и нулю же равна его мощность. В данном контексте пустое множество означает не просто отсутствие какой-либо абстрактной связи между объектами, а отсутствие самой возможности для её установления – и именно это оно утверждает своим наличием. Если вспомнить наше определение математических объектов, то, допустим, 1 – абстрактная связь между всеми объектами во Вселенной, взятыми по одному, то есть, между всем «чего один». Также с остальным натуральным рядом – это абстрактные связи между совокупностями объектов, установленные по сугубо количественному признаку. А вот множества в этом смысле – несколько более абстрактны, так как подразумевают возможность установления гораздо более широкого диапазона связей между объектами. Поэтому пустое множество – самой своей сутью – есть абстрактное отрицание любой возможной связи между чем бы то ни было.
При помощи нуля выводится бесконечный натуральный ряд, и именно на нуле он и «стоит» даже в том случае, если в какой-либо конкретной традиции натуральный ряд начинают с единицы. При помощи же пустого множества выводятся бесконечные множества, вплоть до самой вершины иерархии алефов. И всё это – из ничего. Так из ничего ли или же всё-таки «актуальное отсутствие» также означает «уже наличие» чего бы то ни было? На примере одной из наших предыдущих философских теорем мы можем вывести, что «иногда ничто – это уже что-то». Присутствие пустого множества держит на себе аксиому бесконечности и позволяет выстраивать множества любой мощности. То есть, с одной стороны, оно как раз-таки утверждает «отсутствие присутствия» и «не наличие хоть каких-либо связей» между объектами, а с другой, на нём базируется вся огромная конструкция теории множеств с огромным количеством внутренних связей и оно собой являет не просто «что-то», но целое «нечто». Высказывание ««ничто» всегда больше, чем «что-то»», в данном контексте приобретает глубокий смысл и ответ на вопрос о том, может ли из «ничего» появиться «нечто» находит свой закономерный ответ.
Однако, какова природа подобного феномена? Почему удаётся из «ничего» получить «нечто»? Причём целесообразно также особо отметить иногда «случающуюся» обратно пропорциональную зависимость между «стартом» и «финишем»: чем с «большего ничто» что-либо начинается, тем к большим высотам оно способно прийти. То есть если нуль – это просто отсутствие всякой связи между хоть какими-либо объектами в контексте Вселенной, то уже пустое множество – отсутствие даже самой возможности возникновения подобной связи. И, основываясь на этом, мощность множества всех натуральных чисел – алеф 0, а мощность множества всех его подмножеств 2^алеф 0, то есть гораздо больше, если тут вообще применимы «обычные» понятия. То есть «большее ничто» потенциально даёт «большее нечто». Мы здесь не хотим реально утверждать правомерность приведённого примера в том контексте, что «оно так работает – вот смотрите». Это просто пример, где это действительно так, но на данный момент мы ничего не утверждаем, кроме того, что это так происходит конкретно в этом примере.
Но всё же, почему же так происходит и как это работает? Нам представляется, что здесь функционирует тот же самый механизм, который мы ранее определили, как джиминацию. Мы уже ранее показывали, как это работает на некоторых примерах, но теперь хотим несколько углубить и расширить «область юрисдикции» феномена джиминации. Вполне возможно, что некоторые моменты могут показаться несколько дискуссионными и даже спорными, но мы полагаем, что наш «образ когнитивного действия» в любом случае имеет некоторое право на существование. И логическую основу для этого права.

Основание. Для простоты и краткости определим высказывание как D (от латинского dictum) – высказывание, сказать, сказал. Представим ряд D, индексированный целыми числами.
D –n. …
D -2. Если после этого D не будет хотя бы одного D, то пусть и здесь не будет D, предшествующего следующему D.
D -1. Если после этого D не будет никакого D, то пусть и здесь не будет первого D.
D 0. Здесь нет никакого D.
D 1. Если до этого D не было никакого D, то пусть здесь будет первое D.
D 2. Если до этого D было хотя бы одно D, то пусть здесь будет D, следующее за предыдущим D.
D n. …

Вывод и заключение. Мы предоставили очень отдалённо подобную формулировку, но главное – принцип сохраняется. То есть, на основе того, что «есть некое ничто», выводится как «что-то», так и «всё остальное».

P.S. Основные рассуждения. Наша «иерархия высказываний» несколько напоминает, в контексте функциональной составляющей, один из символов, которые мы приводили в части о теореме Гёделя, а именно – последователь числа (ssss0 = 4). И это неспроста, потому что, именно таким образом в действительности и выводится натуральный ряд на основе аксиоматики Пеано. В этом смысле те подходы, которые классифицируют натуральный ряд с единицы, выглядят несколько более искусственными, так как вынуждены сразу же постулировать наличие чего-то, не предоставляя никакого конструктивного обоснования этого наличия – по сути, призывая просто поверить в то, что нечто существует. В контексте конструктивистской парадигма, подобное и вовсе не правомерно, так как для утверждения существования чего-либо необходимо предоставить способ конструирования этого самого «чего-либо».
Поэтому целесообразно выглядит начало именно с нуля. И в контексте определения «ряда наличий» с началом в виде нуля подобных проблем не возникает – обоснование предоставлено, способ конструирования определён. Зато имеется другая проблема. Если посмотреть на структуру D, то можно заметить, что каждое последующее сконструировано на основе предыдущего и как бы выводится из его наличия и формы. Так, первое D принимает в качестве основания своего существования отсутствие каких-либо предшествующих ему существований. И вот здесь и возникает вопрос: правомерно ли считать, что до первого D не было ничего? То есть самореферентное D, утверждающее отсутствие самого себя, правомерно ли вообще?
Если бы мы не определили ранее и не показали на примерах реализацию феномена «латентного удвоения», то теперь нам было бы крайне затруднительно понять, что же именно происходит. А произошло следующее (и здесь уже действительно жёстко напрашивается в актуальный контекст феномен «эффекта наблюдателя» из области теоретической физики): в тот момент, когда ничто определяется в виде «ничто», то закладывается начало «латентного удвоения» и оно разъединяется одновременно на две ипостаси: «ничто» и «нечто». Точно также, как отсутствие доказательства становится доказательством и в то же время не перестаёт быть отсутствием в нашей «Теореме о доказательстве»; как истинное высказывание становится не выводимым или противоречивым у Гёделя и Тарского; точно так же как одно и то же множество становится и принадлежащим себе и не принадлежащим у Рассела; также как универсум у Кантора и должен и не должен принадлежать самому себе; так же как критянин становится и лжецом и говорящим правду; и как алгоритм должен быть одновременно остановлен и не остановлен у Тьюринга при доказательстве неразрешимости проблемы остановки; и ровно так же, как пустое множество в качестве элемента самореферентного множества Х одновременно гарантирует отсутствие пересечений и служит основой для опровержения самого смысла пустого множества в качестве гаранта отсутствия пересечений – везде один и тот же принцип. И этот принцип важен в том смысле, что за счёт его применения можно на «ничто» выстроить «всё», но также он и весьма «капризен», так как малейшее некорректное его применение тут же приводит к парадоксам и абсурду.
И вот уже в этом смысле, выявленные Гёделем и Тарским «несовершенства» всякой «более или менее функциональной» формальной системы выглядят несколько иначе. В данном контексте также целесообразно упомянуть, в качестве показательного примера и качественного объяснения, один старый анекдот: «В синагоге шел молебен. Перед одной из молитв половина молящихся встала, а вторая – осталась сидеть. Те, что сидели, начали кричать на стоящих, чтобы те тоже садились, а стоящие стали кричать на сидящих, чтобы те вставали. За разрешением противоречия толпа верующих обратилась к рэбе. Он, соответственно, не желая оказаться промеж двух огней, не стал самостоятельно разрешать возникшую ситуацию, а вместо этого предложил обратиться за помощью к старому Мойше, который был ещё строителем этой самой синагоги. Ну, вот пришёл старый Мойше и верующие насели на него с вопросами. Те, что стояли, спрашивают, мол «Ведь стоять во время молитвы – это наша традиция»? Нет, отвечал старый Мойше, – это не наша традиция. Тогда сидящие радостно уточняют, что «Получается, сидеть во время молитвы, такова наша традиция»? Но старый Мойше и им отвечал, что нет – не в этом наша традиция. Тогда уже не выдержал сам рэбе и воскликнул «Но ведь они же из-за этого постоянно ругаются»! – Вот! – сказал старый Мойше – Вот это и есть наша традиция!»
Как видно из наших примеров, на той же самой основе, на которой возникают противоречия и парадоксы, также конструктивно выстраивается и вся система в целом. Таким образом, в виде некоего резюмирования, целесообразно будет предположить, что именно сами противоречия и лежат в основе всякой систематизированности и структурированности вообще – первичная неупорядоченность, будучи вынуждена под «эффектом наблюдателя» и в ходе процессе джиминации приобретать свойства и качества видимого порядка, всё же не лишается полностью также и своей изначальной сущности – хтонического бушующего хаоса. В этом смысле очень жаль, что гений Брауэра, осознавший внутреннюю противоречивость закона исключённого третьего, всё же не пошёл дальше – к тому, что лежит «трансфинитно» за этим утверждением – к джиминации (или к чему-то подобному), как показательному случаю, в контексте которого это самый закон не применим. Для статичных объектов закон истинен и «исправен», для динамичных – ложен и «сломан». Просто обычно никто не привык рассчитывать на какую-то «внутреннюю латентную динамику» в контексте достаточно богатых формальных систем. Мы же, в свою очередь, неоднократно показали в нашей работе, что, по всей видимости, стоило бы.
Возможно будет целесообразным в контексте экспликации предлагаемого нами понятия джиминации также упомянуть об одном ассоциативно связанном филологическом феномене и прояснить его отличия от самой джиминации. А именно – об эвфемизмах [с]. Изначально может показаться, что речь и вовсе идёт об одном и том же и введение ещё и джиминации, в качестве отдельного феномена, является чрезмерным и не соответствует принципу «бритвы Оккама» [с]. Мы же полагаем, что в этом случае мы имеем дело с совершенно различными феноменами, которые, несмотря на степень своих различий, обладают некой «внешней» схожестью. Итак, эвфемизм – это замена одного понятия, по каким-либо причинам неуместного, иным – подходящим по контексту и валидным по значению в синонимическом качестве. Более формально, это некий Х, который служит для замены «здесь и сейчас» некоего Y. То есть это обязательно различные по форме, звучанию и написанию слова – формальные пересечения Х с Y могут иметь место, но это не существенный критерий. Таким образом, мы здесь имеем дело с, в общем-то феноменом даже противоположным джиминации – с формально, то есть по внешней видимости, различающимися (причём обязательно различающимися) Х и Y, которые, тем не менее, указывают на одно и то же значение. Наиболее просто пояснить разницу можно на примере с программированием: с одной стороны, джиминация, как рекурсия с неявной трансформацией самого рекурсивного объекта, а с другой же стороны, эвфемизм – наличие разных ссылок на один и тот же объект. Нам показанная разница представляется самоочевидной и явственно обосновывающей базовые различия между эвфемизмом и джиминацией.
Для более полного раскрытия джиминации рассмотрим ещё одну формулировку, за которой в данном случае не придётся далеко ходить, более того – не придётся даже покидать нишу философии: «Я знаю, что я ничего не знаю» – высказывание самого Сократа, и о нём мы уже, в общем-то, упоминали. Утверждение ли это о знании как незнании или же всё-таки о незнании в контексте знания? Вывод знания из незнания в виде отрицания знания – один из прекраснейших и древнейших примеров джиминации вообще. В сущности, подобное высказывание утверждает нечто большее, чем знание – постижение всей совокупности знания и отрицание этой совокупности, как «ничто». Знание уравнивается с незнанием, ибо во всём знании нет ничего, кроме истинного незнания. Самой формулировкой постулируется право отрицать знание, подразумевая преодоление всякого конструктивного знания и воздвигая «незнание на основе знания» – выше всякого чистого знания. И, собственно, так далее.
Само применение феномена джиминации создаёт своеобразный «бесконечный смысловой генератор», за счёт функционирования которого, в процессе бесконечной рекурсии, можно создавать всё новые и новые фрагменты контекста.
Наиболее мощные «смысловые генераторы» создаются тогда, когда рекурсивная сущность в ходе джиминации трансформирует сама себя до уровня своей бинарной оппозиции: ничто-нечто, незнание-знание, противоречивость-непротиворечивость, полнота-неполнота, истина-ложь, бытие-небытие и прочие варианты. Как писал наиболее глубокий из всех известных исследователей природы небытия на постсоветском пространстве – А.Н. Чанышев, о небытии «Бытие не имеет основания в самом себе. Основа Бытия в Небытии. Бытие вторично – Бытие есть небытие Небытия» [Алексеев П. В. Чанышев Арсений Николаевич // Философы России XIX-XX столетий. Биографии, идеи, труды. — 4-е изд., перераб. и доп. — М.: Академический проект, 2002. — 1152 с. — ISBN 5-8291-0148-3.]. Обратим внимание на два последних слова в приведённой цитате – это одно и то же слово, но написанное одно со строчной буквы, а второе – с заглавной. То есть Чанышев явно указывает на разницу между двумя различающимися ипостасями одного и того же – небытия, представленного с маленькой буквы – в виде сущностного отрицания, а с большой – в виде утверждения обособленного и объективированного наличия. Но даже несмотря на то, что Чанышев явно указывает на две разные ипостаси одного и того же, попытка представить себе «небытие Небытия» вряд ли окажется проще попытки представить пустое множество. Хоть мы не можем быть уверены, что сам Чанышев подразумевал здесь именно «явное» указание на «две стороны одной монеты», а не просто использовал подобную форму высказывания для передачи смысла, но даже в подобном случае «явной» джиминации формируется бесконечный смысловой генератор. В случаях же иных, таких как доказательство теоремы Гёделя о неполноте, Тарского о невыразимости истины в рамках формальной системы средствами это же системы, доказательство Тьюринга о неразрешимости проблемы остановки, парадокс Рассела и прочие – во всех этих случаях на реализацию феномена джиминации не просто не указывается, а – куда более того – её там даже не подразумевается в явном виде и само собой, что и доказательства будут казаться правомерными, ибо они суть формируют парадоксы и сами парадоксы станут выглядеть сложными и непонятными.
И раз уж затронута тематика небытия, то считаем целесообразным привести ещё один пример, который хоть и не использует джиминацию в типичном виде, но тем не менее, также очень даже подпадает под эту категорию. В контексте древнегреческой мифологии бытует миф о Силене, который был спутником Диониса и царе Мидасе. Он приводится в различных интерпретациях и переводах – мы предоставим нечто вроде «усреднённого варианта». Согласно сказанию, царь Мидас хотел поймать Силена, дабы поговорить с ним и прильнуть к его мудрости, которой Силен славился. Но поймать его всё никак не получалось. Тогда царь прибег к хитрости. Через лес, где находился Силен протекала река и царь приказал вылить в реку огромное количество вина. Силен, напившись из реки, уснул и его смогли поймать. Когда Силена привели к Мидасу, то царь спросил его: «Что есть наивысшее благо для человека?». И Силен ответил ему: «Зачем ты, смертное создание, спрашиваешь меня о том, чего тебе лучше не знать? Наивысшее благо для тебя – не родиться вовсе, а самое ближайшее к нему – как можно скорее умереть». Говорят, что после этого Силен с Мидасом несколько дней совместно пили вино и беседовали.
Вышеприведённый миф можно тезисно выразить следующим образом: «Мальчик был настолько удачливым, что умудрился вообще не родиться». И джиминация, которая может быть не слишком заметной, здесь заключается в некотором подобии на парадоксальные вопросы: «пахнет ли роза, когда её не нюхают?» и «каким образом благо может быть отсутствием возможности воспринимать благо?». Если, в определённых ситуациях, человеку может показаться, что лучше бы не просто не присутствовать «здесь и сейчас», но и вовсе быть «никогда не существовавшим», то это мысли существующего и они сущи потому, что сущность имеет бытие. Не будь бытия или сущности – не было бы и возможность утверждать, что наивысшее благо есть отсутствие бытия и сущности. Таким образом, «мысль, отрицающая мышление», «наличие, утверждающее стремление к отсутствию наличия», «сущность отрицающая бытие» и прочее – примеры джиминации. Та же рекурсия с изменением рекурсивного объекта. И нам хотелось бы рассчитывать на то, что с учётом большого количества предоставленных примеров и, в общем-то достаточно фундаментальной природы самого феномена джиминации, уже будет действительно более или менее понятно, о чём мы хотим сказать и на что именно указать.
Само собой разумеется, что все подобные примеры, как и многие приведённые ранее, представляют ситуацию в несколько мрачных оттенках. Казалось бы – что вообще конструктивного и позитивного в феномене джиминации? Неполнота и противоречивость формальных систем (достаточно богатых, конечно), огромное количество парадоксов и невозможность реализации алгоритма прогнозирования конечности процесса вычисления, а также, конечно, возможность жизни отрицать саму себя и возможность мысли уничтожать собственный источник, что, к слову, является не только сугубо эфемерной «проблемой формулировок», но также и весьма острой социальной проблемой – вряд ли это всё способно кого-либо сильно обрадовать (Гильберта вот в своё время совсем не обрадовало).
И это действительно несколько меланхолично. Но, только до тех пор, пока мы не обратим внимание на то, что, собственно, для того, чтобы мысль хотя бы в потенциале могла прийти к самоотрицанию – ей для начала нужно стать своим собственным объектом (здесь уже латентная джиминация начинает выходить из тени). Одну из ключевых проблем в философии сознания можно резюмировать так: каким именно образом вся совокупность разрозненных сознательных интенций субъекта складывается в единый субъективный опыт, который переживается целостно? То есть проблема сознания в некотором смысле, редуцируется к проблеме самосознания. Касаемо наличия непосредственно сознания у некоторых видов животных (дельфинов, например) ведутся дискуссии – можем ли мы считать, что у них также есть сознание? А вот насчёт самосознания всё более или менее ясно – этого у них точно нет, по крайней мере пока. А вот у человека как раз-таки есть оно – самосознание. По крайней мере, большинство исследователей данной сферы согласятся с подобным высказыванием.
И мы полагаем, что наше следующее высказывание является достаточно очевидным, учитывая предыдущий дискурс. Мы предполагаем, что наличие самосознания фундаментализировано на феномене джиминации. Здесь мы не будем предоставлять полное обоснование нашего предположения (иначе оно бы стало утверждением), так как это не вполне подходит к общей теме, но немного всё же тему раскроем, а здесь укажем, что в любом случае весьма целесообразно применять подобный подход в контексте описания развития и функционирования когнитивной сферы. В данном же контексте для нас важно то, что сама возможность осуществлять процесс мышления на «уровне достойном человека» – базируется на том же самом феномене, который лежит в основе самореферентных парадоксов, неполноты и противоречивости, превращения логики в абсурд и так далее.
В «Критике чистого разума» Кант говорит о том, что «На долю человеческого разума в одном из видов его познания выпала странная судьба: его осаждают вопросы, от которых он не может уклониться, так как они навязаны ему собственной природой; но в то же время он не может ответить на них, так как они превосходят возможности человеческого разума» [c]. Кант считает, что проблема заключается в несовершенстве человеческого разума в контексте невозможности полноценного и однозначного ответа на некоторые вопросы, которые можно корректно определить, как «смысловые ловушки» (что такое истина? в чём смысл жизни? и прочие). Мы же полагаем, что здесь дело как раз-таки в самой основе человеческого самосознания – в бесконечной рекурсии с перманентной трансформацией рекурсивного объекта – в том феномене, который мы и назвали джиминацией.
Не будь джиминации, то, конечно и подобных проблем бы не было также, но только за счёт того, что человек и вовсе «не умел бы думать по-человечески». В общем, целесообразно предположить, что уж «такова наша традиция» и, по всей видимости именно на неё, собственно, и базируется наша способность к осознаванию самих себя, возможности строить сложные и многофакторные умозаключения, концептуальному мышлению, квалиа и прочим неотъемлемым аспектам «бытия человеческой сущности».
В этом смысле зачастую утверждают, что одной лишь эмерджентности недостаточно для воспроизведения таких феноменов, как сознание, самосознание, вообще целостный живой организм и так далее – на этом же уровне. В виду имеется то, что вроде бы, диалектический переход количественных позитивных изменений в качественные фазовые переходы – это всё хорошо и понятно, но «как же одних количественных приростов может быть достаточно для воспроизведения настолько сложной и многофакторной структуры, как человек? Ведь необходимо что-то ещё». Под этим самым «что-то ещё», при подобной постановке вопроса практически во всех случаях, подразумевается некое «метафизическое наличие»: душа, Божественное вмешательство, инопланетный или вообще Космический разум, «Великий архитектор Вселенной» и тому подобное – примеров огромное количество. Однако все эти примеры можно свести к одному ключевому характеристическому свойству – внешнему вмешательству. В определённом смысле в эту же самую категорию мы можем отнести и крайне дискуссионный феномен «свободы воли», с той лишь оговоркой, что конкретно в этом случае вмешательство также подразумевается внешним, но исходящим «изнутри», то есть также и внутренним в каком-то смысле. Собственно, свобода воли сама по себе является в русском языке тафтологией, но это не означает её рекурсивность, а как раз таки говорит о совокупности двух эвфемизмов в данном словосочетании.
По указанному нами критерию «локализации влияния» мы как раз можем осуществить вполне логичную демаркацию: на внутренние, внешние и смешанные «влияния». Конечно, золотая середина представляется уже очевидной, но с наших актуальных исследовательских позиций – «не всё так однозначно». Напомним, что выше мы определили джиминацию в качестве некоей основы для функционирования феномена самосознания. Если мы редуцируем сознание к не-рекурсивной функции «осознавания», то есть некоему «апгрейду» узнавания – узнаванию не просто «вещи», но именно «вещи в контексте», то самосознание как раз-таки и окажется основанным на джиминации – то есть на уже непосредственно рекурсивной функции «осознавания».
Приведём не особо строгие, но достаточным образом демонстрирующие суть примеры. Допустим, мы говорим о «внешнем влиянии». Представим себе некую интерфейсную функцию F, которая получает на вход два числа – «х» и «y», и должна применить к числу «х» некую арифметическую операцию «y» раз, после чего вернуть полученный результат. Но сама эта функция «не умеет» применять арифметических операций. Поэтому мы вводим ещё одну функцию – G. Функция G не будет интерфейсной в плане отсутствия доступа к ней извне. Доступ к ней будет только у функции F. Таким образом, получив два числа, наша функция F передаёт функции G числа «x» и «y», а затем F просто возвращает итоговый результат функции G. Итак, у нас есть некая функция, которой нужно что-то сделать, и она обращается к некой иной функции, а потом предоставляет результат работы этой функции. То есть – объектов ожидаемо 2. Итог подобного действа будет, конечно же, закономерно успешен.
Следующий пример. Теперь мы говорим о «смешанном влиянии». У нас будет та же самая функция F, которая получает на вход те же самые числа «х» и «y», и должна сделать с ними то же самое, что и в прошлый раз. Но теперь эта функция «знает», как именно применять арифметические операции. Однако, так как применение этих операция должно быть выполнено должное количество раз, функция, соответственно, реализует внутренний цикл, применяя необходимые операции к числу столько, сколько нужно. А потом просто возвращает результат. То есть, у нас есть некая функция, а в её рамках – некий внутренний цикл. То есть опять же – объектов 2, но один из них как бы внутри второго. И это тоже будет успешно завершено.
Ну и третий пример – о сугубо «внутреннем влиянии». И это, конечно же, рекурсивная функция. Один объект. И, само собой разумеется, в данном случае подразумевается столь же успешное завершение выполнения задания, как и в двух предыдущих примерах.
Резюмируем фрагмент. Мы здесь привели три примера из области программирования, каждый из которых служит метафорой «влияния» на возникновение сложных форм упорядоченности, таких как самосознание, например. Причём здесь мы рассматриваем функции до известной степени изолированно от конкретных языков программирования и подходов к проектированию – без обсуждения деталей реализации циклов, функций или непосредственно рекурсии. Дело здесь в другом. А именно в том, что во всех приведённых нами примерах результат будет, так или иначе, успешно достигнут вне зависимости от количества объектов, участвующих в реализации. В этом контексте не имеет смысла вводить «дополнительные сущности» просто потому, что они не являются необходимыми, а значит – их введение не целесообразно и избыточно. Всё хорошо сработает и на одной только рекурсии, которая для случаев формирования сложных феноменов, имеющих дуальное представление, являет собой джиминацию – рекурсию с изменением объекта рекурсии на каждом рекурсивном шаге. Поэтому, при рассмотрении ситуации с наших позиций, никаких дополнительных «внешних наличий» не подразумевается – всё и так функционирует должным образом, а само «ощущение» того, что «нужно что-то ещё», как раз и являет собой закономерный итог невозможности остановки рекурсивного процесса, но потенциально бесконечное стремление к этому. «Что-то ещё» – это тот самый терминальный случай рекурсии, который не может быть достигнут до тех пор, пока сохраняются основания для реализации рекурсивного процесса и не более того. Ещё раз – «такова наша традиция».

ВЫВОДЫ ПО ГЛАВЕ

В данной, не совсем обычной, части нашего исследования мы разобрали некоторые аспекты теории множеств, обоснований вещественных чисел, понятия бесконечности и специфики его использования в контексте математики, а также предоставили своё видение самореферентных парадоксов и доказательств – в частности доказательства Гёделя его теоремы о неполноте.
Те выводы, которые нам хотелось бы сделать, мы уже сделали ранее – в каждой из частей данного раздела нашей работы. Касаемо же некоторых общих замечаний, хотим сказать, что при ближайшем рассмотрении вызывают вопросы о противоречивости многие постулаты, теоремы и даже аксиомы математики. И эти вопросы ещё острее звучат, если вспомнить относительно недавнее прошлое математики – например, начало-середину девятнадцатого века. Тот период, когда у вещественных чисел ещё не существовало обоснования и конструктивного определения, когда Гаусс только недавно ввёл в контекст математики комплексные числа, когда Коши только начал активно заниматься основаниями математического анализа и когда ещё самой теории множеств не существовало – тогда многие вещи, которые представляются нам сейчас сами собой разумеющимися, были инновациями и открытиями, к которым относились соответствующим образом – с подозрением и недоверием. Потом это недоверие было отогнано прикладными успехами математики, кризис оснований был преодолён и «всё пошло своим чередом». Но в том то и дело, что со времён Гаусса, Вейерштрасса, Коши, Дедекинда и Кантора далеко не многие задумывались об основаниях и тех «китах», на которых это основание покоится – о понятиях бесконечности, о сущности вещественных чисел, о правомерности трансфинитной индукции и так далее. Причём не задумывались не с «внутри-математической» дисциплинарной точки зрения, а в контексте общенаучной и философской правомерности тех или иных теорем, доказательств и заключений.
Мы в этой главе занялись как раз этим. И на основе наших «исследовательских поползновений» мы пришли ко многим нетривиальным выводам. К примеру, о явной неправомерности использования понятий актуальной и потенциальной бесконечности не только в контексте диагонального аргумента Кантора, но также и в теории множеств вообще. пришли к выводам об обширных внутренних противоречиях в этой теории – опять же, при взгляде не изнутри самой дисциплины, а как бы несколько со стороны и сверху. В частности, мы предложили заменить понятие бесконечности понятием «индифферентности», а конкретнее – «индифферентной мощности». И это потому, что, как мы показали, зачастую это понятие куда лучше отражает суть происходящего в рамках теории множеств. Мы показали, что правомерные доказательства для некоторых случаев, тут же формируют противоречия для случаев иных. Также было прямо указано на возможность формирования противоречий и парадоксов на основе системы аксиом теории множеств Цермело-Френкеля.
Наиболее же значимым из того, что на что мы указали в данном разделе, можно считать отсутствие конструктивных способов построения множества всех вещественных чисел и, как логическое следствие этого, отсутствие взаимно-однозначного соответствия между отрезком, или любой геометрической фигурой, и соответствующим множеством вещественных чисел. Конкретно к этому пункту мы ещё вернёмся далее.
Ещё мы считаем целесообразным выделить в качестве отдельного пункта наш парадокс о самореферентном множестве, состоящем из самого себя и пустого множества, как важное указание на внутреннюю противоречивость аксиоматики теории множеств.
Мы охарактеризовали доказательство Гёделя для его теоремы о неполноте и показали на многих примерах, что самореферентные доказательства и парадоксы не могут считаться правомерными, так как в их контексте используется рекурсия с нарушением одного из основных правил рекурсивного вывода – рекурсивные сущности на любом уровне вложенности должны обладать равными возможностями. Собственно, мы также показали, что нарушение этого рекурсивного правила лежит в основе формирования многих феноменов нашего мира. Этот процессуальный феномен мы определили, как джиминацию – от латинского слова gemellas, что означает «двойняшки».
Мы показали, каким путём могут формироваться логические парадоксы и «замыкания» и как сама логика может быть абсурдной, если снять с неё ограничения на рекурсию и «запустить» джиминацию на полную мощь.
В финальном фрагменте это части мы рассмотрели природу «ничто» и «небытия», сущность пустого множества и числа 0. Мы продемонстрировали, что, при рассмотрении ситуации в философском ключе, можно достаточно обоснованно предположить, что приведённый нами выше процесс джиминации лежит в основе формирования таких феноменов, как, к примеру, самосознание – а значит, можем предположить далее, основу нашей когнитивной деятельности составляют, в некотором смысле, «жизнеутверждающие противоречия».
По итогу этой части и, в контексте уплаты должного «гигантам» прошлого, мы считаем, что Гёдель доказал и Рассел «показал» гораздо больше, чем полагали они сами. А мы, на их уже «плечах стоя», смогли заметить некие «тени на стене пещеры», о которых и говорили в данной главе.
Ещё хотим добавить интересное соображение Мориса Клайна на тему данной части нашего исследования: «…прогрессу математики, несомненно, способствовали главным образом люди, наделенные не столько способностью проводить строгие доказательства, сколько необычайно сильной интуицией» [c]. Смеем лишь надеется, что наши интуитивные соображения, представленные здесь, также хоть в какой-то степени поспособствуют прогрессу в контексте философии математики, а может быть даже – и самой математики.
 
ГЛАВА 6. МОДЕЛЬ «РВАНОГО» КОНТИНУУМА

ПРЕДИСЛОВИЕ

То, о чём будет сказано в данной части нашей работы, представляет собой осмысление ранее обоснованного и охарактеризованного представления о наличии разрывов в континууме и некоторых иных вышеприведённых соображений, с переходом в дальнейшее построение гипотетической модели. Конечно же компьютерная модель была бы гораздо нагляднее, однако на данный момент далеко не все те аспекты, которые мы хотим предложить, с тем же успехом можно и визуально смоделировать – зачастую это не представляется возможным, что, собственно, станет понятно из нашего последующего изложения. Ключевыми, а также наиболее фундаментальными феноменами, которые мы собираемся использовать в контексте нашего изложения этой части, являются бесконечность и вычисления.
Мы полагаем, что наши соображения могут стать основой для зарождения и дальнейшей эволюции концептов и последующих моделей (выводимыми из наших или выводимых из опровержения наших), в особенности для некоторых дисциплинарных специалистов в рамках соответствующих научных направлений. Именно поэтому и предлагается модель – в качестве «опорной точки». А мы же намерены просто следовать открывшимися, как нам представляется, «тропами mundus» в том смысле, что собираемся сформировать логическое продолжение ключевых выводов предыдущей части нашей работы. Корректность или же напротив – неправомерность этого шага в любом случае останется дискуссионным вопросом, однако сама перспектива – действительно существенна и значима. Мы не рассуждаем здесь по типу «цель оправдывает средства», а на самом деле полагаем подобный подход правомерным, целесообразным и, более того, фундаментально перспективным.
Также дополним, что в науке иногда бывает так, что некие «верные» открытия совершаются исходя из неверных предпосылок (например, электродинамическая теория Максвелла выводилась из существования эфира, наличие которого позже было «отменено»), также порой бывает и наоборот – из верных предпосылок делаются неверные выводы (как наиболее яркий пример – имевшая место быть в истории советской науки «лысенковщина»). В нашем же случае дискуссионными могут быть как предпосылки, так и их следствия, но вот что мы дискуссионным не считаем, так это актуальность и целесообразность глубокого и разностороннего осмысления предлагаемой нами модели и использование её в качестве возможной основы для дальнейшего формирования решений фундаментальных научных проблем и ответов на сложные вопросы.
Конечно, чтобы те интуитивные предположения и эвристические гипотезы, которые мы приводим в данном разделе применительно к формируемой нами модели, имели под собой реальную основу, они должны быть снабжены математическим и физическим фундаментом, а также экспериментальной базой, которая может быть соответствующим образом верифицирована. Мы же хотим построить модель на основе уже сделанных теоретических заключений, «заставить» её функционировать в соответствии с ними, а далее – попробовать «обосновать» некоторые известные феномены таким образом, как если бы они воспроизводились в контексте нашей модели. И в конце посмотреть, к чему это может привести в целом.
Для простоты изложения тот концепт, который мы в данной части собираемся представлять и формировать, мы понятийно обозначим как модель «рваного» континуума.


6.1. ОБОСНОВАНИЕ ЦЕЛЕСООБРАЗНОСТИ И ОБЩИЙ НАБРОСОК МОДЕЛИ «РВАНОГО» КОНТИНУУМА

Рассмотрим же поподробнее ранее нами выведенное в контекст актуальной работы соображение о наличии разрывов в континууме. Как мы уже продемонстрировали, в процессе конструктивного численного формирования континуума, можно до некоторой степени заполнить его рациональными числами, а также попытаться «закрыть» иррациональными числами разрывы. Далее мы показали, что, по сути, эти самые разрывы окончательно «закрыть» не представляется возможным, так как сколько бы не продолжался бесконечный процесс осуществления попытки «попасть в точку» – успешно завершён он быть не может, так как сама «точка» отсутствует. Подобный процесс именно «должен» оставаться бесконечным и развёртывающимся (то есть потенциально бесконечным), по крайней мере это представляется логичным и само собой разумеющимся. Таким образом, мы вывели, что в любом случае в численном представлении континуума остаются некие разрывы, которые можно идентифицировать иррациональными числами в качестве неких направлений «развёртывания» вычислительных процессов. Непосредственно «закрыть» эти самые разрывы, как мы показали ранее – не представляется возможным.
В попытке устранить эту проблему обычно осуществляют приближение к некоторому искомому, но реально не существующему, значению, а затем, по достижении заранее заданного предела, просто останавливают процесс приближения и на этом всё заканчивается – полученное приближённое значение полагают достаточным для «закрытия» разрыва. То есть, ещё раз – число служит в качестве «заплатки».
Если же, как мы указывали ранее, данный процесс искусственно не останавливать, то он «превзойдёт» актуальную бесконечность в том смысле, что мы не имеем возможности указать где именно он должен остановиться в принципе, даже при условии актуально бесконечного «развёртывания». И причина в том, что данный процесс является безуспешным в любом случае, поэтому он традиционно и останавливается искусственно – так как нет никакой целесообразности продолжать. А, как мы уже говорили ранее, применение актуальной бесконечности к процессу может быть обоснованным только в том случае, если процесс полагается «успешным» и единственное, что ему мешает «сделать свою работу» – нехватка времени. Если же процесс априори безуспешен, то никакая актуальная бесконечность тут уже не поможет – нет смысла. Зато мы увидели смысл как раз в ином – в бесконечности потенциальной и именно развёртывающейся в направлении некоего диапазона целевых точек, диапазона между двумя рациональными числами.
Ранее мы вывели заключение о том, что численная аппроксимация континуума не является с ним взаимно-однозначной, а является иньективной ему. Если же пойти от противного и предположить, что это на самом деле не так, и что численная аппроксимация континуума в действительности ему биективна, но при этом также и не способна точно отобразить некоторые его локации, а напротив – именно в них она бесконечно «развёртывается», то ситуация станет совершенно иной. Мы можем попытаться построить аналогию и предположить, что ровно те же самые «локации», которые мы не можем конструктивно определить в численном виде, вместо этого бесконечно развёртывая в их направлении некий вычислительный процесс – также могут быть конструктивно не определёнными и в геометрическом смысле.
Здесь можно вспомнить математиков времён эпохи Возрождения и чуть более позднего периода, которые были свято уверены в том, что, занимаясь математикой и описывая законы Мироздания математически – они тем самым постигают истинный замысел Творца. Ну вот вслед за ними мы, соответственно, и попытаемся предположить, что сам Творец пытается нам – «жалким смертным» – что-то подсказать таким вот оригинальным способом – «подсовыванием» нам, вместе с числами и геометрическими фигурами, невозможность однозначно сопоставить одно к другому. Казалось бы, одна из самых простых фигур – квадрат, настолько простая, что даже наличествует высказывание «простой, как квадрат», характеризующее человека, не отличающегося креативом. Однако, для этой простой фигуры не существует рационального числа (собственно, вообще не существует никакого точного числа), которое мы можем поставить в соответствие соотношению стороны квадрата и его диагонали, что было уяснено ещё древними греками. И со времён же древних греков численные операции и вычисления долгое время полагались своеобразным «придатком» к геометрическим действам. То есть, по сути считалось, и вполне возможно в неявном виде, что если не существует числа, которое может точно отразить «нечто геометрическое», то это «проблема самих чисел», то есть – внутренние алгебраические пертурбации. Об этом также сказано у Мориса Клайна в книге «Математика. Утрата определённости» таким образом: «Здание античной математики – структуры, состоящей в основном из евклидовой геометрии, – оказалось вполне устойчивым. Правда, в нем обнаружился один досадный дефект. Дело в том, что длины некоторых отрезков выражаются иррациональными числами: например, длина гипотенузы равнобедренного прямоугольного треугольника с единичными катетами равна иррациональному числу √2. Ho греки признавали только обычные целые числа и их отношения; поэтому они не могли допустить существование таких величин, как √2. Греки решили возникшую проблему, попросту изгнав иррациональные числа: они отказались считать √2 «числом», а следовательно, отказались и от идеи сопоставлять любым длинам, площадям и объемам численные значения. Тем самым греки не внесли никаких дополнений в арифметику и алгебру целых чисел, которые можно было бы включить и в структуру геометрии» [c]. Вот примерно это мы и собираемся здесь сделать – включить в геометрию (условно, конечно, и только в контексте формирования нашей модели) наше представление об иррациональных числах.
Причём мы здесь, как уже было указано выше, не утверждаем, что наш подход «истинный». Как, кстати, и обратного мы тоже не утверждаем, то есть того, что «фигуры правильные», а «числа плохие» – не утверждаем. Мы всего лишь считаем, что то, что предлагаемое может быть продуктивно в контексте демонстрации принципа соответствия некоторой «вещи» описанию этой самой «вещи», то есть предполагаем, что «вещь» такова, каким получается её численное (именно численное) описание – эквивалентность означаемого означающему. По сути, предполагаетсям, что некоторое несоответствие между числами и геометрическими фигурами уже само по себе – феноменологически – может о чём-то говорить. И мы собираемся на основе этого фактора сформировать некий «вывод» с тем, чтобы использовать его в контексте предлагаемой нами модели. Нам представляется, что подобный подход может оказаться продуктивным с точки зрения формирования совершенно новой «позиции наблюдателя» по отношению ко многим «привычным» феноменам.
То есть, мы предлагаем в буквальном виде принять, что континуум и его численное описание являются взаимно-однозначными, в том смысле, что каждой точке континуума соответствует некоторое число. И, далее там, где эти числа являются иррациональными – мы будем примерно «то же самое» предполагать и о континууме (с поправкой на специфику геометрии, конечно).
С озвученных выше позиций некоторые «обычные вещи» могут представляться достаточно парадоксальными. К примеру, мы знаем, что соотношение длины окружности к её диаметру равно числу pi, которое является иррациональным числом. Также знаем, что длина окружности круга с диаметром равным единице – равна числу pi. Так выходит, если следовать нашим предыдущим умозаключениям, что единичного круга не может существовать? Этот вопрос является интересным в том плане, что в контексте объективной Вселенной не существует такого круга, длина окружности которого не будет равна некоему натуральному числу, при выборе соответствующей точки отсчёта и должной меры. Поэтому в некотором смысле ответ на предыдущий вопрос является положительным.
Далее, тогда получается, что можно представить континуум в виде некоей упорядоченности точек, которым соответствуют некоторые рациональные числа и которые, при условии предоставления условному алгоритму достаточного количества времени, заполнят собой «практически» весь континуум. Но, конечно же, не весь. Множество рациональных чисел является счётным, хоть и, само собой, бесконечным. Так что при таком подходе в континууме остаются некоторые разрывы, которые, повторим в очередной раз, «залеплены» иррациональными числами, множество которых, соответственно, несчётно.
Наша мысль состоит в том, чтобы не останавливать приближение иррациональных чисел, а попробовать взглянуть на саму необходимость этой остановки с несколько иной стороны. Как мы сказали, континуум, при нашем подходе, можно представить в виде множества, состоящего из двух множеств: множества упорядоченных значений – рациональных чисел; и множества разрывов, уходящих «в никуда». Более формально, континуум можно представить, как «Х (Q^n, H)», где Х – континуум, Q – множество всех рациональных чисел, возведённое в степень n, H – множество разрывов, представленных потенциально бесконечными последовательностями иррациональных чисел, n – актуальная размерность рассматриваемого континуума. В том случае, если не останавливать приближение иррациональных чисел, а просто «принять» это, как неизбежность и потенциальную бесконечность, то есть действительно интерпретировать их как потенциально бесконечные вычислительные процессы, то можно вывести следующую картину.
Эти самые разрывы можно представить в виде «воронок», куда как бы «утекают» вычисляемые данные. Далее, (ключевая мысль) мы можем смоделировать трёхмерную сферу и «проколоть» её в тех точках, которые являются взаимно-однозначными с соответствующими им иррациональными числами. Эти «проколы» будет представлять собой «воронки», куда, соответственно, утекает вычисляемая информация. Поясним почему именно воронки, а не просто разрывы. При статичном состоянии сферы в 4-х измерениях разрывы остаются статическими разрывами, так как их «форма» не деформируется в какую-либо из возможных сторон. При динамическом состоянии сферы, например, при её сжатии или растяжении, разрывы преобразуются в воронки с изогнутыми краями. Поэтому мы и представляем разрывы в виде воронок. Ещё важный момент заключается в том, что понятие воронки несколько шире понятия разрыва, в этом смысле разрыв – подмножество воронки, так как всякий разрыв может быть описан как воронка с гладкими краями, а структура воронки не всегда может быть корректно описана через понятие разрыва. Под информацией же мы имеем ввиду всё то, что может быть описано математически: материя, энергия, элементарные частицы и так далее, то есть – информация в общем смысле слова, как мера устранённой неопределённости.
Если бы мы прокололи таким же образом сферу двумерную, то есть поверхность шара, то ситуация была бы более или менее понятной и было бы ясно, куда именно и в какое измерение информация утекает. Но со сферой трёхмерной сразу ставится вопрос: куда ведут эти воронки? Трёхмерная сфера обычно рассматривается в контексте 4-х измерений, где 4-е измерение при математическом описании определяется дополнительной 4-й координатой, а в физической же интерпретации под 4-м измерением обычно подразумевается составляющая времени: то есть лево-право, верх-низ, вперёд-назад и время. Таким образом, ещё раз: «куда» ведут воронки? Сразу же предположим, что они ведут в пятое измерение, которое мы идентифицируем как N5. Саму сферу же мы определим, как caseum, что в переводе с латинского означает сыр (так как он со всеми своими внутренними отверстиями, полостями и щелями отдалённо напоминает то, что мы имеем ввиду под нашей сферой). Само собой, эти воронки предполагаются весьма малых размеров и их «вотчина» – квантовый и «до-квантовый» уровень (то есть тот самый уровень, который подпадает под юрисдикцию теории струн) [c].
Предоставим немного посильных пояснений. На теоретическом субстрате смоделированной и в общем смысле охарактеризованной нами сферы можно попробовать более детально и обоснованно представить такие эффекты, как квантовая запутанность, квантовая гравитация, эффект наблюдателя, вообще вероятностный характер квантовых эффектов и так далее.
Также, что крайне важно, само наличие воронок, причём именно мельчайших воронок как предполагается, может служить в контексте нашей модели для самого обоснования гравитации как на уровне общей теории относительности, так и в случае с квантово-механическими эффектами. Представление гравитации в виде мельчайших эффектов «утекания», которые по мере увеличения размерностей объектов соответствующим образом «суммируются» и «аккумулируются» может служить концептуальным базисом для единого описания гравитации как в квантовой механике, так и общей теории относительности. И само собой разумеется, что наша концепция никоим образом не противоречит ни одной теории, ни другой, а скорее совсем немного дополняет обе. Дополняет немного – это да, но почему-то представляется, что именно это вот как раз каким-то загадочным образом «всё меняет».
Далее, те воронки, которые мы подразумеваем, не просто представляются очень малыми с точки зрения размерности, но скорее являют собой некие совокупности этих воронок, что логическим путём вытекает из самой природы иррациональных чисел, которыми аппроксимируются воронки. То есть представляется, что воронки расположены на диапазонах между рациональными числами, если выражаться математически. Более того, они действительно «кажутся» имеющими квантовую природу, то есть мы не можем точно сказать, что в такой-то конкретно точке наличествует воронка, иначе идентификатор воронки и не был бы иррациональным числом, а был бы определённым – рациональным числом. Но, в контексте модели рваного континуума мы можем утверждать только то, что на некотором диапазоне расположено то или иное количество воронок – их совокупность, их некоторое множество. То есть ситуация с воронками и соответствующими им иррациональными числами ровно такая же, как в случае с корпускулярно-волновым дуализмом: у нас есть некая «волна», где-то на «в которой» с той или иной вероятностью может находится частица; и у нас есть диапазон между рациональными числами, где-то «на котором» с той или иной вероятностью могло бы находится иррациональное число, в том случае, если бы оно вообще существовало. К слову сказать – это очень точное определение в том смысле, что таким образом и осуществляется аппроксимация иррациональных чисел.
Также полагаем целесообразным уточнить, хоть это и тривиальное уточнение, что эти самые воронки не предполагаются «лежащими на полу» континуума нашей сферы, а скорее пронизывающими его «весь полностью» во всех тех локациях, которые аппроксимируются иррациональными числами. Но, ещё раз – это должно быть и так понятно.
К нашему предыдущему дополнению прилагается текущее – о том измерении, которое мы обозначили как N5. Логично предположить, что оно и должно будет представлять собой тот самый «хаос», о котором столько сказано и нами, и не нами. Метафорически выражаясь – неоформленное, не сотворённое, потенциальное, вероятностное – хаос, в общем недифференцированном смысле – хтоническая мощь. Но не совсем недифференцированный. То есть не просто хаос как «непонятно что», а именно «вычислительный» хаос, некий «вычислительный джаггернаут» – процесс, который при «выходе» всегда предоставляет определённый «перевычисленный» результат в ответ на полученные данные, которые перерабатываются с учётом внутренней вычислительной логики соответствующего «измерения» – N5 в данном случае. То есть можно определить N5 как некий «тотальный процессор» для всей сферы.
Через само наличие N5 должно быть очень удобно описывать квантовые эффекты, так как там представляется весьма даже вероятным взаимодействие тех процессов и явлений, которые не взаимодействуют явным образом в остальных 4-х измерениях. Собственно, даже само «физическое» существование N5 в контексте нашей модели в качестве отдельного измерения не является обязательным, так как можно просто предположить, что вход в воронку из «обычного» измерения с одной стороны уже сам по себе является выходом обратно в «обычные» измерения в иной локации – с другой, а под «вычислительным» хаосом тут разумеется именно само состояние перехода.
Собственно, концепция воронок и N5 как бы обобщает собой такие топологические концепты и сформированные на их теоретической почве физические «метафоры» как червоточины, кротовины, чёрные дыры и прочие. Так как любой из подобных концептов может быть конструктивно описан через «интерпретационный базис» нашей модели – воронок и N5 в её рамках. К примеру, общие и/или локальные деформации сферы тем или иным образом, вполне закономерно могут приводить к формированию соответствующих «скоплений» воронок и соответствующему возрастанию эффекта утекания информации – чёрная дыра собственной персоной. Несколько забегая вперёд, да ещё и немного утрируя, можно определить физические феномены, подобные чёрным дырам, через чрезмерные «скопления» иррациональных чисел, которые образуются путём специфических локальных деформаций сферы.
В этом смысле мы можем вывести и смоделировать некоторое обобщение проблемы с утерей/сохранением информации в чёрных дырах. Информация «утрачивается» в том виде, в котором она пребывала до утекания, но в целом она преобразуется в соответствии с общим и/или локальным состоянием N5 на момент её туда попадания, и вытекает частично в иной, а частично – в той же самой локации, но уже всегда в преобразованном, как бы «перевычисленном» виде. Также является закономерным, что все локации сферы (чёрные дыры, ядра звёзд и так далее) должны обладать своими специфическими способностями по преобразованию информации. Ещё является интересным с этих позиций подход к утере информации в чёрных дырах с той точки зрения, что чёрная дыра поглощает всё, в неё поступившее и просто становится тяжелее, то есть перерабатывает всё в свою собственную массу. В этом контексте подобный феномен вполне может быть рассмотрен в качестве формирования дополнительных воронок или объединения с некоторыми интервалами близлежащих воронок для уже сформированного их объединения – чёрной дыры. Потреблённая же таким путём информация далее может быть распределена в соответствии с принципами распределения, действующими в N5. То есть чёрная дыра становится «тяжелее» (более «затягивающей») именно от самого процесса «поглощения» за счёт количественного увеличения наличия воронок, которые становятся её частью. А информация, собственно, вообще не поглощается и, разумеется, не теряется, а просто как бы «протекает» через совокупность воронок, представленных чёрной дырой. Если принять концепцию воронок, то получается, что вся Вселенная в рамках нашей модели пронизана мельчайшими чёрными дырами.
Собственно, сами процессы и «утекания», и «вытекания» должны развёртываться в соответствии с этими самыми принципами распределения, которые, как нам представляется, должны будут функционировать по вероятностному типу. Мы полагаем, что, какими бы ни были «условия» в N5, некая имманентная «логика» там будет в любом случае. Поэтому считаем себя вправе предоставить, просто для гипотетического примера, некую «формулу» для приблизительных расчётов специфики утекания/вытекания. Собственно, при принятии в контекст расчётов N5, всё остальное вполне можно свести к более или менее классическим ситуациям. То есть к примеру, для той или иной локации N5 вероятность события Х будет определяться предположительным количеством актуальных воронок в локации, внутренним/внешним «давлением», которое может быть описано в терминах локального искривления caseum, свойствами актуального внутреннего/внешнего содержимого – информации и так далее. Вполне возможно, что описание квантовых вероятностей на основе нашей модели поможет сделать расчёты более точными и понятными.
Нам представляется, что многие актуальные проблемы теоретической физики могут быть достаточно успешно охарактеризованы с предлагаемых позиций. К примеру, ввиду широты самого подхода, мы можем считать более или менее решённым извечный (на самом деле стоящий ещё со времён формирования научной мысли в Древней Греции) вопрос о прерывной или непрерывной природе «ткани Мироздания» – в контексте модели рваного континуума эти свойства просто «не мешают» друг другу и вполне успешно сочетаются. То есть сама ткань признаётся непрерывной ровно настолько, насколько плотно можно заполнить множество рациональными числами, а разрывы, преобразуемые в воронки за счёт «утекания» в них вычисляемых значений, представляются логичными и закономерными.
Конечно, остаются некоторые «небольшие» проблемы, например – объяснение того, «как и почему вообще всё это должно работать?» (онтологическая составляющая), с «чего всё это должно будет начаться?» (генеалогическая составляющая) и тому подобное. В последующих разделах данной части нашей работы мы дадим посильные пояснения.
Также укажем, что выведенное далее, касаемо особенностей функционирования нашей модели, будет основано на логических связях и эвристических предположениях. При непосредственном «воплощении», компьютерной имплементации и физической верификации модели рваного континуума некоторые детали могут быть скорректированы.

6.2. ГЕНЕАЛОГИЯ И ОНТОЛОГИЯ СФЕРЫ В МОДЕЛИ «РВАНОГО» КОНТИНУУМА

Собственно говоря, при нашем подходе вся актуализируемая динамика редуцируется к деформациям самого caseum – всё движение в глобальном масштабе можно описать и смоделировать с точки зрения локальной/глобальной деформации сферы. А вот почему именно должны происходить эти самые деформации – вопрос отдельный. В любом случае, если они происходят, то специфическая форма caseum «сама» обеспечит всё остальное. Если же смотреть на проблему несколько глубже и искать ответ на вопрос о некоем «имманентном двигателе», то мы тут же упрёмся в необходимость описания специфики внутренней организации N5, да и, собственно, не только. Нам она представляется некой бинарной оппозицией по отношению к условно «внешней» поверхности сферы, которую мы определим, как cera (от лат. Cera – воск). Идея «топлива» для динамики caseum, по нашему мнению, может быть заключённой в некоторой полярности cera и N5 – единство и борьба противоположностей как в философском, так и в физическом смысле.
Сразу же дополним важным замечанием о том, что, согласно нашим текущим представлениям о Вселенной, она примерно на 95% состоит из тёмной материи вкупе с тёмной энергией. Что же именно они собой являют – вопрос на данный момент открытый (иначе вряд ли их бы изначально определили в качестве «тёмных»). Здесь мы могли бы, подобно Лапласу пропустить доказательство и обоснование, и постулировать нечто вроде «из чего следует, что наша гипотеза верна» и, в принципе, обоснование если и не совершенно очевидно, то уж точно и не сокрыто во мраке неизвестности. Тёмная материя в классической интерпретации предполагается в качестве некоей точно не определённой «силы», которая стремится к сжатию Вселенной, а тёмная энергия, в свою очередь, в качестве противоборствующей ей «силы», стремящейся к растяжению Вселенной. То есть, для нас является очевидным, что оба эти процесса отлично описываются через концептуальный аппарат нашей модели рваного континуума.
Более того, можно также вывести ещё и обратную связь – тоже, конечно же, только в виде предположения. Ранее упоминалось о том, что множество всех рациональных чисел является счётным, а множество иррациональных – нет. Также говорилось о том, что тёмная материя и тёмная энергия составляют примерно 95% всей Вселенной, а соответственно, остальные 5% – всё, что мы обычно и имеем ввиду под Вселенной. Таким образом, можем предположить (только предположить), что мощность множества иррациональных чисел составляет примерно 95% от множества всех вещественных чисел, а доля рациональных чисел примерно равна 5% от этого множества. Можно даже пойти в наших предположениях ещё дальше и сказать, что от этих же 95% иррациональных чисел около 27% должно приходится на алгебраические, а соответственно, около 68% – на трансцендентные. Мы ранее не проводили эту демаркацию иррациональных чисел, чтобы ещё сильнее не запутывать наше изложение, но вот именно в данном контексте – оно кажется ясным.
Также дополним ещё одной причиной: разделение, в сущности, не представлялось нам необходимым, ведь и алгебраические и трансцендентные иррациональные числа в любом случае – аппрокисмации. К примеру, квадратный корень из числа 2 считается алгебраическим иррациональным числом, так как удовлетворяет решению полинома «х ^ 2 – 2 = 0», для случая где х – квадратный корень из числа 2. Но, как мы уже показали ранее и что, собственно, является общеизвестным фактом – квадратного корня из числа 2 в действительности не существует. Соотношение же алгебраических и трансцендентных чисел мы предположили, исходя из того, что из совокупности «всего» тёмной материи и тёмной энергии, равной 95% от «всего» во Вселенной, на долю первой приходится около 27%, а на долю второй – около 68%. Отсюда мы и предположили соотношение, причём – конечно же, повторим ещё раз, довольно притянутое и даже несколько спекулятивное, но тем не менее являющееся вполне обоснованным в контексте именно нашего подхода и текущего изложения. Также, в рамках математики в действительности полагается, что алгебраических иррациональных чисел меньше, чем трансцендентных, так что в общем виде соответствие наличествует.
Теперь же детальнее про тёмную материю и тёмную энергию, а также про ответ на вопрос «с чего всё это началось?» Контекст ответа на вопрос «как и почему всё это работает?» мы уже несколько очертили. Заранее дополнительно укажем на эвристическую природу наших текущих постулатов в контексте построения теоретической модели.
Для начала, в этой связи вспомним то, что мы говорили ранее о «ничто» и о том, как из этого самого «ничто» способно формироваться «нечто», а также о феномене джиминации. Напомним, что джиминация представляет собой рекурсивный процесс с изменением самого объекта рекурсии на каждом рекурсивном шаге. Тафтологично звучит и выглядит, но суть именно такова. Нам представляется момент начала формирования нашей модели неким отторжением (отрицанием) этого самого «ничто» самим собой с одновременным превращением его в «нечто», но как бы не окончательно – там, где было «ничто» ничто и осталось самим собою, но вот где оно претерпело метаморфозу до определённого нечто – там оно стало, соответственно, вовсе иным – уже появилось «нечто». И таким образом на основе данного феномена мы получаем два уже совершенно «новых» объекта, как бы сцепленных друг с другом в бесконечной реализации феномена джиминации – «ничто» и «нечто». Некоего иного смысла и процесса сотворения, рождения и появления – из всего контекста философии выведено быть, скорее всего, не может. И что здесь значимо – сам этот момент, который мы охарактеризовали как начало реализации феномена джиминации в виде установления некоторой рекурсивной связи между различными «версиями» одного и того же феномена. Зачастую сам факт того, что мы имеем дело именно с различными как минимум двумя объектами – является скрытым, что было нами показано ранее на многочисленных примерах. Поэтому важно чётко понимать, что в действительности при подобных ситуациях мы имеем дело всё-таки более, чем с одним единичным объектом.
Итак, в момент «самоотрицания» «ничто», это самое «ничто» становится также ещё и чем-то определённым – в качестве некоего истинного наличия. Это и есть начало бесконечного процессуального феномена джиминации, которая есть вечный переход от рекурсивного объекта к своей несколько отличающейся версии, причём это самое «отличие» и формируется самим фактом перехода, как было нами показано ранее в том числе и на примере доказательства теоремы Гёделя о неполноте. В этом смысле мы получаем очень даже корректную метафору «вечного двигателя». В данном контексте физическое отображение этого процесса представляется нам следующим.
Допустим, в контексте нашей модели мы можем понятийно определить изначальное «ничто», как lac (от лат. lac – молоко). Самоотрицание и «запуск» механизма джиминации полагают начало всей модели в этом случае: формирование caseum на основе lac. При экстраполяции этого соображения из контекста формирования нашей модели в область актуальных теоретических представлений космологии, это и представляет собой так называемый Большой Взрыв. При физическом воплощении самоотрицание формирует бинарные оппозиции также в физическом смысле. Наиболее вероятной оппозицией нам здесь представляется подобная связь между N5 и cera – между самой «внутренней» и самой «внешней» частями. Конечно, и внутреннее и внешнее здесь подразумеваются в условном смысле, так как по сути здесь нет ни «потолка», ни «пола». Но, в контексте хоть какой-то визуализации и для нужд построения нашей модели построения нашей модели, именно самоотторжение lac приводит к формированию базальной связки «ничто» с «нечто» и формированию N5 и сera – как новых наличий со своими уникальными «обязанностями». Пространство же «между» ними (и включая cera, кстати) – представляет собой видимое «поле боя» противоположных сил. Истинное же «поле боя» – как раз-таки N5, которое представляет собой как бы некоторое «подпространство», в котором, как мы вновь интуитивно предполагаем, бесконечно осуществляется нечто вроде «до-квантового» вычислительного процесса в глобальном масштабе. В этом смысле N5 представляет собой, метафорически выражаясь, как бы тотальный процессор для всей сферы, о чём мы уже говорили выше.
Здесь предположим, что cera «стремится» сжаться до «изначальных» масштабов, то есть до уровня фундаментального и изначального «не существования», а N5 как раз не позволяет этого сделать. Данное предположение обосновывается тем, что при попытках вычисления иррациональных чисел сам процесс вычисления как бы «проваливается» в потенциальную бесконечность, «вглубь» – вплоть до бесконечно глубокого уровня, если мы экстраполируем данный феномен на физическую реальность и далее – «трансфинитно» – «за» квантовый уровень. Нам это представляется происходящим примерно следующим образом.
На квантовом и подразумеваемых более глубоких уровнях целесообразно предположить некоторое «рассеивание» вычисленной информации между воронками в актуальном их диапазоне, с последующим «утеканием» вычисленной информации в N5 и «вытеканием» её уже «в другом месте» и «в другом виде», но само собой разумеется, на том же самом уровне. То есть, если бы не воронки, то вместо них были бы пустоты, которые можно было бы заполнить просто «стягивая» «ткань». Но так как наличествуют воронки – стянуть ничего не выходит, а даже наоборот – чем сильнее эффект «утекания», тем сильнее и «вытекание», дополнительно усиленное внутренними особенностями N5. Чем сильнее действие, тем сильнее должно быть противодействие: тёмная материя стремится к сжатию и «заталкиванию внутрь» информации, а тёмная энергия – наоборот. Собственно, тёмная материя представляется нам силой стремящихся к потенциальной бесконечности вычисления иррациональных чисел, что в физическом смысле означает попытку заполнения воронок и выстраивания на их основе реальных (рациональных) локаций, а тёмная энергия, в свою очередь, предстаёт силой реакционного характера – ответной реакцией на вычисления, которая, как можно предположить (важный момент), и создаёт вообще вероятностную природу квантовых эффектов.
Математически (условно математически, конечно) это можно обосновать следующим образом. Если мы вспомним метод Ньютона приближения квадратного корня и применим его к числу, у которого в действительности этого самого квадратного корня не существует, то заметим, что если мы станем потенциально бесконечно приближать вычисление, то получим, соответственно, потенциально бесконечную последовательность иррациональных чисел. Если сравнить эти числа с выстрелами в цель, то мы получим бесконечные попадания слева и справа, а также несколько выше и сколь-нибудь ниже от мишени, которая к тому же сама всё время двигается, двоится и так далее, то есть фактически находится в суперпозиции – «мишень Шрёдингера» – и это так по причине её конструктивного отсутствия в некой строго определённой «локации».
И, в соответствии с нашей аналогией, если представить этот процесс топологически на примере физической реальности, то получится, что попытка «прорисовки» некоторой части «реальности» создаёт на глубинном уровне огромное количество «белых пятен», то есть как бы «недорисованного мира» и эти самые белые пятна мы и представили в виде воронок, куда «утекает» вычисленная информация. А её «выброс», «вытекание» обратно через иные воронки – создаёт вероятностную природу «прорисовки» на мельчайших, то есть квантовом и до-квантовом уровнях.
Далее. Казалось бы, что логично будет осуществить некоторую демаркацию самих воронок на «входные» и «выходные». Но всё же нам представляется это не необходимым, так как мы, как уже упоминалось выше, совершенно не обязательно полагаем необходимость именно физического моделирования N5, так как его вполне можно редуцировать просто к принципу затягивания/выталкивания; к тому же мы всё-таки полагаем, что навязчивое стремление к соблюдению принципа «бритвы Оккама» ещё никому не повредило.
Поясним на примере. Представим себе некий специфический калькулятор, который к любому результату любых вычислений добавляет случайное число. Предположим, что у нас полностью отсутствуют некие иные способы конструктивного определения итогов вычисления. Каждый раз, когда мы пытаемся вычислить точный результат какой-либо операции – мы получаем приближение, сформированное с учётом «внутренних» особенностей самого калькулятора. Это очень притянутый, но всё же интуитивно понятный пример того, каким нам видится функционирование реальности на данном уровне в контексте нашей модели. Само собой разумеется, что такое понятие как случайность не может быть полностью правомерным на всех уровнях, так как оно всегда оставляет вопрос на тему «почему именно это значение, а не другое?». И при невозможности сформировать хотя бы предположение на эту тему, зачастую осуществляют теоретический уход далеко за пределы научного познания. Таким образом, нам представляется целесообразным предположение о том, что конкретное значение, которое некоторым образом «вытекает» из воронки просто некоторым «перевычислением» ранее «утёкшего» значения, с учётом специфики «имманентной арифметики» N5.
Возможно могло бы оказаться целесообразным разделение воронок по тому же типу, что и иррациональных чисел: на обладающее меньшей условной мощностью множество «алгебраических» и большей, соответственно, мощностью – множество трансцендентных. Однако именно этот вариант не представляется нам достаточно обоснованным. Некоторые значимые аспекты из имеющей место быть демаркации иррациональных чисел на алгебраические и трансцендентные мы выведем чуть позднее и непосредственно применим эти выводы к формируемой нами модели, но вот сами воронки – нам представляются до известной унифицированными. В сущности, на «поведение» воронок тем или иным способом должно влиять столь огромное количество разнообразных факторов, что ситуация и без дополнительных сложностей представляется такой, что, если бы модель подобного уровня формировал тот же, кто сотворил Вселенную (при принятии этой гипотезы, конечно), то он вряд ли захотел бы излишне усложнять те аспекты, где можно сделать то же самое столь же эффективно за счёт более простых решений. То есть логично предположить, что «Бог мыслил просто», что соответствует мнениям на эту тему таких фигур, как Платон, Аристотель, Фома Аквинский, Дунс Скотт, Уильям Оккам и прочие, а также это было официально принято католической церковью в 1215-ом году [c].

6.3. ОБЩЕЕ ОБОСНОВАНИЕ РЕАЛИЗАЦИИ ФИЗИЧЕСКИХ ФЕНОМЕНОВ НА МОДЕЛИ «РВАНОГО» КОНТИНУУМА

Далее, просто для примера, в самых общих чертах покажем, каким образом функционирование некоторых физических феноменов может быть представлено и реализовано на субстрате нашей модели.
Электромагнитное взаимодействие на квантовом уровне очень удобно интерпретируется с точки зрения концепции воронок, так как получается полностью обоснованной вероятностная природа этого феномена. Также, для примера, такое понятия как электромагнитное поле может быть представлено в качестве некоей локальной деформации caseum с соответствующими эффектами. Скорость света представляется в таком контексте «минимальной скоростью вычислений» в «обычных» условиях, но про скорость света мы подробнее скажем чуть позже. Под «обычными» условиями здесь понимаются ситуации без геометрически сложных и в общем случае нетипичных деформаций сферы.
В широком же смысле является вполне ясным, что воронки служат в качестве общих для всей модели каналов передачи информации и взаимодействия между элементами. С этой точки зрения, к примеру бета-распад, как пример слабого ядерного взаимодействия в виде преобразования, допустим, протона в нейтрон, тоже весьма «плавно» интерпретируется на основе взаимодействия «блоков информации» (то есть тех же частиц, например) через воронки. Собственно, всё слабое ядерное взаимодействие может быть проинтерпретировано таким простым способом.
В контексте сильного ядерного взаимодействия описание поведения, например, кварков – отлично вписывается в объяснение их взаимодействия через всё те же воронки. По большому счёту, если у нас, допустим, есть некоторая частица совершенно любой природы, то эта её «природа» являет собой результат вычислительной деятельности на квантовом уровне и поэтому, при «утекании» данной информации через воронки и получении из воронок же «новых вводных» – частица просто «станет другой» самым логичным образом и природу этого преобразования мы подробнее эксплицируем чуть ниже, на примере тоннельного эффекта. В общем то, все элементы модели представляют собой просто «блоки вычисляемой информации».
Из вышесказанного также очевидно, что на плацдарме нашей модели могут быть по сути объединены фундаментальные взаимодействия.
Конечно же нельзя не упомянуть о природе времени в контексте нашей модели, тем более что с тех позиций, на которых мы уже сейчас находимся, проблема не представляется особенно серьёзной. Если внимательно посмотреть на ту модель, которую мы «создали» с опорой на нашу интерпретацию континуума, как обладающего «воронками», то она – модель – представляется просто одним очень большим вычислительным процессом. Из теоретической физики мы знаем, что время может идти с разной скоростью в зависимости от различных физических параметров актуальной ситуации, к примеру, в чёрных дырах оно подразумевается в сущности околонулевым, также при приближении к скорости света оно замедляется пропорционально приближению к данному пределу. Однако, как мы уже говорили про чёрные дыры, они очень хорошо описываются через большие совокупности воронок. Также, на квантовом уровне время подразумевается развёртывающимся совершенно иначе, чем на уровне макромира. Нам представляется это логичным, и мы предполагаем, что чем быстрее завершаются вычисления, тем медленнее «идёт время».
Немного поясним, что мы имеем ввиду: мы полагаем, что чем скорее некий вычислительный процесс «получает» конструктивное разрешение или же актуальное «завершение», тем меньшее количество времени на это затрачивается – это само собой понятно; но мы здесь идём чуть дальше и выводим, что чем более «быстро-вычисляемой» является некая упорядоченность (будь то галактика или человек), тем менее она подвержена «влиянию времени» в классическом понимании. В сущности, при отсутствии необходимости вычислений вообще – времени не будет существовать. А все остальные ситуации – просто некоторое приближение к подобному. На мельчайшем же уровне ответ на вычисления получается практически сразу же. Конечно, «ответ» тут же перестаёт «быть ответом» и мгновенно (практически «вне времени») преобразуется обратно в «вопрос» и так далее – по циклу. Тем самым выведем, что время представляет собой в буквальном смысле функцию скорости вычислений. «Ход времени» и его явное влияние на события макромира мы считаем основанным на латентном остатке вычислений, которые суммарно приводят как раз к тому, что мы и воспринимаем как «стрелу времени». То есть, чем более сложным и многофакторным является вычисляемый объект, тем быстрее «идёт его время» и тем быстрее «иссякает» его «вычислительный ресурс».
Само понятие латентного остатка мы здесь не станем глубоко характеризовать, а скажем лишь о том, что этот самый латентный остаток представляет собой основу для так называемой «эмерджентности», то есть для реализации феномена целостности, и лучшей метафорой для этого представляется следующая: «целое больше, чем сумма всех частей». В этом плане интересна некоторая аналогия. Например, что условно «легче» вычисляется – камень или человек? Интуитивно представляется, что камень. Вот поэтому и «стареет» камень медленнее, а человек – быстрее. Можно возразить, что компьютер тоже осуществляет очень много вычислений, но «стареет» гораздо медленнее человека. На это мы ответим, что вычислять собственный «вычислительный аппарат» компьютеру, например, не приходится, а вот человеку приходится – это и есть разница в наличии или отсутствии «жизни», которая, кстати, так и определяется с этих позиций: жизнь – это необходимость вычисления собственного «вычислительного аппарата», что, походя дополним, также является серьёзной отсылкой к уже не раз упомянутому феномену джиминации. В этом смысле очевидно, что для специфических вычислений некоторые «аппараты» подходят лучше, чем иные. Также скажем о том, что наше определение жизни может показаться слишком ограниченным, так как оно, вроде бы, не учитывает такие феномены, как сознание, эмоции, социальная сфера и прочие. Но на самом деле мы намеренно вводим наоборот весьма широкое определение, так как полагаем, что в рамки нашего определения должна попадать любая форма жизни в контексте модели, сколь непохожей на людей, животных, простейших микроорганизмов, «совокупности атомов углерода» бы она ни была. А наиболее общими качествами любой жизни, как некоей самоорганизованной упорядоченности, являются способность сохранять информацию и способность преобразовывать эту же самую информацию. И, с этих позиций представляется достаточно очевидным, что данный феномен зиждется именно на введённом нами понятии джиминации, точнее – на том рекурсивном процессе, который под этим понятием нами подразумевается.
Теперь неожиданно, но ненадолго перепрыгнем в совершенно ином направлении, а именно – к проблеме сознания, чтобы показать, каким образом можно использовать предлагаемый нами понятийный аппарат и нашу модель для описания подобных феноменов. Здесь скажем, что ведь по сути, и человеческое тело и вообще любое тело – как физический объект, представляет собой не более, чем каким-либо образом упорядоченную и «скрученную» локацию самой сферы – caseum. И в любом случае само это «тело» на квантовом уровне (уровне «реальных» вычислений) представляет собой некую совокупность вычисляемой и «вычисленной» информации. Таким образом, мы можем постулировать, что, исходя из предлагаемой нами модели рваного континуума, сознание может быть описано как явление, базирующееся на специфической локальной деформации caseum, при которой, ввиду необходимости вычисления собственного «вычислительного аппарата» и «латеральной» этому явлению реализации феномена джиминации, формируется некое до какой-то степени (но, разумеется, не полностью) замкнутое пространство воронок. И смысл этого в том, что при деформации подобного рода вычисляемая информация, предположительно, может становится менее «разреженной» и хаотичной, и, соответственно, более упорядоченной и стабильной, так как на уровне воронок в этой локальной полу-замкнутости присутствует более или менее однотипная информация, все внешние влияния на которую являются как бы ослабленными, что представляет собой логичное следствие полу-замкнутости локации и ведёт к своеобразной стабилизации состояния системы. То есть то, что мы понимаем под феноменом сознания, представляет собой результат специфической деформации сферы с образованием полузамкнутого пространства (множества) воронок, функционирующих в большей степени «сами в себя», что собственно и представляет собой реализацию феномена джиминации в этом самом полузамкнутом пространстве. Ещё раз и ещё проще, сознание – это специфическая деформация сферы с образованием полузамкнутого пространства воронок, что позволяет стабилизировать процесс «имманентных» вычислений. С такой точки зрения, например, шизофрения может быть описана, как нарушение стандартной (для человека) формы деформации сферы с получением большего доступа к «внешней» информации и, как следствие, появлению некоторой «спутанности».
Выходя обратно из области проблемы сознания к непосредственно нашей теории, немного скажем о том, что вообще подразумевается нами под большей или меньшей «упорядоченностью», «стабильностью», «устойчивостью» и прочими характеристиками caseum. Как мы помним, демаркация проходит между рациональными и иррациональными числами. То есть, топологически – под теми «локациями» caseum, которые могут быть взаимно-однозначно отображены рациональными числами и теми «диапазонами» актуально несформированных участков, которые представлены воронками и могут быть приближены с помощью чисел, соответственно, иррациональных. Локальные деформации сферы формируют соответственные численные описания и те из этих деформаций, которые взаимно-однозначны по большей части числам рациональным – должны будут, в соответствии с нашим подходом, подразумеваться более «стабильными», чем те, в которых соотношение обратное. То есть, формальнее – степень «стабильности» локального участка caseum определяется как мощность множества всех вещественных чисел, которыми описывается локация минус мощность множества всех иррациональных чисел, которыми описывается локация.
Ну и вернёмся в русло физики, конкретнее – квантовых эффектов. Для начала рассмотрим гипотетическое понятие, которое определяется как излучение Хокинга. Согласно теории, пары частиц и античастиц могут возникать при квантовых флуктуациях в пространственно-временном континууме и когда одна из частиц попадает в чёрную дыру, а вторая нет – наблюдатель за пределами горизонта событий теоретически может увидеть эффект, который как раз и определяется как излучение Хокинга. Лично нам, в контексте нашей модели и при условии принятия предыдущих высказанных положений, представляется достаточно очевидным практически прямая параллель с описанными нами эффектами «утекания» и «вытекания» и интуитивным представлением о феномене излучения Хокинга. То есть, представляется, что одна частица, которую мы определяем просто как некий «блок вычисляемой информации» «утекает» в воронку, а обратно как раз-таки «вытекает» иная – «перевычисленная» с учётом состояния N5 частица, которая также представляет собой просто «блок вычисленной информации». Также следует заметить, что, как мы уже указывали ранее, вычисления конкретно в чёрной дыре представляются практически мгновенными, то есть время там не особенно влияет на происходящий процесс. При таком подходе можно предположить, что излучение Хокинга как раз и представляет собой внешнюю наблюдаемую составляющую «истинного» вычислительного процесса на тотальном процессоре – N5. В этом смысле излучение Хокинга – это «вычислительный пар».
Далее – эффект туннелирования. Под этим феноменом в контексте квантовой механики подразумевается возможность частицы преодолевать некоторые «барьеры» даже в том случае, если энергия частицы меньше необходимого для успешного преодоления барьера уровня. Само собой разумеется, что данный эффект является целиком и полностью подлежащим юрисдикции квантовой механики и не имеет аналогов в макромире. Визуализация данного феномена представляет собой весьма красивое и загадочное зрелище: приближение совокупности частиц к барьеру и возникновение на противоположной стороне барьера некоторого, меньшей размерности, чем сама изначальная совокупность, множества частиц. Может сформироваться впечатление, что частица, исчезнув с одной стороны барьера, тут же возникает с другой. Эффект туннелирования имеет вероятностный характер. Если рассматривать данный феномен с точки зрения формируемой нами модели, то необходимо вспомнить каким образом должно определяться состояние любого элемента модели. Если говорить о том, что важно для интерпретирования ситуации с туннелированием, то вспомним, что на состояние любой локации caseum влияет результат того процесса, который мы определили, как «перевычисление». То есть, при «утекании» вычисленной информации в воронки результирующее «перевычисление», которое будет значением «вытекающей» информации, зависит в том числе и совокупности локально поступивших в N5 значений.
Говоря буквально, состояние каждой частицы в некоторой локации, предположительно, определяется в некоторой степени вообще всеми теми информационными блоками, которые актуально «наличествуют» в N5, но в несколько большей степени – теми информационными блоками, которые геометрически «ближе». Если бы не этот фактор, то вряд ли можно было бы сформировать устойчивые системы – модель просто представляла бы собой всё время изменяющийся хаос. Но, поскольку так происходить не должно и в какой-то мере устойчивые системы наличествовать должны, то представляется вполне логичным, что на актуальное состояние любой локации могут влиять «ближайшие наличия» сильнее, чем отдалённые. Конечно, на уровне макромира нашей модели результаты уровня квантовых вычислений не будут заметны для невооружённого глаза, но вот непосредственно на самом квантовом уровне – уже вполне.
То есть, можно предположить, что вычисленная информация элементов с одной стороны барьера, при попадании в N5 вполне логично может повлиять на состояния элементов с противоположной стороны этого же барьера. Здесь можно задать вопрос: почему же именно с противоположной стороны барьера, а не, например, на месте самого же барьера или вообще где угодно? Это потому, что состояние любой, начиная с самой мельчайшей, организованности caseum определяется, по существу, неким «средним арифметическим» между текущим состоянием, информация о котором «утекает» и состоянием «перевычисленным», информация о котором, соответственно – «вытекает». То есть, если информация о «импульсе» частицы и её координатах поступает в N5, то наибольший шанс «превратиться» в неё будут иметь те частицы, которые, условно говоря, лучше всего для этого «подходят» и являются уже как бы «готовыми» к преобразованию в соответствии с «новыми вводными». Конечно же именно самой частице непосредственно «преодолевать» барьер нет никакой надобности – она как бы просто «передаст» информацию о «намерении» в N5 и эта самая информация, многократно «перевычисленная» и несколько «распределённая», будет «доведена» до всех доступных частиц, и те из них, которые могут быть преобразованы до состояния той изначальной частицы – будут преобразованы соответствующим образом. Очень отдалённо и крайне приблизительно описанный нами процесс может напоминать некое суммирование векторов: то есть вектор «А» суммируется с вектором «В» их среднее значение добавляется к вектору «С», а затем осуществляется проверка – какие из значений вектора «С» находятся в диапазоне среднего значения вектора «А», те, соответственно, и считаются успешно «преобразованными». Вектор «А» представляет состояние частиц с одной стороны барьера, вектор «В» представляет актуальное состояние N5, а вектор «С», соответственно – состояние частиц с другой стороны барьера. Пример весьма притянутый, но всё же это хоть что-то.
Ну и – принцип неопределённости Гейзенберга и принцип дополнительности Бора. Тезисно говоря, первый принцип гласит, что чем точнее мы измерим какой-либо один показатель частицы (например, ипульс), тем менее точно мы сможем измерить некий другой (координаты, например). Данный принцип считается фундаментальным для Вселенной, то есть проблема не предполагается в уровне развития или качестве наших измерительных приборов, а вместо этого предполагается, что просто «так устроен мир». Тесно с принципом неопределённости пересекается принцип дополнительности, вполне в соответствии со своим названием, как бы дополняя его. Итак, принцип дополнительности Бора предполагает дуальность мира на квантовом уровне. То есть, например, свет может быть представлен как волной, так и частицей – и оба эти «представления» полагаются «истинно сущими». В соответствии с подобной дуальностью самого мира на квантовом уровне, таковой же, в соответствии с принципом, полагается и необходимость его описания – с учётом дуальной природы и одинакового уровня значимости разных представлений. Собственно, оба эти принципа, один в более явном виде, а второй – в менее явном, постулируют дуализм мира на глубинном уровне.
Если на мгновение взглянуть на ситуацию с сугубо философских позиций, то, собственно, утверждение об именно дуальной природе реальности изначально представляется прямо пересекающимся с нашей собственной теорией – ведь мы тоже проводим демаркацию на два глобальных регистра, один из которых представлен множеством всех рациональных чисел, а другой, соответственно, множеством всех иррациональных. Но всё же следует помнить о том, что иррациональных чисел в принципе не существует – под ними мы подразумеваем уникальные аппроксимированные идентификаторы и не более того. Так что наша демаркация, если уж смотреть глубже, это по сути про разделение на воронки и «не-воронки», то есть на упорядоченное и сконструированное с одной стороны, и бесконечно вычисляемое – с другой.
В общем смысле – это действительно сугубо философский вопрос, физический он скорее вторично. С этим постулатом мы не видим особого смысла спорить с позиций физического дуализма и утверждения о том, что «вот частица», а «вот волна», так как однозначно мы не можем утверждать ни того, ни другого – об этом, собственно, и свидетельствует принцип дополнительности. К тому же, если уж совсем глубоко анализировать, то и «частица», и «волна» – это скорее просто метафоры, которые представляют собой совокупный образ результатов измерений, чем гарантированно существующие точно в таком же виде реальные наличия. Синтез принципов неопределённости и дополнительности также свидетельствует о том, что мы не можем однозначно указать на то, с чем именно имеем дело. Что, собственно, уже само по себе являет собой косвенное и очень отдалённое (но всё же являет) подтверждение наших догадок о рваном и бесконечно вычисляемом континууме.
В общем, утверждение о дуальной природе реальности представляется нам в каком-то смысле весьма противоречивым. Интуитивно представляется, но всё же именно противоречивым. То есть это, как если бы у нас было не 36 классических систем счисления (и сколько угодно не классических), а соответственно – только 2. И мы бы утверждали, что мир, хоть и один, но он представлен двумя одинаковыми по значимости уровнями данных – 0, 1 и различные их сочетания в контексте двойственности. Нам же скорее представляется, что мы всё-таки просто имеем дело с различающимися описаниями одного и того же. Примерно, если судить по аналогии, как и с числами в различных системах счисления – одно и то же число в разных представлениях будет по-разному и выглядеть, но от этого оно не изменит своей абстрактной природы. Исключая, конечно, один случай, о котором мы сейчас скажем.
Собственно, если мы не можем однозначно утверждать, с чем имеем дело, но в то же время описывая ситуацию как бы с двух сторон, можем достичь более или менее полной картины реальности, то это уже должно нам нечто напоминать, с чем мы уже имели дело ранее. А именно: апории Зенона, парадокс Рассела и подобные ему, доказательство Гёделя, Тарского и прочие феномены, которые красной нитью проходят через всё наше исследование. Как мы неоднократно показывали, все эти феномены подпадают под юрисдикцию того, что мы определили, как джиминацию. Повторим ещё раз, что под этим процессом мы понимаем рекурсию некоего объекта с изменением хотя бы одного качества или свойства этого же рекурсивного объекта на каждом шаге процесса рекурсии.
Вот сформулированное высказывание в рамках некой формальной системы, значение которого можно редуцировать к утверждению о том, что это самое высказывание является недоказуемым в рамках этой же формальной системы – будучи в одних символических «локациях» смыслового контекста формальной системы истинным, а в других – ложным, это высказывание представляет собой тот самый рекурсивный объект, о котором мы и говорим в контексте феномена джиминации. «Парадокс лжеца», в контексте которого один и тот же критянин предстаёт в разных «смысловых участках» и лжецом и говорящим правду и тому подобные феномены – все они были нами отнесены к той категории, основу для которой как раз-таки и составляет джиминация. И мы ранее указывали на то, что в контексте джиминации не соблюдается принцип рекурсивного вывода, который подразумевает то, что на любом уровне вложенности рекурсивный объект должен обладать равными возможностями. А так как в контексте реализации феномена джиминации рекурсивный объект изменяется при каждом переходе, то мы постулировали, что имеем дело не с одним объектом, а с разными «ипостасями» одного и того же объекта, каждую из которых мы определили в отдельном виде – в виде уникального объекта. То есть мы имеем дело, в этом смысле, не с одним объектом, а с разными. Однако и тот и другой объект – оба формируются именно рекурсивно и обязательно с изменением хотя бы одного свойства или качества объекта. Таким образом, джиминация «начинает» с одного объекта, тут же – в момент начала – формируя на его основе два различающихся объекта и далее продолжает делать то же самое – бесконечно.
Теперь же посмотрим на, собственно, этот же дуализм в контексте квантовой механики. Как представляется нам, он может быть достаточно корректно охарактеризован с очерченных выше позиций. То есть предположим, что изначально мы имеем дело всё-таки с одним объектом (элементом, частицей, логическим высказыванием и так далее), но в тот самый момент, когда мы хотим тем или иным образом проинтерпретировать то, с чем имеем дело – в тот же самый момент осуществляется запуск процесса джиминации и «формируется» 2 разных объекта. На уровне макромира это не будет заметно, по причине существенной разницы в скоростях, но вот на квантовом уровне (или при осмыслении высказываний) мы уже можем явственно идентифицировать «латентное раздвоение». Будь то частица или высказывание – изначально оно пребывает в единственном уникальном виде, но при попытке каким-либо образом с ним манипулировать – раздваивается на «похожие», но различающиеся ипостаси. Чем больше «истинности» в одной ипостаси, тем менее – в другой. И здесь одни и те же высказывания одинаково применимы как к реальности на уровне квантовых эффектов, так и к формулировкам в контексте достаточно богатых формальных систем. В этом смысле целесообразно повторить ранее уже постулированное – противоречивость лежит в основе функционирования нашей модели на самом глубинном уровне и, собственно, эта самая противоречивость и является «генератором жизни».
И ещё раз в этом же смысле, однозначность – означает смерть и чистое небытие, а диалектика жизни явственно противоречива. Так что наше высказывание о противоречивости постулата утверждающего двойственную природу реальности одновременно и верно и, как и следовало ожидать – не верно. Таким образом, в некотором роде теорема Гёделя верна для всей нашей модели, а принцип неопределённости Гейзенберга и дополнительности Бора – для достаточно богатых формальных систем. Однако и в одном и в другом случаях мы имеем дело с одним и тем же процессом – джиминацией. И в этом смысле – основа всё-таки не дуальная, а именно единая.
Раз уж была затронута тематика противоречивости, то нельзя не сказать об одном моменте, который логически следует из наших предположений о воронках, «утекании» и «вытекании». Как мы предполагали выше, на «диапазонах» наличия воронок реальность представляет собой достаточно неупорядоченную систему. Неупорядоченную в том смысле, что осуществляется постоянное «перевычисление» локаций и их состояний. В сущности, если следовать соотношению между рациональными и иррациональными числами – с одной стороны, и «определёнными» и «определяющимися» топологическими локациями – с другой, то «стабильной» является в каждый отдельный момент около 5% Вселенной, а остальная – бесконечно «перевычисляется». Отчего вокруг не должен происходить логичный и очевидный при таком соотношении хтонический хаос мы несколько пояснили выше за счёт предположений о степенях влияния «перевычисляемой» информации на конкретные участки, где предположили, что близлежащие будут влиять сильнее, чем более отдалённые, что позволит несколько стабилизировать информацию; а также мы предположили, что на «стабильность» тех или иных участков caseum существенным образом влияют его локальные деформации, за счёт которых возможно формирование некоего минимально «необходимого» для стабильного состояния уровня наличия взаимно-однозначных рациональных чисел, которыми можно описать локацию.
Если совсем уже отпустить «путы смыслового скакуна», то в контексте этой терминологии, к примеру, смерть организма в контексте нашей модели можно описать как специфическую локальную деформацию сферы, которая приводит к возрастанию сверх критического уровня мощности множества иррациональных чисел, взаимно-однозначных с геометрией локации, которая в данном случае представлена организмом. Но, это просто к примеру, и на этом пути слишком уж контр-интуитивными представляются оттенки смысла, хотя он сам, собственно, кажется в общем достаточно конструктивным. То есть, если человек, например, попал под поезд или упал с большой высоты – то данное нами определение смерти представляется очень даже интуитивно понятным; если человек допустим болен раком или отравился – тоже. Но вот, если ему в сердце попала пуля – уже нет. Хотя даже такая ситуация может быть логически обоснована с точки зрения локального распространения «перевычисленной» информации, обладающей достаточным «иррациональным зарядом» – и быть может это отдалённо и примерно так и работает на глубинном уровне. Но, опять же – это просто к примеру.
Добавим ещё, что сам принцип суперпозиции, как некоей возможности «элемента» находится одновременно в различных, до известной степени эквивалентных, состояниях – аналогичен «интервалу» воронок, который идентифицируется при помощи любого иррационального числа: при помощи «одного» иррационального числа «закрывается» целый диапазон воронок, каждая из которых в некотором смысле «топологически эквивалентна» этому числу.

6.4. АЛГЕБРАИЧЕСКИЕ И ТРАНСЦЕНДЕНТНЫЕ ИРРАЦИОНАЛЬНЫЕ ЧИСЛА В МОДЕЛИ «РВАНОГО» КОНТИНУУМА

Что следует отдельно оговорить, так это некоторое противоречие между «утеканием» вычисляемой информации в воронки и её «вытеканием» из, соответственно, этих же воронок. То есть, как мы помним, в соответствии с предполагаемым (в этот раз – не нами предполагаемым) соотношением тёмной материи и тёмной энергии во Вселенной, «напор» «вытекания» должен превышать напор «утекания» и между ними должно быть установлено соотношение примерно 2 к 1. А значит, что, в соответствии с нашими предположениями о соотношении трансцендентных и алгебраических иррациональных чисел – у них должно быть ровно такое же соотношение и, более того, если следовать и далее по пути нашей же логики, то «утекающими» представляются алгебраические числа, а «вытекающими» – трансцендентные. Собственно, если всё так и должно обстоять в контексте нашей модели, пусть хотя бы в контексте нашей «теории», то у этого должно быть какое-либо обоснование. Для того, чтобы определить это самое обоснование, необходимо несколько глубже исследовать природу алгебраических и трансцендентных иррациональных чисел.
Собственно, сразу скажем, что ни тех, ни других – «не существует» в том же виде, в котором «существуют» числа, соответственно, рациональные, целые, натуральные. И алгебраические и трансцендентные иррациональные числа представляют собой аппроксимации, которые мы ранее охарактеризовали в качестве идентификаторов неких бусконечных вычислительных процессов, «развёртывающихся» в направлении неких диапазонов воронок, расположенных между двумя рациональными числами. Вообще, сами вещественные числа в качестве определителей для тех или иных последовательностей, которые могут быть бесконечными, предложил ещё сам Кантор, о чём мы ранее уже упоминали. Однако мы хотели сделать отдельный акцент на том, с чем мы имеем дело – с «идентификатором», которым мы называем некоторую область геометрической фигуры – и не более того.
Кстати сказать, если прямо из этой части нашего изложения перейти на мета-теоретический уровень, то можно заметить, что в некоторых «символических локациях» нашего смыслового контекста через природу иррациональных чисел обосновываются многие квантовые эффекты и вообще функционирование всей нашей модели, а вот, например, прямо здесь – это просто «идентификаторы» некоторых недифференцированных диапазонов на геометрических фигурах. В этом и проявляется фундаментальная имманентная противоречивость природы вообще, которая предоставляет богатый спектр возможностей и обеспечивает «функцию существования» – иначе ничего бы «не работало».
Итак, вернёмся к иррациональным числам. Так как ни алгебраических, ни трансцендентных чисел «не существует», то могло бы показаться, что логично ввести также и «степень существования чисел», предложив более низкую для трансцендентных и, соответственно, более высокую – для алгебраических. Некий смысл в этом наличествует, но всё же однозначно сделать вывод о «большем» или «меньшем» приближении к реально существующим числам ни в одном, ни в другом случае нельзя. А само определение «степени существования», которая в данном случае должна называться скорее «степенью не-существования» выглядит пока не вполне конструктивно. Так в чём же разница между алгебраическими и трансцендентными числами?
Собственно, эта разница в классическом варианте обычно определяется так: алгебраические числа могут служить корнями полиномов с рациональными коэффициентами, а трансцендентные – нет. Как это выглядит. Допустим, мы можем записать выражение «x ^ 2 – 2 = 0», где х – переменная. И решением этого уравнения является квадратный корень из числа 2, который, ввиду как раз этого факта, определяется как алгебраическое иррациональное число. Логично было бы задать вопрос о том, как это, собственно, вообще возможно? То есть мы же не знаем и не можем знать такого числа, как квадратный корень из числа 2, так как его просто не существует. Поэтому, в реальности что бы мы не поставили на это место этого самого х – по итогу 0 мы не получим и равенство не будет выполнено. И то, что мы здесь постулируем – на самом деле является верным и ни один математик не станет спорить с подобной постановкой вопроса. Таким образом, всё же, как и почему?
А дело здесь в том, что мы только что натолкнулись на ещё одно «допущение» в контексте математики. Ранее мы уже близко познакомились с понятием актуальной бесконечности – фундаментальным «допущением» в рамках теории множеств. Мы исследовали это понятие, охарактеризовали его свойства, очертили границы его применимости и, что самое главное, указали критерии тех ситуаций, в рамках которых использование актуальной бесконечности может считаться «условно правомерным». Собственно, мы сейчас как раз и находимся в ситуации нарушения заданных нами ранее критериев применимости понятия актуальной бесконечности. Мы имеем дело с «допущением» иного уровня – когда собственно ни вычисление, ни аппроксимация даже не подразумеваются. То есть в случае с актуально бесконечными множествами мы работали с «допущением» вида «если бы бесконечность реально существовала, то Х», где Х – то утверждение, которое мы хотим доказать. И путь, соответственно, здесь один – через «допущение» о возможности актуальной бесконечности реально существовать.
А вот уже в нашем теперешнем случае мы имеем допущение вида «если бы актуальная бесконечность могла существовать и если бы Х могло существовать, то Y», где Х – то число, которое мы ищем, а Y – утверждение, которое мы хотим доказать. И является в общем-то понятным, что ни одно, ни другое условия соблюдены быть не могут. Но, в том случае, если бы могли – то алгебраические иррациональные числа определялись бы правомерно. В общем-то здесь мы ведём себя так, как будто считаем, что то, что существовать не может – существует. И стоит признать, что определённый смысл в этом всё-таки имеется и заключается он в том, что это, собственно, единственный критерий различения алгебраических и трансцендентных чисел.
Потому, что для трансцендентных чисел, в соответствии с их определением, не существует даже тех полиномов с рациональными коэффициентами, куда мы можем подставить «допущение» с алгебраическими числами так, чтобы выполнялось некое равенство. Порой также определяют, что числа алгебраические могут быть «получены» за конечное число операций, но это мы даже не будем разбирать, так как понятно, что даже за актуально бесконечное число операций оно не будет получено, а сама формулировка обладает смыслом, сопоставимым с предположением о том, что «за бесконечное число операций может быть получено число Х, которое и является, и не является натуральным». Подобное высказывание может быть верным только в случае использования логики, существенно отличающей от классической. К примеру, если бы мы имели дело не с высказыванием вида «квадратный корень из числа 2», а хотя бы с высказыванием «член потенциально бесконечного ряда, полученный на n-й итерации процесса аппроксимации квадратного корня из числа 2» – вопросов бы в принципе не было. Не было бы касаемо правомерности конструктивного определения иррациональных чисел. А вот иные вопросы посыпались бы как из рога изобилия, что, собственно, в полной мере демонстрирует наше исследование.
И среди этих вопросов базовым является фундаментальная значимость принципиального несоответствия геометрической фигуры и её численного описания. Как только подобное становится понятным – следуют нетривиальные предположения об устройстве реальности вообще. На основе означенного несоответствия мы, вполне в соответствии со спецификой контекста, вывели его отрицание и пришли как раз-таки напротив – к полному соответствию, на основе которого предложили вообще нашу модель, «динамическим» фундаментом которой являются именно глубинные противоречия и кажущиеся «несовершенства». И всё это на основе постулата о том, что иррациональных чисел не существует, а «вместо них» – бесконечные вычислительные процессы.
Однако математики, глядя на ровно то же самое что и мы, пришли к совершенно иным умозаключениям, что мы ранее и показывали. Вполне в соответствии с тенденцией всё упорядочить, «расставить по полкам» и нивелировать всё «неаккуратное» иррациональные числа были призваны заполнить явно имеющие место быть пустоты. В случае с трансцендентными было несколько «сложнее», а в случае с «алгебраическими» – «проще», но всё же дело было сделано.
В общем случае получилось, что алгебраические иррациональные числа «могут подразумеваться» в контексте некоторой операции с числами рациональными для получения однозначно верного решения. То есть, если бы они могли существовать – это бы работало. И это всё же хоть какой-то критерий. Так как, повторим, для трансцендентного числа не существует «такого места», куда его можно поставить в контексте некоторой операции, рядом с числами рациональными так, чтобы соблюдалась правомерность вывода.
Обобщим: алгебраические иррациональные числа могли бы быть получены при выполнении некоторой операции с числами рациональными, если бы они вообще существовали. А числа, соответственно, трансцендентные – не могли бы быть получены при выполнении какой-либо операции с числами рациональными, даже в том случае, если бы они вообще существовали. Наши высказывания правомерны с логической точки зрения, но, если сделать ещё один логический шаг, то второе высказывание всё же приводит к тому, что трансцендентные иррациональные числа не существовали бы даже в том случае, если бы они существовали – при условии как минимум функциональной эквивалентности «существования» с «возможностью» быть полученным при помощи операций с рациональными числами.
Стоит заметить, что идейный противник Кантора – Кронекер, одобрил бы высказывание подобного рода, так как, по его мнению, которое мы уже ранее упоминали, «Бог создал целые числа, остальное – дело рук человека». Если Кронекер столь непримиримо критиковал Кантора не только за теорию множеств, но и за его обоснование чисел вещественных, то можно предположить, как бы он отнёсся к тому, что высказывается нами в ходе данной части нашего изложения. Однако, мы всё же считаем, что в том, что «трансцендентные иррациональные числа не существовали бы даже в том случае, если бы могли существовать» – свой определённый смысл всё же наличествует. Раз уж «Бог создал целые числа», то можно предположить, что затем на их основе человек создал числа рациональные, а потом смог предположить наличие и иррациональных алгебраических чисел в некоторых символических «локациях» арифметических операций. Но вот с трансцендентными всё же несколько иная история.
В общем то, множество всех рациональных чисел является счётным, что означает, напомним, возможность поставить ему во взаимно-однозначное соответствие равномощное множество натуральных чисел. Множество алгебраических иррациональных чисел – также счётное. А вот уже множество трансцендентных иррациональных чисел счётным как раз-таки не является. На этом моменте целесообразно несколько остановится и прояснить причины этого. Если несколько абстрагировать ситуацию, то более или менее правомерной будет следующая формулировка: множество всех алгебраических иррациональных чисел является счётным потому, что множество всех рациональных чисел является счётным и потому, что алгебраические иррациональные числа в некотором роде «основываются» на числах рациональных и могут быть представлены, как результат операций с ними. Разумеется, данная формулировка уже подразумевает те допущения, которые были приведены нами ранее и, в сущности, представляется целесообразным не забывать о том, с чем мы всё же имеем дело.
Теперь же, что касается иррациональных трансцендентных чисел. Как сказано выше – их множество не является счётным. И оно не является счётным по той причине, что, ввиду доказательств несчётности множества всех вещественных чисел и тому, что трансцендентных чисел считается «больше», чем алгебраических, соответственно, следует, что и множество всех вещественных чисел также не является счётным именно по причине наличия в нём чисел трансцендентных. Следуя далее станет понятно, что и множество комплексных чисел также не будет являться счётным из-за трансцендентных чисел. Они как бы «рушат» всю симметрию. Из истории математики мы знаем, что пифагорейцы имели представление об иррациональных числах, но старались это скрывать, так как те казались им «неправильными». Известно даже, что во время плавания на корабле некий грек в результате расчётов, суть которых канула в глубину веков, получил иррациональное число и был выброшен за борт за то, что «внёс разлад в этот мир».
И если алгебраические иррациональные числа ещё могут быть неким образом сопоставлены с числами рациональными, то трансцендентные – уже нет. Возможно, это не является особенно заметным из всего нашего исследования, но всё же мы стараемся по мере возможности не вводить новых понятий и классификаций. Однако теперь сам контекст в сущности не оставляет нам выбора. Итак, мы полагаем, что будет уместно и целесообразно осуществить демаркацию на 2 различных вида деятельности по «уточнению» тех или иных аспектов реальности и здесь неважно – объективная или «математическая» реальность уточняется, разделение в любом случае производится на: расчёты и измерения.
Расчёты – это то, что может базироваться на использовании тех или иных рациональных чисел. Измерения же – это «сопоставление» тех или иных частей и фрагментов геометрических фигур (в самом общем смысле слова), которые не могут быть выражены рациональными числами. Говоря более буквально – при помощи «только чисел», мы не можем «измерить» некоторые соотношения. К примеру, из уже упомянутых соотношение диагонали и стороны квадрата, длина окружности в некоторых случаях, также синус и косинус от 1 и так далее – в результате получаются трансцендентные иррациональные числа. Иными путями получаются алгебраические, но вот в случаях именно измерений (которые традиционно полагаются «обычными» расчётами) – числа именно трансцендентные. 
С означенных позиций вывод представляется нам достаточно очевидным. И заключается он в предположении о том, что в рамках нашей модели алгебраические иррациональные числа должны будут представлять результат наших попыток «запуска» потенциальной бесконечности в виде «стремления» вычислить те или иные иррациональные значения и именно они и являются «утекающими»; числа же трансцендентные можно определить, как «перевычисление» – то, что «вытекает» и вносит полный «разлад» в симметрию и непротиворечивость. В данном контексте складывается «весь остальной» пазл. В этом смысле представляется, что как будто числа алгебраические генерируем «мы сами», а вот числа трансцендентные нам уже именно «предоставляются». Мы, конечно, не стали бы постулировать того, что в нашей модели всё должно быть устроено именно таким образом, однако именно примерное направление представляется нам достаточно чётко определённым и, сколь это вообще возможно на данном этапе, детально охарактеризованным. Также мы здесь не говорим о том, что результатом «перевычисления» являются трансцендентные иррациональные числа и только они – мы полагаем, что они могут быть получены только таким путём, но это не значит, что таким путём не могут быть получены и рациональные числа тоже.

6.5. КОМПЛЕКСНЫЕ И ПРОСТЫЕ ЧИСЛА В МОДЕЛИ «РВАНОГО» КОНТИНУУМА

В ходе ознакомления с нашим исследованием у некоторых (из тех, кто разбирается в математике), возможно, возникнет некое соображение относительно того, почему же мы не «идём дальше» в контексте расширения специфики рассматриваемого иерархичного числового ряда, а столь упорно сосредоточились непосредственно на числах вещественных и практически на них одних и строим весь контекст. Почему же мы не говорим о комплексных и гиперкомплексных числах, таких как кватернионы, октинионы и так далее? Вот сейчас немного и объясним почему мы столь подробно рассматриваем именно вещественные числа.
Вещественные числа – это «последние» числа, которые можно назвать «аутентичными» числами. Все остальные – комплексные, кватернионы и так далее составлены из вещественных чисел и только вещественные числа являют собой «последнее уникальное расширение» численной системы. Мы не можем «свести» их к натуральным числам, потому что тогда и сами натуральные тогда пришлось бы сводить к цифрам. К цифрам действительно можно в той или иной степени свести вообще все числа и об этом мы ниже немного подробнее скажем. Но в этом контексте вещественные числа – последние «настоящие» числа, а всё остальное – уже надстройки. То есть вещественные числа - это «простые» (по аналогии с натуральными простыми числами), а все, что выше – «составные». Все вышележащие числа мы можем правомерно «разъединить» на вещественные числа и «дополнительные данные нечисловой природы», а сами вещественные – только на цифры, как и все нижележащие. В данном контексте, числа вещественные – это кирпичики, а вот уже надстройки – это стены, потолок, крыша, то есть комплексные числа, кватернионы, октинионы и так далее. Конечно, это всё важно и полезно и дом – это не груда кирпичей. Но всё же кирпичи – это именно вещественные числа и ничто более.
Здесь же, по тему момента дискурса, дополним то, что мы сказали о «цифровой редукции» чисел интересными и даже, конечно же, логически верными, но, разумеется, вряд ли имеющими право быть воспринятыми всерьёз, замечаниями. К примеру, если рассматривать несчётность множеств, как невозможность биекции с натуральным рядом, то вот вопрос: чему будет равна мощность актуально бесконечного множества всех натуральных чисел, записанных в унарном формате? Логичным будет предположить, что конечно же – алеф 0. Всё складывается. Тогда ещё один вопрос: чему будет равна мощность актуально бесконечного множества всех вещественных чисел, записанных в унарном формате? Напомним – мы говорим о множестве, а не о мультимножестве, то есть подразумевается, что одному множеству один отдельно взятый элемент не может принадлежать более одного раза. Тогда представим актуально бесконечное множество всех вещественных чисел и переведём каждое из них в унарный формат. Так чему равна его мощность? Его мощность равна 1, так как все числа в нём – одинаковые. В этом смысле можно доказать, что континуум в унарном формате является счётным, так как его мощность равна 1. В данном случае особенно ярко прослеживается буквально прямая зависимость между форматом представления чисел и их «поведением». Конечно, представлять множество всех вещественных чисел в унарном формате – несколько нерепрезентативно, так как оно, вполне логичным образом, превращается просто в бесконечную совокупность бесконечно «развёртывающихся» единиц, которые ни о чём не говорят.
На основе этих (а также некоторых предыдущих) умозаключений мы можем также рассмотреть континуум-гипотезу Кантора. В её рамках предполагается, что между мощностью актуально бесконечного множества всех натуральных чисел N, то есть – алеф 0 и мощность актуально бесконечного множества вещественных чисел R (континуумом) – 2 ^ алеф 0, нет никаких «промежуточных» мощностей. На фундаменте представленного соображения о конечности «дополнительного» множества вещественных чисел, которое было нами рассмотрено ранее, мы можем вывести некоторые следствия о гипотезе Кантора. Рассмотрим мощность актуально бесконечного множества всех вещественных чисел R вместе с мощностью его «диагонального дополнения» D, которое было охарактеризовано нами ранее, то есть «R + D = RD». И получим алеф 0 (ввиду того, что мы ранее показали невозможность потенциального увеличения актуально бесконечных объектов) + мощность D. То есть действительно мощность множества всех вещественных чисел, при добавлении к ней мощности «диагонального дополнения» оказывается выше, чем мощность множества всех натуральных чисел. Рассмотрим наименьшую мощность RD. Она равна 2, при представлении чисел в двоичной системе счисления. Также, в соответствии с тем, что мы ранее показали на примерах, что 1 она равна быть не может, так как в унарной системе счисления вещественные числа выглядят всё же не репрезентативно и мощность самого R в унарной системе счисления равна 1. Получается, что мощность R + мощность D – мощность N = Х = мощность D. А так как минимально возможная мощность D равна 2 и так как 1 она равна быть не может, следовательно, континуум-гипотеза Кантора верна, так как мощность множества натуральных чисел на интервале между 2 и нулём (оба числа не включительно), при логичном исключении 1 – равна 0, то есть пустому множеству натуральных чисел.
Конечно, оба вышеприведённых умозаключения со всей возможной строгостью не являются «серьёзными», однако те логические шаги, которые осуществляются на пути к выводам – в общем смысле корректны. Мы привели эти соображения для того, чтобы показать какое влияние на «поведение» чисел и их совокупностей могут оказывать различные их отображения.
Теперь же, для полноты контекста и пущей обоснованности наших предыдущих (не ближайших предыдущих) соображений, всё же уделим некоторое внимание и комплексным числам также. Собственно, прочному положению в области математики комплексные числа обязаны не кому иному, как самому Карлу Фридриху Гауссу. Причём, хотя и комплексные числа предлагались ещё до него и их обоснование в виде комплексного поля также ещё до Гаусса было предоставлено – никому до этого «не было дела» (Жирар, Даламбер, Декарт, Карно). И только тогда, когда уже сам Гаусс это сделал – то математики резко обратили на это внимание, назвали в честь Гаусса некоторые нововведения и начали серьёзно к ним относится. Вообще изначально Гаусс не горел желанием вводить комплексные числа в математику, однако их удобство при использовании в расчётах было несомненным и поэтому на данный момент «имеем то, что имеем». В сущности, комплексные числа обычно определяются примерно следующим образом: «комплексные числа – это числа вида (x + yi), где х – действительная часть, y – мнимая часть, а i – мнимая единица, такая, что i ^ 2 = -1». По нашему субъективному мнению, тому гуманитарию, который сумеет понять, что такое комплексное число, следует выдавать премию математического сообщества «За волю к победе» или же «За проявленную стойкость духа». Впрочем, не следует отвлекаться. Получается, что комплексное число состоит из двух вещественных чисел, вполне в соответствии с нашими предыдущими постулатами о том, что все числа, которые «выше» вещественных – композитные надстройки. Здесь сразу добавим, что, к примеру, кватернион – это такое же комплексное число, но так как оно подразумевается не двумерным, а четырёхмерным, то в его состав входят, наряду с «базовым» вещественным числом, сразу 3 мнимых числа; для октинионов полагается уже 7 мнимых чисел и так далее.
Демонстрируемая нами «композитность» комплексных и прочих подобных чисел не означает, что они не важны для математики, не значимы для практической деятельности или не привносят свойственных себе «инноваций» в общий контекст. Если, к примеру, мы захотим сформировать множество вещественных чисел, взаимно-однозначное некоему отрезку, то у нас (в рамках стандартной ситуации, принятой в классической математике) в общем-то получится то самое актуально бесконечное множество всех вещественных чисел с континуальной мощностью. Если же мы придадим отрезку ширину и сделаем его двумерным объектом – прямоугольником, например, то для описания (если мы берём в расчёт сугубо описательные характеристики) подойдёт актуально бесконечное множество пар вещественных чисел – первое число характеризует некую точку по оси Х, а второе – по оси Y, соответственно. Однако, для этой же самой цели могут подойти также и числа комплексные. И они – пары вещественных чисел и числа комплексные – будет одинаково хорошо «делать свою работу», но только в том случае, если мы подразумеваем именно статическое описание некоего объекта. Если же мы хотим описать внутреннюю динамику объекта (возможно весьма специфического объекта), то тут уже числа комплексные являются, конечно, очень удобными и до известной степени незаменимыми.
В общем, комплексное число состоит из 2-х вещественных чисел и мнимой единицы, которая обозначается как «i» и представляет собой «условный» корень квадратный из числа -1. Условный потому, что корень из числа должен быть всегда положительным, а здесь же – он предполагается равным -1, в общем-то для того, чтобы, как можно предположить, снизить вероятность ошибки при расчётах и дополнительно указать на сущность того, с чем мы имеем дело. Первые две составляющие комплексного числа – 2 вещественных числа, и они характеризуют каждое свою ось (Х и Y) – в этом смысле комплексные числа ведут себя ровно так же, как и просто пары вещественных чисел. Но мнимая единица «всё меняет» и добавляет возможность расчётов по «дополнительной» характеристике. К примеру, если у нас есть некая точка в двумерном пространстве, то, при каком-либо динамическом воздействии на неё мы бы хотели предполагать её поведение. Просто по точке этого не скажешь. А вот при внесении мнимой единицы – уже можно, так как в данном случае с её помощью мы можем задать некую дополнительную характеристику – например, «направление точки», то есть то, «куда смотрит» наша точка. И с учётом этого мы и можем осуществлять соответствующие расчёты.
Для кватернионов к примеру, даже несколько интереснее. Казалось бы – для чего вообще сразу 4 числа должны описывать 3-х мерное пространство? А всё потому, что одно из них описывает угол вращения, а остальные – направление этого вращения. Но, впрочем, если абстрагировать и предложить геометрическое обоснование (что в общем-то свойственно большей части нашего исследования), то можно вывести следующее. Те вещественные числа, которые входят в состав чисел комплексных и чисел более высокой размерности – ответственны за информацию о статическом положении некоей точки в пространстве, а мнимые единицы при этих числах – за информацию динамического характера и дополнительные данные о точках. На этом в общем-то и заканчивается налёт таинственности и сходит на нет весь пиетет перед «сложной абстрактной конструкцией». Интересно, что произошло это как раз-таки также в результате абстрагирования – абстракция абстракции рознь, судя по всему.
Теперь должно стать понятно, что если бы мы ранее ввели в контекст нашего изложения комплексные числа, то только в большей степени бы запутали и без того не простую ситуацию – ведь мы и демаркацию самих вещественных чисел на алгебраические и трансцендентные также «долго держали» и представили именно в тот момент, когда это стало одновременно и уместным, и необходимым. Также теперь понятно, что в общем-то в контексте построения нашей модели  рваного континуума, комплексные числа именно здесь и сейчас не играют особой роли. И это так по нескольким причинам.
Во-первых, потому, что как было сказано выше – вещественные числа представляют собой «последний бастион числовой аутентичности». То есть нас интересуют в первую очередь фундаментальные аспекты проблематики и в исследовательском смысле, и в контексте построения модели на основе нашей теории рваного континуума – а детали реализации в любом случае будут дорабатываться и уточняться, возможно даже и не только нами. Во-вторых, потому, что мы на данный момент представляем основу модели – её абстракцию. То есть нашей задачей является проработка наиболее «крупных» в смысловом плане деталей и составных частей модели, а остальное – это уже внутренние дисциплинарные составляющие. Понятно, что любую точку caseum можно представить неким числом или какой-либо совокупностью чисел.  Для нас важно – рациональные они или иррациональные, алгебраические или трансцендентные. Потому что в любом случае только из этих типов чисел будут состоять любые «описательные элементы» для нашей модели. И вот поэтому важны именно базовые типы, а остальные – это уже детали реализации.
Теперь же как раз к слову о базовых и фундаментальных феноменах. Выше уже упоминалось такое понятие, как «простое число». И, собственно говоря, здесь не имеется ввиду бытовое понимание «простоты» и «сложности». Здесь речь идёт о совершенно особом классе чисел – подклассе класса натуральных – простых числах. Вообще, иерархия чисел следующая (причём каждое последующее включает в себя предыдущее): сначала идут все натуральные числа (1, 2, 3, 4, 5), затем все целые числа (-2, -1, 0, 1, 2), за ними рациональные (0.5, 0.2, 0.1), потом вещественные или действительные (3.14, 1.414, 2.71828), потом комплексные (2.12344 + 4.674654i) и прочие числовые расширения. Так вот те числа, о которых мы говорим сейчас, относятся к самому первому и наиболее «реальному» классу чисел – к натуральным числам. Напомним, что о целых и только о целых числах Леопольд Кронекер говорил, что они созданы Богом. Натуральные же числа – это наиболее интуитивно понятное их подмножество. И вот в случае именно с простыми числами такое ощущение, что какой-то смысл в словах Кронекера всё же есть.
Итак, все натуральные числа можно подразделить на два класса – простые и составные. Конечно, при подобной демаркации, логично добавить ещё и третий класс с одним единственным представителем – 1, число не являющееся простым, но и не удовлетворяющее классическому определению чисел составных: все натуральные числа, за исключением 1, либо являются простыми, либо могут быть представлены в виде произведением некоторых простых чисел – и только 1 не соответствует ни то, ни другому критерию. Как мы уже говорили после разбора с октинионами простые и даже вообще натуральные числа кажутся действительно «простыми» и даже «приятными на ощупь». Однако, свои сюрпризы есть и у них и порой такие, что составляют практически неразрешимую на данный момент задачу. Конечно, можно натуральные числа также подразделить и по иным критериям: на чётные и нечётные, на совершенные и несовершенные, на избыточные и не избыточные, на палиндромные и не палиндромные, на числа фибоначчи не являющиеся ими, числа трибоначчи и не являющееся ими, на числа, которые могут быть представлены в виде суммы двух кубов двумя различными способами и остальные, и так далее – критериев огромное количество. Более того, в каждом подобном разделении есть свой смысл и каждый из формируемых подклассов будет представлять исследовательский интерес по причине наличия «в нём» специфических уникальных свойств.
Однако мы, по причине того, что наше исследование всё-таки ограничено по времени и объёму, хотим сосредоточить осмысление локального участка на природе именно простых чисел. Конечно, в стане простых чисел также возможны внутренние демаркации: на суперпростые числа, уникальные простые числа и так далее. Вообще простые числа – это числа, которые не имеют иных делителей, кроме 1 и самого себя. То есть, допустим число 4 мы можем без остатка поделить на 2, следовательно, 4 – не простое число; 6 мы можем без остатка поделить на 2 и на 3 – 6 тоже не простое, а составное число и так далее. А вот, например, числа 3, 5, 7, 11 и, отдельно – число 2 – не делятся на какие иные числа, кроме самих себя и 1. Здесь 2 стоит особняком по той причине, что 2 – единственное чётное простое число, все остальные простые числа – нечётные. Число 1 вообще, в некотором роде, нарушает все правила, так как всё же не вполне понятно, каким мы должны его считать – простым или нет. Оно не составное и это понятно даже просто на интуитивном уровне, и оно вроде как простое – по крайней мере именно такие подозрения возникают при взгляде на него, так как оно попадает под критерии – оно делится только на 1 и на само себя. Вот только это единственный случай, когда оба пункта сходятся в одном числе. В общем, чтобы не нарушать правил и не делать их неоднозначными, число 1 принято считать, как бы «отдельным подразделением».
По остальным же простым числам мы уже сказали – они делятся только на 1 и на самих себя. В этом смысле тот же 0 – не подходит под критерии, а также не подходят все нецелые числа – они не делятся на 1. В соответствии с классическим пониманием, которое принято в математике, простых чисел бесконечное множество, мощность этого множества равномощна множеству натуральных чисел, соответственно – множество всех простых чисел является счётным. Рассчитываем, что на данном этапе эксплицировать такие моменты, как логическое высказывание на тему, что если простых чисел меньше, чем натуральных, то как их множества могут быть равномощны – уже нет целесообразности, так как со свойствами бесконечных множеств мы достаточно много разбирались ранее. В общем – простых чисел бесконечное счётное множество. Те примеры простых чисел, которые мы привели ранее – 2, 3, 5, 7, 11 – понятно, что не имеют делителей, кроме самих себя и 1, но это представляется не особенно выдающимся достижением: например, до числа 11 в натуральном ряде, если не считать 0 и 1, всего 9 чисел. То есть не особенно удивительно, что число 11 на них не делится – их ведь достаточно мало, если смотреть с количественной точки зрения. И верно, для числа 11 – это так.
Вспомним, что мы говорили о числах вещественных: что они являют собой как бы кирпичики для формирования композитных надстроек – комплексных и гиперкомплексных чисел. Если рекурсивно применить эту логику к иерархии числового ряда, причём рассмотреть этот ряд не формально, а семантически, то в сущности мы можем представить простые числа в качестве неделимых строительных блоков для вообще всего числового здания, единственное что – добавив в множество этих неделимых блоков, конечно же число 1. В этом смысле все остальные числа – композитные материалы и не более того, так как они не просто потенциально «могут быть» составлены из простых чисел, а именно буквально «уже состоят» из них.
Конечно, арифметическим операциям чуждо понятие истории и никакое композитное число «не помнит», кем были «его предки». Если же гипотетически и ради примера несколько трансформировать подобный подход, то выяснилось бы, что действительно – все остальные числа «слеплены» непосредственно из чисел простых. А вот они сами – сами простые числа – если не редуцировать их к унарной системе счисления и буквальному «происхождению» только от 1, то почему они являются такими и какова их природа?
Ранее мы показали на примере числа 11, которое является простым, что оно не делится ни на одно число из ряда предшествующих ему чисел, исключая 1. Это не показалось нам удивительным по причине того, что 11 – не особо большое число и, соответственно, нет ничего удивительного в том, что оно не делится на 9 каких-то меньших чисел. А вот если взять простые числа побольше, например – 11113. Здесь необходимо немного «всмотреться» в суть ситуации – во всём множестве чисел, меньших нами предложенного, нет ни одно числа, на которое мы могли бы разделить без остатка число 11113 – то есть оно не делится на 11111 чисел меньших, чем оно (всё так же исключая 1). Это уже представляется немного интересным. Не в плане «эксперимента с интересными числами», который тоже, так сказать интересен, а в том смысле, что во всём действительно немаленьком множестве чисел, меньших указанного нами, нет такого числа, на которое можно разделить 11113 без остатка. Если это всё ещё недостаточно показательно, то посмотрим на число, например, 1000000000039 – один триллион тридцать девять. И оно тоже не делится ни на одно число меньше, чем оно само (за понятным исключением 1). Наибольшим же простым числом, известным на данный момент, является 2 ^ 136279841 -1 – число, в котором содержится 41024320 цифр, при записи в десятичной системе счисления. И да – оно не делится ни на одно число, меньше себя (минус 1).
В общем, мы ведём к тому, что подобные моменты представляются очень странными. Можно понять, например, что число 3 является простым – на что ему там «до себя» особо-то делиться; числа 5 или 11 – тоже особых вопросов не вызывают, как и многие иные не особо огромные числа. Но вот, если осмыслить тот факт, что во всём множестве, множестве такого размера, который и представить трудно, не найдётся ни одного числа, которое может без остатка разделить какое-либо другое число – складывается некоторое подозрение о том, что это «неспроста» (извиняемся за тафтологичность момента). А если добавить ещё и тот факт, что все остальные числа «складываются» из простых и единицы, то, собственно, подозрение только усилиться.
Попробуем подвести некоторый итог. В контексте нашей теории рваного континуума мы можем проинтерпретировать вышеприведённый факт одним единственным образом – простые числа должны будут определять некие наиболее «критичные» локации сферы – те участки, деформация которых может приводить к формированию наиболее существенных «изгибов», «скручиваний» и так далее. Поясним, что значит «наиболее существенных». Если мы просто визуально представим себе, как мы физически изгибаем нечто «композитное» (то есть нечто составленное из нескольких частей: склеенная по торцам палка – как метафорический пример), то мы, при применении должного усилия, можем просто «разорвать» или «разломать» на составляющие части то, что изгибаем – то есть у нас получится более одной части «изгибаемого» объекта и некое пустое пространство между этими частями. А всё потому, что «есть, где рваться». А вот если мы проделаем это же самое, но с чем-то цельным – просто с палкой, например, то итог будет иным. То есть мы сможем сформировать гораздо больший угол сгиба, при этом не нарушая целостность объекта. Понятно, что в реальном мире объекты из композитных материалов зачастую могут быть куда более гибкими, чем объекты из какого-то одного материала. Но в контексте формирования нашей модели те локации, которые могут быть описаны составными числами, подразумеваются нами «палками, склеенными по торцам»; а те, соответственно, которые могут быть взаимно-однозначно сопоставлены с простыми числами – едиными и цельными фрагментами caseum.
Поэтому нам и представляется целесообразным, чтобы при реальном моделировании эта специфика была учтена. То есть, чтобы при попытке «сильно» изогнуть сферу в области того участка, который описывается при помощи составного числа – эта локация имела функциональную возможность как бы «распадаться» на составляющие и формировать определённое количество разрывов – воронок в нашем случае. А при изгибе «в области простых чисел» – сфера оставалась цельной. В сущности, тот процесс, воспроизведение которого в контексте моделирования мы описали, представляет собой не что иное, как топологически отображённую факторизацию – разложение целых чисел на простые множители. И в этом смысле также полностью сохраняется параллельность между математическими операциями с числами и топологическим представлением этих же операций – как и происходит в контексте всей нашей модели рваного континуума.

6.6. ХАОС И ПОРЯДОК КАК ФИЛОСОФСКИЕ ПОНЯТИЯ И ИХ ИНТЕРПРЕТАЦИЯ В КОНТЕКСТЕ МОДЕЛИ «РВАНОГО» КОНТИНУУМА

В связи с наличием в контексте нашей модели такого феномена, как N5 – условного пятого измерения, которое мы определили, как «тотальный процессор» и «вычислительный хаос», считаем целесообразным немного раскрыть специфику наших представлений о хаосе и порядке, так что немного поясним касаемо значимости хаоса и его антиподов. В общем смысле под любым эволюционным процессом для некоторого вида в качестве составляющего элемента лежит процесс обучения одной, отдельно взятой особи. Глубокая связь между обучением и эволюцией давно стала предметом пристального внимания в научно-исследовательской деятельности междисциплинарного толка [1, 2]. Но в общем ситуация примерно такова: обучение отдельно взятой особи в ходе осуществления ею процесса онтогенеза затем, «силами» многих поколений, переходит в «развёртывание» процесса филогенеза всего вида. Те поведенческие в самом общем смысле слова паттерны, которые оказались наиболее удачными – сохраняются и продолжают «жить» в потомках в последующих поколениях; неудачные, соответственно – отмирают за ненадобностью. В этом смысле моделирование эволюции на основе обучения не составляет особого труда. Здесь мы говорим «вид» и «особь» также в абстрагированном смысле – как единиц функционирования глобальных процессов и как объектов, ответственных за «развёртывание» динамики в контексте воспроизведения всей нашей модели caseum – нам представляются, что будет целесообразным выбор именно такого «способа» детализации моделирования.
В сущности, под хаосом обычно понимается такой способ «развёртывания» последовательности состояний некоторой системы, который мы не способны с необходимой точностью спрогнозировать. Нелинейная динамика – мы не можем точно сказать какое состояние будет гарантированно получено при некоторых конкретных стартовых условиях; мы также не можем «откатить» последовательность состояний к уровню стартовых условий, основываясь на тех данных о состоянии системы, которые получены по итогу. Обычно проводится демаркация на системы динамического хаоса и стохастические системы. Под первыми подразумеваются системы, обладающие высокой чувствительностью к начальным условиям и воплощающие степень изменения способа реализации поведенческих паттернов, не соразмерную степени изменения начальных условий. То есть изменение всего лишь одного из базовых условий и всего лишь на одно условное деление – приводит к совершенно иной картине «развёртывания» в целом. Также обычно нет никакой возможности однозначно реверсировать последовательность состояний и на основе конечной картины выйти к изначальной. Под стохастическими же системами обычно подразумеваются системы хаотические в «классическом» понимании этого слова – то есть такие, динамика которых может быть разной даже при одинаковых начальных условиях. В этом смысле не вполне понятно почему так происходит и этот фактор существенно тормозит всё исследования систем подобного рода. В случаях со стохастическими системами не остаётся иного логического вывода, кроме такого, который тем или иным образом свидетельствует о том, что всё-таки наш диапазон отображения стартовых условий системы не покрывает весь «реальный» и «истинный» диапазон условий системы. Для случая же систем динамического хаоса можно предположить сразу два вероятных варианта: либо не учитывается какая-то операция преобразования в процессе «развёртывания» последовательности системных состояний, либо не учитывается формирование новой переменной в процессе этого самого «развёртывания».
Моделирование поведения подобных систем, в отличие от построения нашей текущей модели рваного континуума, сложным не представляется и постоянно используется при исследовании. Однако, даже в том случае, если внутренняя динамика системы отличается полной однозначностью, мы далеко не всегда способны на осуществление одновременно двух диагностических процессов: прогнозирование будущего состояния системы на основе текущего и, второй момент – восстановление изначального состояния системы на основе, соответственно, актуального. То есть для линейных систем и один и другой диагностический процесс осуществимы, а для систем с нелинейной динамикой – уже нет. Причём, зачастую мы не можем чётко идентифицировать заранее, основываясь на описании системы – линейная она или же нет. И даже те системы, которые реализуют очень простые алгоритмы «развёртывания» могут на поверку оказаться нелинейными и способными к формированию весьма нетривиальных поведенческих паттернов. То есть, внутренние системные правила перехода от текущего состояния системы к последующему могут быть линейными и однозначными, а вот реализуемое поведение – уже нет. И соответственно осуществить на системе подобного рода оба вышеупомянутых диагностических процесса – не представляется возможным. Почему так происходит и как же нам идентифицировать систему как линейную или же нелинейную? Причём нам бы хотелось, чтобы мы могли это сделать, основываясь всего лишь на описании алгоритма «развёртывания» системы и в общем, а не сугубо аналитическом, смысле слова «пределах» возможностей системы – то есть совокупности её возможных состояний. И если мы можем реализовать одновременно обоюдостороннюю диагностику, то мы можем однозначно утверждать общую вычислимость системы.
Собственно, для примера того, что здесь имеется ввиду, мы проведём небольшую демонстрацию. Ранее мы уже говорили об алгоритмах и конечных автоматах, теперь же покажем один из примеров – автомат клеточный. Построим конкретную модель. Причём наша модель конструируется в соответствии с принципом «нормального алгорифма Маркова» и будет, соответственно, одномерной [3].
•	Используемый алфавит: «XUY»
•	Правила перехода: последующее состояние каждой ячейки определяется её состоянием и состояниями двух её соседей. В рамках необходимости унификации применения правил для всех ячеек автомат подразумевается циклическим, то есть для последней ячейки правой по отношению к ней будет считаться первая, а для первой ячейки левой по отношению к ней будет считаться последняя. Всего будет 27 возможных состояний, каждое из которых обуславливает следующее состояние конкретной ячейки. Например, «UUU» -> «Y», «YYX» -> «U», «UXY» -> «X» и так далее – конкретные правила могут быть выбраны случайным образом. Случайность здесь подразумевается желательной просто для актуальных нужд в контексте демонстрации.
•	Фиксированный размер: 7 ячеек
Уже на данном этапе мы можем сформировать множество всех возможных состояний нашего автомата. И его размер будет равен 2187. Мы упорядочим это множество, преобразовав его в отсортированный в соответствии с алфавитным порядком список, который затем трансформируем в хеш-таблицу в виде словаря, в котором ключами будут состояния автомата, упорядоченные по алфавиту, а значениями – натуральные числа от 1 до 2187 включительно.
Далее ключевой момент. Соответственно, мы предварительно можем поставить во взаимно-однозначное соответствие нашему множеству состояний системы множество натуральных чисел от 1 до 2187, включительно. Каждое значение будет прямым отражением соответствующей позиции состояния автомата в упорядоченном по алфавиту списке.
Затем мы реализуем алгоритм осуществления перехода в виде функции. Функция будет принимать в качестве входных данных начальное состояние автомата и целое число N – количество итераций, которые необходимо осуществить. На каждой итерации алгоритм будет обращаться к нашему словарю с состояниями и их числовым отражением, и сохранять в список то число, которому соответствует текущее состояние автомата. Этот самый список, вместе с преобразованным автоматом, функция и будет возвращать. По итогу мы получаем новое состояние автомата и целочисленную последовательность, длина которой определяется заданным количеством итераций, и которая являет собой отражение динамики состояний системы – нашего автомата. 
Приводить здесь полную программную реализацию мы считаем излишним по причине достаточно большого размера используемых структур данных. Поэтому продемонстрируем непосредственно результаты работы программы. Допустим, при N равным 20.
•	Для строки «YYXYXYY» мы получим выходную строку «YYYXYXY» и целочисленную последовательность переходов – 2097, 244, 1296, 434, 546, 2168, 19, 2157, 82, 1890, 874, 1640, 1452, 7, 2177, 28, 2088, 292, 1276, 1942, 3.
•	Для строки «UYUXUYX» получим выходную строку «XUYYYXU» и последовательность – 521, 170, 512, 65, 718, 1795, 1699, 121, 1906, 1786, 1630, 57, 655, 1801, 1623, 755, 1684, 93, 298, 738, 967.
•	Для «UUYYUXU» соответственно «UUUXYUU» и – 220, 254, 136, 658, 760, 406, 1972, 92, 1216, 1542, 274, 1460, 252, 820, 6, 754, 272, 16, 74, 814, 46.
Здесь же скажем, что при оговоренных выше условиях (в среднем при случайном распределении значений ячеек для каждого из 27 состояний клетки и её соседей) для нашего автомата максимальная длина целочисленной последовательности без зацикливания равна 55 (то есть при N = 55), а после этого (при N > 55) последовательность зацикливается уже при любых начальных состояниях. Минимальная же длина последовательности без зацикливания равна 2 (третье значение последовательности уже было бы либо первым, либо вторым). Однако до зацикливания в приведённых нами последовательностях довольно сложно уловить хоть какую-то «логику». Можно было бы сказать, что они «Колмогоровски сложны» (так и есть в действительности), но для нас пока это не существенно [4]. А что существенно, так это то, что ни одна из наших последовательностей не образует возрастающую или убывающую подпоследовательность для множества всех возможных состояний системы, которое, как мы помним, упорядоченно по алфавиту.
Казалось бы, а зачем нам вообще это нужно? Ведь мы и так знаем правила перехода и способ изначальной сортировки, и наши последовательности, хоть и не упорядочены по соответствующим их алфавитному порядку целочисленным идентификаторам, упорядочены в ином – логическом порядке.
Да, так и есть, но предположим, что мы не знаем правил перехода и не знаем деталей изначальной упорядоченности. То есть, мы несколько аппроксимируем ситуацию с изначально неизвестной системой. Нам бы хотелось видеть некоторую упорядоченность в последовательности переходов, и в нашем случае (с уже хорошо знакомой системой) мы её априори (до запуска преобразований) знаем, но она никак не отражается в последовательностях. Поэтому последовательности и выглядят наборами случайных чисел.
Здесь мы приведём второй пример. 
•	Используемый алфавит: «АВС»
•	Правила перехода: каждое последующее состояние ячейки зависит только от её собственного текущего состояния, которое определяется как индекс её значения в алфавите, и на каждой итерации смещается вперёд на 1. Состояние «С» переходит в состояние «А» и так далее.
•	Фиксированный размер: 4 ячейки
По остальным характеристикам работа с этим автоматом не будет отличаться от предыдущего примера, кроме значения N – количества итераций. Размер множества всех возможных состояний системы для этого случая – 80 и оно, как и в предыдущем примере, упорядочено в соответствии с алфавитом. Итак, при N = 10.
•	Для состояния «СВСА» выходной строкой будет «АСАВ», а последовательность – 70, 20, 33, 70, 20, 33, 70, 20, 33, 70, 20.
•	Для «ААСС» выход, соответственно, «ВВАА» и последовательность – 9, 37, 77, 9, 37, 77, 9, 37, 77, 9, 37.
•	Для «СВВА» выход «АССВ» и последовательность – 67, 26, 30, 67, 26, 30, 67, 26, 30, 67, 26.
И так далее.
Очевидно, что здесь уже структура момента совсем иная. Если даже просто присмотреться невооружённым взглядом, то можно заметить, что каждая последовательность переходов, при редуцировании её к циклу из 3-х итераций, будет являться подпоследовательностью упорядоченного множества всех возможных состояний системы.
Для пущей убедительности и дополнительной наглядности приведём третий, ещё более упрощённый пример, который, несмотря на тривиальность, являет собой частный случай «Шифра Цезаря» [5].
•	Используемый алфавит: ABCDEFGHIJKLMNOPQRSTUVWXYZ
•	Правила перехода: каждое последующее состояние ячейки определяется её текущим состоянием, которое определяется как индекс её значения в алфавите и на каждой итерации смещается вперёд на 1
•	Фиксированный размер: 1 ячейка
В данном случае SS идентично алфавиту. При N = 10 получим:
•	Для состояния «А» получим выходное состояние «К» и последовательность – 1, 2, 3, 4, 5, 6, 7, 8, 9, 10.
•	Для состояния «Z» получим выходное состояние «J» и последовательность – 26, 1, 2, 3, 4, 5, 6, 7, 8, 9.
Можно было бы продолжать, но уже очевидно, что в данном случае все последовательности системных переходов являются подпоследовательностями изначального упорядоченного множества всех возможных состояний системы – SS. То есть теперь уже ситуация значительно отличается от нашей изначальной. Почему же так происходит?
А произошло так потому, что логика упорядочивания множества всех возможных состояний системы и логика перехода от одного состояния системы к другому являются эквивалентными с логико-математической точки зрения во втором и третьем случаях, и различаются – в первом. Иными словами, во втором и третьем случаях внутренний алгоритм «развёртывания» системы эквивалентен алгоритму упорядочивания множества всех возможных состояний системы, а в первом – нет.
Из вышесказанного мы можем вывести некоторый принцип биективного соответствия для линейных систем, который отражает способность вычисления динамики состояний: система линейна, а следовательно – вычислима только в том случае, если существует некоторое множество натуральных чисел на диапазоне [1, N], которое может быть поставлено во взаимно-однозначное соответствие некоторому конечному упорядоченному подмножеству размера N множества всех возможных состояний системы, такому, для которого любая последовательность системных переходов будет составлять подпоследовательность.
Зачем нам вообще вводить эквивалентность внутреннего алгоритма «развёртывания» системы и алгоритма упорядочивания множества всех возможных состояний системы? Потому что в том случае, если первый эквивалентен второму, то любая последовательность системных переходов будет составлять подпоследовательность множества состояний системы, а значит – любая последовательность системных переходов может быть описана полиномиально и, как следствие – мы сможем прогнозировать, эмулировать и воспроизводить системную динамику. Высокопарно выражаясь, нахождение алгоритма упорядочивания для любого алгоритма «развёртывания», означало бы наличие возможности «отсортировать сам хаос» и выстроить его в «Колмогоровски простую» последовательность.
Теперь же осталось выяснить, существует ли алгоритм упорядочивания для систем, реализующих динамику иного типа, наподобие нашего первого примера, игры «Жизнь» Конвея и тому подобных систем [6].
Предположим, что он существует, и попробуем задать его для нашего первого примера. Единственным возможным логическим критерием в данном случае может служить первичность появления состояния автомата в генерируемых последовательностях состояний: то есть, если «А» «производит» «В», то «А» должно стоять ближе к началу упорядоченного списка состояний, чем, соответственно, «В». Сразу же зададим вопрос: может ли одно и то же конечное состояние автомата быть получено при различающихся начальных состояниях, то есть разными путями? Экспериментально показано, что может (не только в нашем примере, но и в случае с любым клеточным автоматом, отличным от наиболее простых, то есть линейных версий). В частности, для игры «Жизнь» также показано, что для каждого текущего состояния наличествует только одно последующее, но может существовать несколько предыдущих [6]. А значит, мы не можем поставить в соответствие двум или более различным состояниям автомата одно натуральное число, следовательно, и соответствующего множества натуральных чисел также не существует. То есть мы пришли к противоречию. Соответственно, однозначного определителя для подобных систем не существует и некоторые (возможно все, но в любом случае 1 или более) последовательности смены состояний в этих системах не будут являться подпоследовательностями упорядоченного множества всех возможных состояний системы, так как невозможно имплементировать алгоритм упорядочивания, однозначно эквивалентный алгоритму «развёртывания». Конечно, возможны вероятностные способы формирования некоего порядка, но это уже область эвристики.
На основе представленного нами выше логическим путём выводится определение полностью линейной системы. Полностью линейная система – это система, для которой существует алгоритм упорядочивания любого подмножества множества всех возможных состояний системы таким образом, чтобы любая последовательность системных переходов составляла подпоследовательность этого подмножества. Иными словами, линейная система – это система, для которой существует некоторое подмножество состояний, которое может быть взаимно-однозначно отображено на равномощное упорядоченное по возрастанию множество натуральных чисел таким образом, чтобы любая последовательность системных переходов, также отображённая на это множество натуральных чисел, составляла возрастающую подпоследовательность этого подмножества. Соответственно, системы, для которых не существует подобных алгоритмов упорядочивания и подобных подмножеств – не являются полностью линейными и, как следствие, однозначно вычислимыми. К примеру, уже неоднократно упомянутая игра «Жизнь» Конвея – при полностью линейных правилах перехода и при полной возможности на основе актуального состояния системы вычислить последующую динамику состояний, пусть даже и только лишь методом полного перебора – полностью линейной не является, так как мы не можем однозначно провести ретроспективное «восстановление» последовательности системных переходов. Но не только из-за этого, а ещё также и потому, что в случае с данным клеточным автоматом подразумевается возможность перехода от некоторого текущего состояния к состоянию, которое уже наличествовало в последовательности ранее, но при этом мы не можем, как было в двух наших последних примерах, свести всю последовательность к циклу, так как сама последовательность, как мы уже указывали – является «Колмогоровски сложной». Если мы можем свести всю последовательность к циклу, то мы также можем описать её полиномиально, а значит – система является полностью линейной, в соответствии с нашим подходом. Если же, напротив – возвраты к предыдущим состояниям от состояний текущих, имеют место быть, но вся последовательность переходов потенциально не сводима к циклу – система не будет полностью линейной даже при наличии линейного алгоритма «развёртывания» – то есть осуществления переходов от текущего состояния системы к последующему.
Собственно, примерно таким образом и функционирует нелинейная динамика под маской внешней видимой линейности. Однако, учитывая специфику нашей работы, легко можно предположить, что нельзя однозначно трактовать нелинейность – как нечто «плохое» и «негативное». Является интуитивно понятным, что линейная динамика формируется на основе изначальной динамики нелинейной. И в этом-то и состоит основа взаимодействия хаоса и упорядоченности – базис их нерасторжимой «взаимообусловленности».
Одним из наиболее общих способов определения жизни можно назвать следующий: жизнь – это способность к изменению и сохранению информации. Логическая коньюнкция указывает на однозначную важность обоих критериев и на то, что ни одним из них нельзя пренебрегать. То есть, жизнь – это не изменение информации, так как изменение информации в самом общем смысле – это хаос, но также жизнь – это не сохранение информации, так как сохранение информации в самом общем смысле – это нерушимый и неизменный порядок. Жизнь – это и изменение, и сохранение информации и никак иначе – и хаос и порядок одновременно. На основе вышесказанного можно сделать вывод о том, что хаос – это перебор возможных «вариантов» информации, а порядок – это сохранение тех из них, которые лучше всего способны удовлетворить вычислительные потребности объекта. Таким образом, повторим ещё раз, жизнь – это не хаос и не порядок, но и хаос, и порядок только вместе взятые. Не будь хаоса – жизнь застынет и не сможет адаптироваться, не будь порядка – жизнь не сможет сохранить полезные способы адаптации. В этом смысле хаос и порядок – две стороны одной монеты, примерно, как «ничто» и «нечто», как 0 и 1, как рациональные и иррациональные числа – основа самой возможности существования вообще, как единства и борьбы противоположностей. И этот самый рекурсивный процесс перехода между этими противоположностями был нами определён в качестве процессуального феномена джиминации – рекурсии некоего объекта с изменением хотя бы одного свойства этого объекта на каждом рекурсивном шаге и именно вследствие этого самого шага.
Касаемо же нашего «вычислительного хаоса», который мы предлагаем для построения нашей модели на уровне N5, полагаем правомерным считать его непосредственно хаосом ровно до завершения процесса «перевычисления» «утёкшей в него» информации. Информация же эта, как мы помним, в соответствии с нашей моделью должна быть представлена алгебраическими иррациональными числами, а после этого – на какое-то мгновение – сам хаос уже представляет собой «некий намёк» на определённость – «вытекающие» трансцендентные иррациональные числа, если бы они могли реально существовать, конечно. Звучит несколько сложно и довольно запутанно, но примерно такой нам и представляется динамика нашей модели на данном концептуальном её участке. Специфика реальных вычислений в рамках N5 сложна тем, что она, предположительно, должна будет представлять собой «переучёт всего глобального контекста» для каждой локальной ситуации, то есть – воронки.

ВЫВОДЫ ПО ГЛАВЕ

Мы предложили модель рваного континуума, обосновали её через связь иррациональных чисел с разрывами, которые мы обозначили как воронки в топологическом отображении континуума и построили теоретическую модель на этой основе. Также, в контексте рассмотрения вопроса о природе хаоса и порядка и в ходе рассмотрения того, что мы охарактеризовали как «вычислительный хаос», нами был предложен и раскрыт принцип биективного (взаимно-однозначного) соответствия относительно линейных систем и понятие полностью линейной системы, как «вычисляемой системы».
Фундаментальной мыслью и отправной точкой этой части работы было наше предыдущее заключение о том, что численная аппроксимация континуума не является с ним взаимно-однозначной, а является иньективной ему. Далее мы пошли от противного и предположили, что это на самом деле не так, и что численная аппроксимация континуума в действительности ему биективна, но в особом – не в классическом смысле. Это всё равно означало, что численная аппроксимация не способна точно отобразить некоторые локации континуума, а напротив – было предположено, что именно в них она бесконечно «развёртывается». Далее мы построили аналогию и предположили, что ровно те же самые «локации», которые мы не можем конструктивно определить в численном виде, вместо этого бесконечно развёртывая в их направлении некий вычислительный процесс – также могут быть конструктивно не определёнными и в топологическом смысле. Следующим шагом мы приняли моделирование трёхмерной сферы с «проколами» её в тех точках, которые являются взаимно-однозначными с соответствующими им иррациональными числами. Эти «проколы» мы обозначили как «воронки» и предположили, что эти самые воронки, в контексте нашей модели, ведут непосредственно в пятое измерение, которое мы, в свою очередь, обозначили как N5. Функциональным значением этого измерения было признано «перевычисление», поступающей информации. То есть, по сути, N5 было определено в качестве некоего «тотального вычислителя» для всей модели в целом.
Далее нами была охарактеризована специфика реализации на субстрате предлагаемой модели таких феноменов как: сильное и слабое ядерные взаимодействия, электромагнетизм, гравитация, чёрные дыры, излучение Хокинга, тоннельный эффект, тёмная энергия, тёмная материя и прочие. Время нами было определено, как функция скорости вычислений. Мы показали каким образом на интерпретационном базисе модели можно описывать даже такие феномены как сознание, жизнь и смерть. Мы показали, каким образом введение «воронок» и N5 может повлиять на расчёты в контексте квантовых вероятностей и предположили целесообразность подобного рассмотрения всего спектра аспектов микромира.
В контексте исследования генеалогических аспектов нашей модели мы апеллировали к предложенному нами ранее феномену джиминации и показали, как на функциональной рекурсивной основе за счёт самореферентного отрицания из «ничто» формируется «нечто», а вместе с их неразрывным взаимодействием – сущее вообще в виде процессуального, потенциально бесконечного феномена.
Далее, в ходе уточнения специфики «воронок» в континууме, нами были рассмотрены различия между алгебраическими и трансцендентными иррациональными числами. В качестве несколько отдалённой аналогии были рассмотрены соотношения между множествами трансцендентных и алгебраических иррациональных чисел с одной стороны и тёмной материей с тёмной энергией – с другой стороны. В общем смысле нам представился целесообразным вывод о том, что в рамках нашей модели алгебраические иррациональные числа должны будут представлять результат наших попыток «запуска» потенциальной бесконечности в виде «стремления» вычислить те или иные иррациональные значения и именно они и являются «утекающими» в «воронки»; числа же трансцендентные можно определить, как «перевычисление» – то, что «вытекает» из «воронок» и вносит полный «разлад» в симметрию и непротиворечивость. В этом смысле представляется, что как будто числа алгебраические генерируем «мы сами», а вот числа трансцендентные нам уже именно «предоставляются». Здесь же мы ввели демаркацию между вычислением и измерением.
В продолжение предыдущего пункта мы также отдельно охарактеризовали комплексные и простые числа. Было показано, что числа вещественные представляют собой «последние» числа, которые можно назвать «аутентичными» числами. Все остальные – комплексные, кватернионы и так далее составлены из вещественных чисел и только вещественные числа являют собой «последнее уникальное расширение» численной системы. Далее было уточнено, что в общем-то в контексте построения нашей модели на основе теории рваного континуума, комплексные числа именно здесь и сейчас не играют особой роли. В сущности, роль играют не числа вообще, а то, что именно мы вкладываем в их топологический смысл и его физическую реализацию в контексте построения нашей модели. А вот о простых числах мы вывели, что они (в рамках нашей модели) представляют собой наиболее «гибкие» локации для осуществления деформации сферы и позволяют реализовать эти самые деформации достаточно разнообразными способами. В сравнении с ними, как мы показали далее, при тех же самых условиях существенных деформаций сферы, те числа, которые являются не простыми, а композитными, будут претерпевать топологическое отображение процесса факторизации – то есть распад на простые множители. По крайней мере так предполагается в контексте нашей модели.
В заключительной части мы рассмотрели понятие хаоса и упорядоченности. В рамках данной части нами был предложен принцип биективного соответствия для линейных систем, который отражает способность вычисления динамики состояний: система линейна, а следовательно – вычислима только в том случае, если существует равномощное множество натуральных чисел, которое может быть поставлено во взаимно-однозначное соответствие некоторому конечному упорядоченному подмножеству множества всех возможных состояний системы, такому, для которого любая последовательность системных переходов будет составлять подпоследовательность. Также нами было предложено определение полностью линейной системы, как системы, для которой существует алгоритм упорядочивания любого подмножества множества всех возможных состояний системы таким образом, чтобы любая последовательность системных переходов составляла подпоследовательность этого подмножества. Также в этой части мы вывели определение феномена жизни на основе синтеза хаоса и порядка: жизнь – это способность к сохранению и изменению информации, где за хаос «отвечает» изменение, а за порядок – сохранение. Процесс же «балансирования» между тем и другим был нами охарактеризован как непосредственно рекурсивный и далее определён в качестве процессуального феномена джиминации – рекурсии некоего объекта с изменением хотя бы одного свойства этого объекта на каждом рекурсивном шаге и именно вследствие этого самого шага.
Резюмируем. Возможно что-то из сказанного выше о специфике построения нашей модели рваного континуума, при попытке смоделировать некие реальные феномены, окажется не вполне соответствующим действительному положению дел – мы это, более того, даже предполагаем. Но, если хотя бы малая часть предложенного, возможно сама ключевая мысль о инвертировании подхода к соотношению геометрических фигур и представляющих их чисел; или мысль о «недифференцированной» природе некоторых участков континуума; или о процессах «утекания»/«вытекания» и идея воронок; или же наши предположения о фундаментальной природе иррациональных и простых чисел и их значении для отображения «ткани мироздания»; или же о вычисляющем «тотальном процессоре» и его значении для вероятностного характера квантовых эффектов; а может быть определённый и охарактеризованный нами процессуальный феномен джиминации или иные высказанные соображения – если хотя бы что-то из этого поможет иначе взглянуть на окружающий нас мир и сделать на основе этого некие конструктивные выводы – значит всё остальное также имело свой смысл (ведь на данный момент мы не можем однозначно сказать, что имеет смысл, а что нет). К тому же, в контексте данного раздела мы, не раз на это указывая, не пытались ничего однозначно утверждать, за исключением наших соображений касаемо принципа биективного соответствия и специфики определения полностью линейных систем, а предложили теоретическую модель, которая, как нам представлялось, логически следовала из предыдущих выводов и заключений.
Сами же мы просто прошли «тропами mundus» и над магистральной из них «висел» знак, свидетельствующий о взаимно-однозначном отображении множества всех вещественных чисел на соответствующее ему топологическое пространство; над второй же по значимости было установлено соответствие между упорядоченным множеством всех возможных состояний системы и равномощным множеством натуральных чисел; остальное уже – «миражи близ дороги».
 
ЗАКЛЮЧЕНИЕ

В данной работе мы исследовали сущность математики – в общем, и некоторые её фундаментальные аспекты – основания, по отдельности. Этими фундаментальными аспектами в нашем случае стали теория множеств, теорема Гёделя о неполноте и проблема формализации алгоритма.
Изначально нами был поставлен вопрос о природе и генезисе математических объектов и о том, чем же всё-таки является и какими конкретно проблемами занимается математика. Ответы на эти вопросы обычно представляются интуитивно понятными, но при попытке сформулировать более или менее корректные определения, даже великие учёные затрудняются однозначно что-то утверждать. И это, как мы показали, на самом деле так – вопрос о том, «что же такое математика» по праву входит в перечень наиболее сложных, наряду с вопросами о природе времени, определением жизни и прочими. Конкретизация данного вопроса в направлении того, «познаются» ли, или же всё-таки «изобретаются» математические объекты – внесла свою лепту в общий контекст проблематики, а попытки разобраться с тем, что же из себя представляет этот своеобразный «мир математики», в котором и «локализованы» эти самые математические объекты – сформировали опорную точку.
Итак, математический объект был нами определён, как наименее сильная и наиболее абстрактная связь между отдельными феноменами реальности. Мы предоставили обоснование этого определения на основе понятия числа. Мы показали, что, например, число 1 представляет собой аппроксиматор всего того во Вселенной, что представлено единственным экземпляром в контексте хотя бы одной актуальной «позиции наблюдателя»; один – это сама идея единичности, единственности, уникальности и первоначала. И это только один пример и всего лишь с одним числом, но на самом деле именно таким образом нам представляется конструктивное определение любого математического объекта. На основе этого мы заключили, что математические объекты представляют собой абстрактные связи между некоторыми совокупностями феноменов.
Соответственно, сама математика далее была нами определена, как наука об абстрактных связях между объектами и операциях с ними. Нам представилось такое определение наиболее целесообразным и более иных подходящим по существу, так как оно аккумулирует в себе именно то, чем занимается математика: это абстрактнее, чем специфика взаимодействия с предметом и объектом даже для теоретической физики; это более конкретно и формально, чем работа с абстрактными объектами, которая свойственна философии и так далее – в общем определения полагается нами достаточно корректным.
Тот же самый «математический мир», о котором было много сказано уже задолго до нас, мы определили, как когнитивное отражение совокупности объективированных абстрактных связей. Это также полагается нами достаточно качественным определением, особенно в контексте необходимости ответа на вопрос о генезисе математических объектов. Этот самый генезис был нами определён с той точки зрения, что «реальность предоставляет субстрат для осмысления», а «когнитивная деятельность формирует объект». То есть мы пришли к выводу о том, что математические объекты и сам «мир математики» вообще «существует» в области взаимодействия «реального» и «идеального» – объективной действительности и когнитивной деятельности. В общем смысле данный вывод мог бы показаться даже несколько тривиальным, если бы не один нюанс, касаемо «абсолютного существования» математических объектов, по крайне мере некоторых из них – наиболее «реальных» (натуральные числа, например). Мы показали на примере гипотетической формы жизни с отсутствием категории количественного вообще в когнитивной деятельности представителей этой самой жизни, что в «их Вселенной» действительно «не будет существовать» чисел в том виде, в котором они существуют для нас. Но вот именно потенциально они могут существовать и для них тоже, так как сама возможность существования подобных математических абстракций подпадает под юрисдикцию «предела возможного» для нашей реальности. И в этом смысле числа действительно «абсолютно» существуют для всех в нашей Вселенной. «Мир математики», в котором «обитают» все те математические объекты, о которых мы говорили (и о которых не упомянули), мы определили, как mundus – от латинского «мир».
Далее, нами была рассмотрена теория множеств – как одно из фундаментальных оснований математики, как концептуальный базис, на котором в своё время пытались построить целостное и нерушимое здание математической науки, но что привело в результате только к возникновению того, что после охарактеризовали, как «кризис оснований математики». Нами был рассмотрен генезис теории множеств от Древней Греции до непосредственно трудов отца-основателя – Георга Кантора. Нами были раскрыты нюансы «концептуальной ситуации» в математике на момент возникновения этой теории и было показано, что теория множеств стала для многих действительной надеждой на обретение твёрдой почвы под «теоретическими ногами», так как множество – феноменологически представляет собой более общий способ задания и установления связи между различными феноменами, чем число. Мы описали также и специфику сопротивления установлению теории множеств в качестве математической основы, в том числе привели мнение одного из наиболее непримиримых противников Кантора – Леопольда Кронекера, который утверждал, что «Бог сотворил целые числа, всё остальное – дело рук человека». В этом смысле нам показалась интересной динамика «божественного функционала» в плане «сотворения чисел» – ещё за пару веков до Кронекера такие великие умы, как Ньютон, Лейбниц и Паскаль, также не сомневаясь в том, что занятия математикой есть единственный верный путь познания «божественного откровения» – полагали, что Бог всё же ограничился сотворением чисел даже не целых, а сугубо натуральных.
Был охарактеризован закат «наивной теории множеств» с её преобразованием в аксиоматическую версию. Нами было рассмотрено начало данного процесса в виде возникновения так называемых парадоксов теории множеств, первый из которых обнаружил ещё сам Кантор, а затем и прочие учёные, имевшие отношение к этой сфере. Указано, что в момент обнаружения первого парадокса было положено начало кризису оснований математики, который был затем резюмирован в теореме Курта Гёделя о неполноте. Как парадоксы теории множеств, так и доказательство Гёделя были нами рассмотрены отдельно. На базисе исследования природы как парадоксов, так и доказательства мы установили, что вообще феномены подобного рода имеют единую «функциональную», процессуальную и феноменологическую основу – самореферентность.
Соответственно, самореферентность была далее рассмотрена уже в качестве отдельного объекта. Мы пришли к выводу о том, что самореференция представляет собой рекурсивный феномен, который в то же самое время, является нарушением «чистоты» рекурсии и одного из основных принципов как рекурсивного проектирования, так и рекурсивного вывода. А именно – изменение самого рекурсивного объекта на каждом рекурсивном шаге. Мы показали, как именно это происходит на примерах доказательства Гёделя теоремы о неполноте, коснулись этого при доказательстве теоремы Альфреда Тарского о «невыразимости истины в формальной системе средствами этой же системы», продемонстрировали как это работает при формировании парадоксов – от парадокса Рассела до апорий Зенона. Подобный тип самореферентности, ввиду широты его юрисдикции, был нами отдельно понятийно определён в качестве феномена джиминации – от латинского gemellas, что означает «двойняшки». Именно такое определение показалось нам корректным по той причине, что в ходе реализации феномена джиминации происходит нечто подобное «латентному удвоению», то есть возникновению на основе одного изначального объекта двух различных объектов со своими различающимися уникальными свойствами.
Процессуальный феномен джиминации был нами поставлен в основу качества противоречивости вообще – как в формальной логике и достаточно богатых функциональных системах, так и в контексте гораздо более широкого спектра явлений.
Также нами были рассмотрены попытки формализации понятия алгоритма, который были в основной своей массе, осуществлены как раз в те же самые годы, когда подходил к своему логическому завершению кризис оснований математики. В частности, нами были рассмотрены рекурсивные функции (К. Гёдель, Ж. Эрбран, С. Клини), λ-исчисление (лямбда-исчисление) А. Чёрча, разработанная Э. Постом «Машина Поста», «Машина Тьюринга» за авторством, соответственно, А. Тьюринга, а также комбинаторная логика М. Шейфинкеля и Х. Карри и нормальный алгорифм А. Маркова – НАМ. Мы показали, что на данный момент не существует общепризнанной строгой формализации понятия алгоритма, а наличествуют лишь вышеприведённые попытки её осуществления, которые, что весьма значимо, являются эквивалентными с логико-математической точки зрения. То есть, по сути, мы не знаем, что такое алгоритм вообще, а имеем некоторое представление только о его аппроксимациях. Причину подобного недочёта мы усмотрели в том, что имеющиеся формализации, во-первых, отличаются недостаточной фундаментальностью, а во-вторых – не используют философскую составляющую философского же понятия, коим и является непосредственно сам алгоритм. Мы предложили свой теоретический базис для возможной дальнейшей формализации алгоритма. Наше предложение и предположение сводится к тому, что алгоритм вообще представляет собой синтез директивы и способа упорядочивания информации, представленной некими «информационными блоками». И именно упорядочивания, а не просто обработки или переработки этой информации; и не просто директива или способ её выполнения, а именно синтез того и другого. И именно на этом способе, вообще безо всякой опоры на директиву и базировались формализации алгоритма, то есть они брали в расчёт только часть того, что «полагалось» описать и выразить формально – задание способа обработки данных и пошаговая реализация этого способа – собственно, это всё что в общем смысле полагается под понятием алгоритма. Мы же предложили расширить понятие алгоритма от пошагового способа обработки информации до синтеза директивы упорядочивания информации и способа осуществления этого самого упорядочивания.
Далее нами «были взяты» для ближайшего непосредственного рассмотрения некоторые ключевые аспекты теории множеств. Мы осуществляли это рассмотрение в не совсем классическом виде философско-методологического изложения, а в форме, которую мы обозначили, как «философская теорема». Нами были показаны многие неоднозначности в контексте теории множеств, в особенности же мы сосредоточились на специфике использования понятий актуальной и потенциальной бесконечностей, конструктивных способах построения континуума и доказательствах «реальности» вещественных чисел, основу для которых, в свою очередь, и составляет теория множеств. Ключевыми выводами этой части мы признали неправомерность использования понятия бесконечности в контексте доказательств и обоснований теории множеств, так как зачастую осуществляется некорректное смешивание и подмена понятий бесконечностей актуальной и потенциальной друг другом – просто для нужд актуального «участка» теории. В этом случае мы предложили заменить понятие бесконечности на понятие «индифферентности», а ещё конкретнее – на «индифферентную мощность». Ещё два наиболее значимых вывода касаются невозможности конструктивного численного построения множества всех вещественных чисел и отсутствие «реального существования» иррациональных чисел, вкупе с заменой их некими аппроксимациями, с заранее определённой степенью приближения.
На основе вышеприведённых выводов, мы заключили, что актуально бесконечное множество всех вещественных чисел не может являться взаимно-однозначным с соответствующей ему геометрической фигурой – то есть они не биективны. После мы попробовали пойти от противного и предположить, что если мы не можем строго численно определить некие точки на геометрической фигуре (квадратный корень из числа 2, длина окружности круга с диаметром 1 и так далее), то возможно, в некотором смысле, и геометрически эти самые точки не являются однозначно определёнными – по крайней мере это можно было предположить, что мы, собственно, и сделали.
На основе этого предположения мы пришли к тому, что из него следует далее путём логического вывода, а именно – к построению модели трёхмерной сферы. Те ключевые особенности и используемые нами принципы, которые были положены в основу формирования модели, были объединены в совокупность, которую мы синтетически определили в качестве «теории рваного континуума».
Само собой разумеется, что мы занимались только лишь теоретическим моделированием на основе наших предположений, в свою очередь базирующихся на ранее выведенных заключениях, центральным из которых являлись невозможность численного определения иррациональных чисел и, «играющее с ним в паре» взаимно-однозначное соответствие между множеством всех вещественных чисел и соответствующим этому множеству геометрическим «построением». И в качестве такого построения мы и предложили трёхмерную сферу, предположив вместе с этим, что те «локации» этой сферы, которые будут взаимно-однозначны иррациональным числам, не будут также однозначно определены и в пространственном смысле, а будут представлять собой некие «воронки» – разрывы в континууме. В связи с этим сама сфера была нами определена как caseum – от латинского «сыр», так как она по содержанию как раз-таки и должна напоминать нечто подобное, ввиду своей «пронизанности» огромным количеством воронок.
Эти самые воронки, как было предположено нами в ходе формирования теоретической модели, должны будут иметь очень малый размер и существовать на квантовом уровне. Физический смысл воронок мы предположили в «утекании» «в них» вычисляемой информации и обратном процессе – «вытекании» «из них». Далее нами предположено, что общее соотношение этих воронок по сравнению с «остальным материалом» сферы должно будет составлять примерно 95% - то есть воронки должны будут заполнять практически всё пространство на квантовом уровне. Те противоречия и контринтуитивные замечания, которые логичным образом у нас возникли при подобном соотношении «пустот и заполнений», были затем разрешены в ходе описания динамики нашей модели. Было предположено, что воронки в сфере направлены в то, что мы условно определили, как «пятое измерение» или N5 и показали, что N5, в контексте нашей модели представляет собой не что иное, как «тотальный вычислитель» для всей сферы, в известной мере «задающий» специфику каждой мельчайшей «локации» сферы. Нами было предположено, что квантовые вероятности также вполне могут определяться подобным вычислительным механизмом.
Последняя приведённая мысль была нами развита далее, и мы прописали гипотетическое отображение на основе нашей модели основных физических феноменов, таких как: сильное и слабое ядерные взаимодействия, электромагнетизм, гравитация, сущность и ход времени, излучение Хокинга и тоннельный эффект; а также мы коснулись специфики реализации в нашей модели принципа неопределённости Гейзенберга и принципа дополнительности Бора; также мы, просто для примера, при помощи концептуального аппарата нашей модели, дали определения таких феноменов как сознание, жизнь и смерть. Допустим, жизнь, в наиболее общем и абстрактном смысле, но сугубо для некоего отдельного субъекта, была нами определена, как необходимость вычисления собственного вычислительного аппарата; а сознание – как специфическая локальная деформация сферы с образованием до некоторой степени замкнутого пространства воронок, что позволяет несколько стабилизировать и упорядочить локальные вычислительные процессы. Ещё раз уточним, что это было сделано просто в качестве примера того, как можно использовать предлагаемую нами модель для осуществления интегрирования в единое целое разрозненных феноменов, явлений и принципов. В этом смысле наша модель и её основа – теория рваного континуума – оказались достаточно продуктивными, по крайней мере на первую поверку. И, как уже было подчёркнуто ранее, мы полагаем, что наша модель и наша гипотетическая теория, выведенные из предположения о биективности множества всех вещественных чисел и соответствующим этому множеству геометрическим отображением, способны дать весьма богатую почву для осуществления дальнейших размышлений и осмыслений как философам, так и специалистам дисциплинарных направлений научного познания.
В последних частях нашей работы мы исследовали проблематику комплексных и простых чисел, показав специфическую значимость каждого из этих численных классов для контекста формирования нашей модели. Здесь было показано, что вещественные числа представляют собой последние из «аутентичных» чисел, признав за последующими расширениями – комплексными числами, кватернионами, октинионами и прочими – роль композитных надстроек, состоящих, в свою очередь, непосредственно из чисел вещественных. Простые же числа были нами определены, как, в определённом смысле, единственные уникальные числа. Простые числа были затем связаны с нашей моделью и «установлены» в качестве наиболее критичных участков деформации сферы, которые позволяют формировать наиболее «сложные» изгибы и скручивания, при которых локации «обычных» чисел «рассыпаются» на составляющие, что соответствует алгебраическому процессу факторизации целых чисел.
Также мы, в связи с введением понятия «вычислительного хаоса», рассмотрели понятия хаоса и порядка, охарактеризовав глубокую взаимосвязь между ними. Нами было показано, что жизнь вообще – феноменологически и как свойство Вселенной – это не изменение информации, так как изменение информации в самом общем смысле – это хаос, но также жизнь – это не сохранение информации, так как сохранение информации в самом общем смысле – это нерушимый и неизменный порядок. Жизнь – это и изменение, и сохранение информации и никак иначе – и хаос и порядок одновременно. На основе этого мы сформировали вывод о том, что хаос – это перебор возможных «вариантов» информации, а порядок – это сохранение тех из них, которые лучше всего способны удовлетворить вычислительные потребности объекта. Также в данной части нами был предложен принцип биективного соответствия для линейных систем и понятие полностью линейной системы. Принцип определяется следующим образом: система линейна, а следовательно – вычислима только в том случае, если существует некоторое множество натуральных чисел на диапазоне [1, N], которое может быть поставлено во взаимно-однозначное соответствие некоторому конечному упорядоченному подмножеству размера N множества всех возможных состояний системы такого, для которого любая последовательность системных переходов будет составлять подпоследовательность. А полность линейную систему мы определили, как систему для которой существует алгоритм упорядочивания любого подмножества множества всех возможных состояний системы таким образом, чтобы любая последовательность системных переходов составляла подпоследовательность этого подмножества.
Резюмируя, считаем целесообразным сказать о том, что как предложенные нами выводы, так и сделанные нами же на основе этих выводов дальнейшие теоретические построения могут и, по нашему видению достаточно качественно могут, служить в качестве опорных точек для последующих исследований в области философии, математики и теоретической физики, а также в многогранной сфере междисциплинарных исследований. Повторим ранее высказанную мысль о том, что возможно наше соображение о:
•	инвертировании подхода к соотношению геометрических фигур и представляющих их чисел;
•	или мысль о «недифференцированной» природе некоторых участков континуума;
•	или о процессах «утекания» / «вытекания» вычисляемых данных и сама идея воронок;
•	или же наши предположения о фундаментальной природе иррациональных и простых чисел и их значении для отображения «ткани мироздания»;
•	или же о вычисляющем «тотальном процессоре» и его значении для вероятностного характера квантовых эффектов;
•	возможно наши выводы касаемо природы актуальной и потенциальной бесконечности и феноменов «ничто», пустого множества и числа 0 – как оснований для дальнейшего возникновения всего сущего;
•	а может быть определённый и охарактеризованный нами процессуальный феномен джиминации;
•	или принцип биективного соответствия для линейных систем и предложенное понятие полностью линейной системы;
Или же иные высказанные умозаключения – если некая часть этого поможет иначе взглянуть на окружающий нас мир и сделать на основе этого некие конструктивные выводы – значит наше исследование действительно имеет свой глубокий и конструктивный смысл, даже учитывая сугубо предположительный характер некоторых выводов и нашу общую «осторожность» в формулировках.
Напоследок добавим, что «В первой половине XIX в. физики и математики провели многочисленные исследования электричества и магнетизма. Им удалось получить небольшое число математических законов, описывающих различные электрические и магнитные явления. В 60-е годы XIX в. Джеймс Клерк Максвелл поставил перед собой задачу собрать все эти разрозненные законы и выяснить, насколько они совместимы. Максвелл обнаружил, что для математической совместимости необходимо ввести в уравнения еще один член. […] Так, из чисто математических соображений Максвелл предсказал существование огромного класса ранее не известных явлений и пришел к правильному выводу об электромагнитной природе света» [Мат. Утр опр.]. Собственно, хотелось бы верить, что и наша, фундаментальная для данной работы мысль о том, что иррациональные числа представляют собой потенциально бесконечные вычислительные процессы и геометрическая экстраполяция этой мысли на модель континуума, может быть также даст толчок каким-либо новым прорывным исследованиям и результатам.
 
Институт философии НАН Беларуси





СИЛЬНЫЙ ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ И ОБЪЕКТНО-ОРИЕНТИРОВАННОЕ ПРОГРАММИРОВАНИЕ: СИНТЕЗ ПАРАДИГМ

Монография


Скиба И.Р.























Минск 2025
Рецензенты:
Доктор философских наук, профессор, Лукашевич В.К.
Кандидат философских наук, доцент, Колесников А.В.
Кандидат физико-математических наук, доцент, Новыш Б.В.



Скиба И. Р. Сильный искусственный интеллект и объектно-ориентированное программирование: синтез парадигм / И. Р. Скиба – Минск: ИД «Белорусская наука», 2024. – 212 с.

В монографии излагается инновационная авторская концепция технотропного подхода к разработке систем сильного искусственного интеллекта – психо-машин. Концепция является альтернативой превалирующему на данный момент подходу к формированию интеллектуальных систем, который определяется автором как антропный, то есть целиком базирующийся на сугубо человеческих особенностях и ключевых аспектах и, позволяющий создавать только системы слабого искусственного интеллекта – лого-машины. В контексте монографии вводятся и раскрываются структурные основы психо-машин, такие как технотропная психика, технотропное сознание и технотропное бессознательное. В рамках обоснования фундаментальных различий между системами сильного и слабого искусственного интеллекта переосмысляется понятие самоорганизации и проводится демаркация на программную и программно-аппаратную самоорганизацию. В качестве базиса для формирования психо-машин предлагается переосмысленная в философско-методологическом ключе парадигма объектно-ориентированного программирования и, в частности, ключевое внимание уделяется адаптации паттернов данной парадигмы к необходимостям сферы построения интеллектуальной техники нового поколения.
Монографическое исследование представляет интерес для философов, программистов, широкого круга научных работников, аспирантов, преподавателей высших учебных заведений и студентов.

ПЕРЕЧЕНЬ СОКРАЩЕНИЙ И ОБОЗНАЧЕНИЙ

ООП – объектно-ориентированное программирование
ИИ – искусственный интеллект
ПК – программный компонент
ПП – паттерны проектирования

ВВЕДЕНИЕ

Проблематика сильного ИИ захватывает человечество ещё с середины прошлого века. Было осуществлено большое количество попыток непосредственного формирования подобной системы. Однако всё, что удалось создать на данный момент – слабый ИИ. Отчасти это происходит по причине отсутствия единого мнения на тему того, что именно должен представлять собой сильный ИИ, как он должен себя вести, как относиться к человеку и прочее.
К примеру, в работах С. Рассела и П. Норвига ИИ определяется как наука об агентах, получающих из своей среды результаты актов восприятия и выполняющих соответствующие действия [1]. Д. Хокинс считает, что системы ИИ могут не вполне соответствовать человеку по некоторым критериям [2]. По мнению Р. Пенроуза, разум, наделённый сознанием, просто не может работать подобно компьютеру, несмотря на алгоритмическую природу многих составляющих нашей умственной деятельности [3]. Согласно оптимистичному мнению И. Белда, именно сейчас настало время разработать и создать системы памяти, построенные по принципу функционирования коры головного мозга, потому что это очень многообещающая сфера, как в научном, так и в коммерческом плане [4]. С. Хокинг считает, что «…наш разум – это программа, в то время мозг – аналог компьютера» [5].
Также существует пессимистичный подход к ИИ. К примеру, Д. Баррат считает и пытается доказать, что ИИ, как и деление ядер – технология двойного назначения [6]. И. Маск заявляет, что ИИ «…обладает потенциалом для уничтожения цивилизации» [7]. Один из ключевых экспертов в области перспектив ИИ Э. Юдковский считает, что «…ключевая опасность – перспектива создания не такого интеллекта, который будет конкурировать с человеческим, а такого, который будет превосходить человеческий» [8]. О значимости машинного обучения в контексте разработки систем ИИ говорит один из ведущих специалистов в этой области П. Домингос [9]. Сравнение искусственного и естественного интеллекта, живых тканей, клеток и вычислительной архитектуры – основная линия труда А. М. Эндрю [10]. Д. Хокинс и С. Блейкли осуществляют критический анализ современного понимания ИИ и моделей нейросетей и на основе этого представляют гипотезы о «нахождении сознания» [11]. М. Каку в своих работах обобщает футуристические прогнозы ученых относительно искусственного разума [12], а Д. Дойч исследует саму возможность моделирования разума [13].
Таким образом, можно заметить, что тематика ИИ всесторонне прорабатывается специалистами в различных областях и нельзя сказать, что сильный ИИ до сих пор не сформирован ввиду недостаточности усилий исследователей и разработчиков. Также прослеживаются порой диаметрально противоположные взгляды на перспективы развития сферы ИИ – от весьма оптимистичных до крайне пессимистичных. Сильный ИИ, вышедший из-под непосредственного контроля человека, является одной из излюбленных тематик для формирования различных постапокалиптических сюжетов, как в литературе, так и кинематографе. С другой стороны, системы слабого ИИ, при их использовании в сугубо утилитарном ключе, могут принести и уже приносят человечеству немалую пользу в том плане, что позволяют переложить на них многие человеческие функции. Опять же является практически тривиальным утверждение о том, что обладание сильным ИИ позволит какому-либо государству, корпорации, группе исследователей-разработчиков войти в мировую элиту, и что сама по себе разработка сильного ИИ станет «большим шагом для человечества». Об этом заявил также президент РФ В. Путин, определив, что «…тот, кто станет лидером в этой сфере, будет властелином мира» [14].
Из вышесказанного прослеживается то, что ситуация с сильным ИИ является крайне дискуссионной, и мы не можем однозначно ответить на вопрос о том, «хорошо» ли ИИ или «плохо». Однако, в любом случае сильный ИИ на данный момент не сформирован. Наличествуют примеры, пытавшиеся претендовать на эту роль. К примеру, искусственная нейронная сеть компании Google стала лучшим в мире шахматистом, обыграв за четыре часа наиболее эффективные программы по игре в шахматы, сёги и го, а также нашла биологические аналоги лекарств от рака и старения [15]. Тест Тьюринга вообще проходило уже множество программных систем [16, 17]. Выглядит довольно впечатляюще, однако, ни о каком сильном ИИ здесь речь не идёт. Любые примеры подобного рода есть всего лишь результат вычислительной мощности современной техники и детерминированы они количественной трансформацией интеллектуальных систем – качественных скачков не происходит [18]. А именно подобные скачки, как правило, обуславливают возникновение чего-то ранее не существовавшего.
К разработке систем ИИ в целом применяют, как правило, парадигму ООП [19]. Однако ещё никогда данная парадигма не была применена прямо и намеренно к разработке именно сильного ИИ. Ещё никогда и никем не осуществлялось философско-методологическое исследование ООП с целью адаптации ключевых особенностей и возможностей этой парадигмы к потребностям парадигмы сильного ИИ. Мы считаем актуальным сделать краеугольным камнем подобного мероприятия паттерны ООП, а также исследовать специфику контекста их реализации, то есть наиболее фундаментальные особенности ООП в целом. Нами предполагается, что реализация подобного исследования поможет существенно «обновить» обе парадигмы: и ООП и сильного ИИ. Также это поможет заложить основу для инновационных методов формирования систем сильного ИИ и, возможно, существенно ускорит его непосредственную разработку.
В некотором роде может показаться, что если в основе ООП в целом и ПП ООП в частности лежат некие фундаментальные закономерности функционирования бытия, то не вполне актуально отдельно исследовать сами паттерны, а достаточно просто сразу и непосредственно определить в качестве оснований разработки сильного ИИ эти самые закономерности, так как они, как правило, общеизвестны в научном сообществе. Подобная точка зрения не вполне корректно отражает суть нашего исследования. Мы не говорим об абсолютном абстрагировании всякого аспекта исследуемого предмета, а только об абстрагировании до достижения необходимого и достаточного уровня для формирования возможности изъятия образа действия ПП ООП из области промышленного и коммерческого программирования и экстраполяции изъятого образа в контекст разработки системы ПК, которая нами понимается как основа для формирования сильного ИИ. То есть мы не подразумеваем чистую абстракцию, а именно абстракцию, адаптированную для парадигмы сильного ИИ и непосредственно изъятую из контекста, проверенного в области формирования «сущностей». К тому же, как станет видно позднее, не всякую фундаментальную закономерность стоит слепо использовать в контексте разработки сильного ИИ.
Также исследование именно абстракций ПП и вообще ООП, а не в целом всякого фундаментального аспекта бытия нами полагается значимым по той причине, что ПП уже и так реализуются в контексте разработки ИИ, только слабой его версии. Нам представляется целесообразным осмыслить в первую очередь уже применяемое, а не безосновательно вводить нечто иное. Также мы считаем, что необходимо осуществлять исследование именно с позиции ПК в общем смысле слова, специфицируя их характеристики только там, где это непосредственно значимо для контекста.










 
ГЛАВА 1
СИЛЬНЫЙ И СЛАБЫЙ ИИ КАК ДИВЕРГЕНТНЫЕ ВЕТВИ РАЗВИТИЯ ИНТЕЛЛЕКТУАЛЬНЫХ СИСТЕМ


Некоторая размытость и разобщённость проблематики ИИ, как одна из ключевых особенностей сферы разработки интеллектуальных технологий, отражается как на практических разработках, так и на концептуальных положениях данной области. Отсутствие общепринятых определений в рамках теории ИИ также, в свою очередь, влияет как на непосредственный процесс разработки интеллектуальных систем, так и на интерпретацию результатов деятельности разработчиков. Разрыв между теорией и практикой интеллектуальных технологий порождает явление параллелизма в развитии данной области в целом, что негативно сказывается на прогрессивных тенденциях и перспективах. Отсутствие в рамках науки общепринятых критериев ИИ в общем смысле и сильного ИИ, в особенности, грозит данной области подъёмом паранаучных исследований, а также приводит к тому, что рамки разработок ИИ ставятся на государственный контроль, за неимением иной возможности регулирования. Вследствие этого необходимо осмыслить понятие ИИ, как он воспринимается со стороны теоретиков и практиков в сфере интеллектуальных технологий. 
Немаловажно также понимание масштабов влияния ИИ на социальное сознание и массовую культуру, в том числе и на социальное сознание научного сообщества. По мнению некоторых исследователей, главенствующая роль в разработке ИИ позволит той или иной стране, корпорации, группе ученых войти в мировую историю в качестве совершивших колоссальный прорыв. Стивен Хокинг высказал мнение по этому поводу: «Я думаю, что наш разум – это программа, в то время мозг – аналог компьютера. Теоретически возможно скопировать содержимое мозга на компьютер и таким образом создать форму вечной жизни» [20]. Слова Хокинга о возможности скопировать содержимое мозга на компьютер и таким образом создать формулу вечной жизни буквально воспроизвели и образно продемонстрировали в художественном фильме Нила Бломкампа «Робот по имени Чаппи», в котором сознание сохраняли в виде информации на цифровом носителе. Тематика ИИ и внедрение его в самые разнообразные сферы нашей жизни уже давно перестала находиться под юрисдикцией творческой сферы деятельности. Учёные всего мира используют все имеющиеся ресурсы, дабы приблизиться к созданию ИИ, который сможет превзойти интеллект человеческий в исконно человеческих направлениях деятельности. 
Данная проблема настолько актуальна, что лидеры мировых держав всерьёз обеспокоены завоеванием лидерства в этой сфере. К примеру, президент РФ В. В. Путин 1 сентября 2017 г. в ходе открытого урока в Ярославле заявил, что «ИИ – это будущее не только России, это будущее всего человечества. Здесь колоссальные возможности и трудно прогнозируемые сегодня угрозы. Тот, кто станет лидером в этой сфере, будет властелином мира. […] Если мы будем лидерами в этой сфере, также будем делиться этими технологиями со всем миром, как мы сегодня делимся атомными технологиями, ядерными технологиями» [21]. Также в Госдуме РФ началась подготовка законопроекта, направленного на регулирование и упорядочивание взаимоотношений социума и ИИ. В США вопрос стоит ещё более остро. Дональд Трамп, в своё время, поручил законодательно контролировать сферу ИИ и учёных, работающих в этой области, издав регламент, в соответствии с которым было обозначено, чем можно и чем нельзя считать ИИ, где можно, а где нельзя его применять. В Индии, напротив, общество весьма встревожено тем, что системы ИИ могут оставить без рабочих мест большую часть населения. Северная Корея и КНР пытаются обойти США и РФ по уровню развития интеллектуальных технологий. В Республике Беларусь Президент объявил о необходимости скорейшего формирования цифровой экономики. Некоторые случаи увлечённости интеллектуальными технологиями становятся основой для создания культов: бывший инженер Google Энтони Левандовски создал некоммерческую квазирелигиозную организацию «Путь в будущее», в основе которой лежит идея замещения Бога ИИ. Логично предположить, что в начале 21 века гонка вооружений вышла на качественно новый уровень, в противовес «количественному» состязательному аспекту, господствовавшему ранее. Детерминанты данной смены парадигм были заложены ещё в середине 20 в. на состоявшейся в 1956 г. конференции «Dartmouth Summer Research Projecton Artificial Intelligence» [22]. Выступление на ней Джона Маккарти и тематика его доклада позволила проблемам, связанным с ИИ, выйти на трансдисциплинарный уровень. 
Истоки и предпосылки к возникновению самой сферы подобных исследований можно обнаружить ещё в 17 в. в работах Рене Декарта «Рассуждение о методе» 1673 г. и Томаса Гоббса «Человеческая природа» 1640 г., в которых были заложены основы механистического восприятия человеческой сущности [23, 24]. В 1623 г. Вильгельмом Шикардом была сконструирована первая механическая вычислительная машина, за которой последовали машины Блеза Паскаля в 1643 г. и Готфрида Лейбница в 1671 г. [25, 26, 27]. В 1832 г. С. Н. Корсаков выдвинул принцип разработки научных методов и устройств для усиления возможностей человеческого разума и впервые озвучил понятие «интеллектуальная машина» [28, 29]. В 19 в. Чарльз Бэббидж и Ада Лавлейс уже работали над программируемой механической вычислительной машиной [30, 31]. Даже несколько ранее, в начале 19 в., французский изобретатель Жозеф Мари Жаккар представил первую в истории программируемую машину в виде ткацкого станка, работающего на перфокартах. Характерной особенностью станка была способность автономно реализовывать программные задания, и в то время это было настоящим прорывом. В 1910-1913 гг. Б. Рассел и А. Н. Уайтхэд работали над трудом «Принципы математики», который произвёл революцию в формальной логике [32]. Далее в 1936 г. А. Чёрч делает достоянием научной общественности теорию лямбда-исчисления, которая используется для формализации алгоритмов, а вскоре после него А. Тьюрингом была предложена «Машина Тьюринга», представляющая собой абстрактную модель, осуществляющую определённые вычисления [33, 34]. На основе работ Чёрча и Тьюринга С. Клини, бывший учеником Чёрча, вывел эквивалентность лямбда-исчисления и «Машины Тьюринга», которое можно в простейшем виде формализовать следующим образом: всякая функция, при её принципиальной вычислимости, может быть определена и реализована как при помощи лямбда-исчисления, так и при помощи «Машины Тьюринга». Данное утверждение было названо «Тезис Чёрча-Тьюринга» и оказало существенное влияние не только на развитие математики и информатики, но и на всю науку в целом [35]. В 1941 г. Конрад Цузе построил первый работающий программно-управляемый компьютер [36, 37]. В 1943 Д. Хебб в работе «Организация поведения» описал основные принципы обучения нейронов [70]. В том же 1943 г. в своей статье «Логическое исчисление идей, относящихся к нервной активности» Уоррен Мак-Каллок и Уолтер Питтс опубликовали доклад, который заложил теоретические основы для дальнейшего практического воплощения нейронных сетей [38]. Тогда впервые было употреблено понятие искусственной нейронной сети. В частности, ими была предложена модель искусственного нейрона. Также немаловажное значение для формирования концептуальной основы ИИ имеет кибернетика Н. Винера, которая нуждалась в ЭВМ для решения своих задач, и эволюционировала в прямой зависимости от технического прогресса в области электронной вычислительной техники [39]. В рамках кибернетики исследовались основные закономерности поведения сложных систем и выявлялись ключевые аспекты системного управления, вследствие чего данное направление оказало влияние на дальнейшее развитие таких дисциплин, как компьютерное зрение, робототехника, цифровая философия и многие другие. Цифровая философия же, в свою очередь, возникла на основе исследования механизмов функционирования клеточных автоматов, пионерами введения которых в научное познание были С. Улам и Дж. Ф. Нейман в 1940-х гг. [40, 41]. 
Среди советских учёных интеллектуальные технологии были главной областью научной деятельности Д. А. Поспелова. В частности, им был впервые в мире представлен логико-лингвистические) модели, который послужил теоретической основой ситуационного управления крупными системами [42, 43, 44]. В СССР была разработана первая персональная ЭВМ «Мир-1» академиком В. М. Глушковым в 1966 г. Также академик Глушков предложил идею обучению компьютера распознаванию осмысленных предложений на естественном языке, чем предвосхитил целое направление в дальнейшем машинном обучении [45]. Ученик А. Ляпунова академик А. П. Ершов был одним из первопроходцев корпусной лингвистики, теории алгоритмов и программирования, создал языки программирования Альфа и Альфа 6 [46]. Также идеями моделирования и создания интеллектуальных программ интересовался первый советский чемпион мира по шахматам М. М. Ботвинник, который участвовал в создании шахматной программы «Пионер» и в 1980-е-1990-е гг. работал над проблемой компьютерного моделирования человеческой мыслительной деятельности [47, 48, 49]. 
Начало исследований непосредственно в области ИИ в современном понимании связывают с работами Алана Ньюэлла и Герберта Саймона конца 1950-х гг., исследовавших процессы решения различных задач. Результатом их совместной работы стали программы «Логик-теоретик» и «Общий решатель задач», действующие на основе применения разнообразных эвристических методов [50, 51]. Модель перцептрона Фрэнка Розенблатта, предложенная им в 1957 г., была воплощена в виде электронной машины «Марк-1», ставшей первым в мире нейрокомпьютером [52]. В начале 60-х гг. Сеймур Пейперт совместно с М. Мински основал лабораторию ИИ в Массачусетском технологическом университете, а в 1970 г. они издали книгу «Персептроны» [53, 54]. Также Пейперт в 1967 г. создал язык программирования LOGO [55]. В 1970-х гг. появилась система автоматизированного перевода документации ЭСПРИТ, разработанная в рамках европейской программы. Значительным продвижением в развитии теории ИИ было выдвижение идеи М. Мински и Р. Шенка о фреймах (информационных структурах). 
Подходы, применяющиеся различными исследователями, разнятся: одни считают, что интереснее, полезнее и практичнее воспроизводить различные функции, ранее считавшиеся исключительно человеческой прерогативой, другие же пытаются сделать нечто похожее на то, что в качестве интеллекта функционирует в самом человеке. Философы продолжают спорить о «сильном и (или) слабом искусственном интеллекте» [89, 90, 91] со времён Джона Сёрла. Аргументация «Китайской комнаты» противопоставляется положениям теста Тьюринга [56]. Сильный ИИ, который, по мнению Сёрла, «будет разумом в том смысле, в котором человеческий разум – это разум», со своей стороны противоречит тем, кто считает это невозможным [57, 58]. Несколько особняком стоят работы Д. Хофштадтера, в частности его книга «Гёдель, Эшер, Бах: эта бесконечная гирлянда» [59]. Ключевой идеей Хофштадтера является несколько скептическое отношение к достижению некоторых разработок ИИ, в смысле того, что они, по мнению Хофштадтера, не имеют никакого отношения к возникновению искусственного разума [60].
Современными исследованиями ИИ на постсоветском пространстве занимается К. В. Анохин, автор понятия «когнитом» и создатель гиперсетевой теории мозга и сознания, который внедряет свои нейробиологические идеи в сферу ИИ [61]. В Республике Беларусь проблематикой ИИ занимался доктор филологических наук В. В. Мартынов, который посвятил свой научный поиск лингвистическим аспектам ИИ, в частности, семантической формализации [62, 63]. Валерий Цуриков, один из создателей проекта «Изобретающая машина» – белорусского стартапа, ещё в советское время в 1987 г., выразил мнение о том, что ИИ принципиально возможно создать. «Изобретающая машина» – сложная программа для поддержки многофакторных инженерно-изобретательских задач. Теперь Цуриков работает над проектом «TrueMachina» – интеллектуальной системой, которая будет генерировать творческие решения самостоятельно, без помощи человека. Над проблемой одушевлённых машин в наше время работает А. В. Колесников. Он же вводит футуристическое понятие «пси-машины», как сложного объекта, обладающего зачатками психической деятельности [64, 65]. Академик НАН РБ С. Я. Килин, помимо проблем квантовой механики, исследует также и проблематику ИИ, в частности, робототехнику. По его мнению, «… ИИ – это системы, выполняющие интеллектуальные задачи, которые сравнимы с интеллектуальными возможностями человека» [66]. Стоит выделить работы Г. А. Прокоповича и Д. А. Вятченина по робототехнике и нейронным сетям, как значимые для понимания современной ситуации в плане восприятия парадигмы ИИ белорусским научным сообществом [67, 68]. Наиболее ярким отражением интереса к сфере ИИ в РБ является возникновение Центра ИИ при Национальной академии наук Республики Беларусь в 2015 г. на базе Центра ОИПИ и Института физиологии Национальной академии наук Республики Беларусь.
Зарубежные наработки в сфере ИИ также весьма разнообразны и существенно значимы для понимания проблемы в целом. 
Последние из достижений и открытий в области разработок ИИ достойны повышенного интереса. Программа, написанная сотрудниками Технологического института Джорджии в США, воссоздала игру SUPERMARIO без знания исходного кода. Актуальными задачами для искусственных нейронных сетей выступают подготовка отзывов для кафе и ресторанов, определение лиц с нетрадиционной ориентацией по фотографиям пользователей соцсетей. Искусственная нейронная сеть Яндекса «Алиса» уже довольно успешно общается с пользователями социальных сетей и, возможно, соответствует всем критериям успешного прохождения теста Тьюринга. Программа, разработанная компанией DISNEY и заявленная в качестве ИИ, может оценивать качество текста. Искусственная нейронная сеть компании Google помогла найти в созвездии Дракона планету Kepler-90i в Солнечной системе подобной нашей, и удалённой от нас всего на 2545 световых лет. Также искусственная нейронная сеть компании Google стала лучшим в мире шахматистом, обыграв за 4 часа лучшие в мире программы по игре в шахматы, сёги и го, нашла биологические аналоги лекарств от рака и старения. 11 декабря 2017 г. на конференции по вопросам ИИ, проводимой в Калифорнии, учёные признались, что «больше не понимают принципов функционирования искусственной нейронной сети» и что она «выбивается из-под контроля». Ну и, конечно же, из последних мировых наработок можно особенно отметить ChatGPT от компании OpenAI, представляющий собой чат-бот с ИИ, который был разработан при помощи машинного обучения [73]. Указывается, что данное приложение способно на осуществление анализа и перевода текстов, анализа изображений, поддержание диалога и следование контексту этого диалога, нахождение точных ответов на вопросы и даже на написание программного кода.
Все эти события ставят ряд ключевых вопросов, связанных с процессами самореализации ИИ, и требуют разработки критериев восприятия (ответа на вопросы: «Что он есть такое?», «Чем он должен быть?», «Что мы можем им считать?», «Возможно ли его создать и если возможно, то как нам это сделать?») систем ИИ со стороны общества в целом и научного сообщества, в частности. Несмотря на многие достижения, вопросы, связанные с ИИ, остаются далеко не окончательно разрешёнными. Корреляция между теоретическими определениями ИИ и предполагаемыми практическими его функциями достигает существенных масштабов в своём несоответствии. Не решён вопрос о сущностной природе слабого и сильного ИИ.
Как уже было сказано выше, в определениях ИИ господствуют плюралистические тенденции, поэтому зачастую можно встретить следующие толкования: ИИ – это комплексная научная дисциплина, которая занимается созданием ИИ; ИИ – это программа, способная к самообучению и саморазвитию; ИИ – это машина, способная «думать»; ИИ – это алгоритм, способный на творчество. Непосредственно же ретроспектива ИИ, как нового научного направления, начинается примерно в середине 20 в. К этому времени уже было сформировано множество предпосылок его зарождения: среди философов давно шли споры о природе человека и процессе познания мира, нейрофизиологи и психологи разработали ряд теорий относительно работы человеческого мозга и мышления, экономисты и математики задавались вопросами оптимальных расчётов и представления знаний о мире в формализованном виде; также был сформирован фундамент математической теории вычислений – теории алгоритмов – и были созданы первые компьютеры. Возможности новых машин в плане скорости вычислений оказались существенно выше человеческих, поэтому в учёном сообществе зародился вопрос: каковы границы возможностей компьютеров и достигнут ли машины уровня развития человека? И в 1950 г. один из пионеров в области вычислительной техники, английский учёный Алан Тьюринг пишет статью под названием «Может ли машина мыслить?», в которой описывает процедуру, с помощью которой можно будет определить момент, когда машина сравняется в плане разумности с человеком, получившую название «теста Тьюринга» [16]. В некотором смысле, в «аксиоме» о том, что вычислительные возможности компьютеров намного превосходят те же возможности человека, есть некоторые противоречия. К примеру, так называемые люди-калькуляторы, мнемонисты, ярко выраженные «синестетики» и прочие представители человечества порой демонстрируют способности, превосходящие возможности даже современных компьютеров. Можно сказать, в противовес, что такие люди скорее исключение, чем правило, однако нам представляется, что это не вполне так. Целесообразно будет предположить, что в данном случае основанием служит степень «чувствительности» субъекта к собственному бессознательному. То есть, сообразно с данной гипотезой, в бессознательной области психики непрерывно происходит количество вычислений, намного превосходящее количество вычислений в самых современных интеллектуальных технологиях. Однако это остаётся неосознанным потому, что сознание субъекта не способно справиться с подобным количеством информации, а непосредственно до самого сознания доходят лишь вердикты и принятые решения (аналогия «сознание-бессознательное» – «монитор-железо+ПО»). Таким образом, «аксиома» о тотальном интеллектуальном превосходстве интеллектуальных технологий над человеком противоречива и не вполне корректна.
В истории же непосредственно ИИ, как правило, за некую отправную точку принимается определение, данное Джоном Маккарти в 1956 г. на конференции в Дартмутском университете. Поясняя своё определение, Джон Маккарти указывает: «Проблема состоит в том, что пока мы не можем в целом определить, какие вычислительные процедуры мы хотим называть интеллектуальными. Мы понимаем некоторые механизмы интеллекта и не понимаем остальные. Поэтому под интеллектом в пределах этой науки понимается только вычислительная составляющая способности достигать целей в мире» [74]. Согласно Маккарти, исследователи ИИ вольны использовать методы, которые не наблюдаются у людей, если это необходимо для решения конкретных проблем. В то же время существует и точка зрения, согласно которой интеллект может быть только биологическим феноменом.
В начале 1980-х гг. ученые в области теории вычислений Барр и Файгенбаум предложили следующее определение ИИ: «ИИ – это область информатики, которая занимается разработкой интеллектуальных компьютерных систем, то есть систем, обладающих возможностями, которые мы традиционно связываем с человеческим разумом, – понимание языка, обучение, способность рассуждать, решать проблемы и так далее…» [75]. Мнение Барра и Фейгенбаума начала 1980-х гг. всё ещё определяет ИИ как некую область какой-то науки – информатики, в данном случае. И подобное мнение ранее довольно часто встречалось. На современном же этапе уже происходит некая своеобразная объективация ИИ: его считают каким-либо отдельным феноменом со своими особенностями, а в контексте нашего исследования, более того, в противовес объективации происходит его специфическая «субъективация». То есть он более не считается классическим (по типу научной рациональности) научным объектом, а скорее постнеклассическим, способным к некой аутентичности, самобытности и самоорганизации.
Проблематика выявления общих оснований разработки систем ИИ заключается не только в сугубо концептуальной области. К примеру, некоторые исследователи детерминируют плюралистичность понятийного аппарата сферы разработки интеллектуальных технологий лингвистическими аспектами. Как указывает председатель Петербургского отделения Российской ассоциации ИИ Т. А. Гаврилова, в английском языке словосочетание «artificial intelligence» не имеет той несколько фантастической антропоморфной окраски, которую оно приобрело в довольно неудачном русском переводе. Слово «intelligence» означает «умение рассуждать разумно», а вовсе не «интеллект», для которого есть английский аналог «intellect». Из этого следует, что в мировом научном сообществе непонимание и расхождение в понимании природы теоретического феномена ИИ детерминировано отчасти и лингвистическими причинами. Участники Российской ассоциации ИИ дают следующие определения ИИ: научное направление, в рамках которого ставятся и решаются задачи аппаратного или программного моделирования тех видов человеческой деятельности, которые традиционно считаются интеллектуальными; свойство интеллектуальных систем выполнять функции (творческие), которые традиционно считаются прерогативой человека. При этом интеллектуальная система – это техническая или программная система, способная решать задачи, традиционно считающиеся творческими, принадлежащие конкретной предметной области, знания о которой хранятся в памяти такой системы. Наука под названием «Искусственный Интеллект» входит в комплекс компьютерных наук, а создаваемые на её основе технологии – к информационным технологиям. Задачей этой науки является воссоздание с помощью вычислительных систем и иных искусственных устройств разумных рассуждений и действий. Одно из частных определений интеллекта, общее для человека и «машины», можно сформулировать так: «Интеллект – это способность системы создавать в ходе самообучения программы (в первую очередь эвристические) для решения задач определённого класса сложности и решать эти задачи» [76]. В целом же ИИ теми или иными исследователями могут даваться определения, в соответствии с двумя основными интерпретационными векторами данной сферы: первое определение – системы, которые «думают», подобно людям; второе определение – системы, которые «думают» рационально (в зависимости от контекстуального значения понятия рационального) [77]. К примеру, под первую категорию подпадает так называемый «коммуникативный» ИИ, способный пройти тест Тьюринга, а также игровой ИИ; под вторую категорию – программы для интеллектуальных игр и программы способные успешно решать задачи, требующие работы с большими объёмами информации.
Единого ответа на вопрос, чем занимается (и чем должен заниматься) ИИ, также не существует. Почти каждый автор, формулирующий некий подход к ИИ, отталкивается от какого-либо определения или какой-либо отдельной дисциплины, рассматривая в проблематику сквозь некую дисциплинарную или концептуальную призму. В философии и психологии не решён вопрос о природе и статусе человеческого интеллекта. Нет и точного критерия достижения компьютерами «разумности», хотя на заре ИИ был предложен ряд гипотез, например, тест Тьюринга или гипотеза Ньюэлла-Саймона. Последняя (гипотеза Ньюлла-Саймона) также представляет определённый интерес, связанный с использованием символов в системах обработки информационных данных. Гипотеза Ньюэлла-Саймона свидетельствует о том, что «если система действует осмысленно, то, значит, она выполняет символьные вычисления (операции с использованием символов), а если система использует символьные вычисления, то она действует осмысленно» [78]. Интерпретационная позиция данной гипотезы, сформированной Алленом Ньюэллом и Гербертом Саймоном в 1976 г., обязывает считать все имеющиеся компьютерные технологии неизменно осмысленными. Развитие программного обеспечения, компьютеров и интеллектуальных технологий пошло именно по пути символьных вычислений и оперирования с информационными блоками посредством символов, поэтому, согласно гипотезе Ньюэлла-Саймона, человечество окружает огромное количество машин, действующих по-настоящему «осмысленно» [79]. 
Несмотря на наличие множества подходов, как к пониманию ИИ и его задач, так и непосредственно созданию интеллектуальных информационных систем, можно выделить два основных подхода к разработке ИИ:
•	Нисходящий, семиотический – создание экспертных систем, баз знаний и систем логического вывода, имитирующих высокоуровневые психические процессы: мышление, рассуждение, речь, эмоции, творчество и прочее.
•	Восходящий, биологический – изучение нейронных сетей и эволюционных вычислений, моделирующих интеллектуальное поведение на основе биологических элементов, а также создание соответствующих вычислительных систем, таких как нейрокомпьютер или биокомпьютер [80]. 
Последний подход, строго говоря, не относится к науке об искусственном интеллекте в смысле, данном Джоном Маккарти, – их объединяет только общая конечная цель. Логично будет предположить, что ни один из этих двух подходов не способен привести к формированию систем сильного ИИ, потому что в рамках первого подхода искусственно создаётся «верхушка айсберга» («высшая нервная деятельность»), полностью лишённая основы (нервной системы, психики, сознания, бессознательного), а в рамках второго подхода формируется только лишь некое отдалённое подобие на биологическую основу «высшей нервной деятельности», которое не обязательно может быть целесообразным, потому что в процессе его – подобия на биологическую основу – формирования полностью отсутствуют факторы аутентичной эволюции и самоорганизации.
Кроме двух основных подходов, которые, ввиду своей глобальности, являются скорее векторами развития науки об искусственном интеллекте, чем подходами в собственном смысле слова, выделяют ещё некоторые, соответственно, подходы.
На современном этапе можно выделить два направления развития ИИ, которые можно кратко и условно обозначить как: 
•	Уподобление человеческому разуму. Для данного подхода критерием успешности разработки систем ИИ служит схожесть-несхожесть с аналогичными человеческими показателями.
•	Генерирование рационального искусственного «разума». Для данного подхода ключевой задачей является генерирование разума в принципе, не обязательно подобного человеческому, но обязательно «мыслящего» конструктивно, продуктивно и целесообразно.
Однако в настоящий момент в сфере ИИ наблюдается вовлечение многих предметных областей, имеющих скорее практическое отношение к ИИ, чем фундаментальное. Многие подходы были апробированы, но к возникновению искусственного разума ни одна группа разработчиков пока не приблизилась. С другой стороны, невозможно предсказать, как поведёт себя нейросеть компании Google (или какая-либо иная) в ближайшее время. Также не стоит забывать о том, что недавно в процессе машинного обучения начали использовать квантовый компьютер, как альтернативное решение проблемы ИИ. 
В истории ИИ наиболее значимой (в контексте нашего актуального исследования) вехой является проведённая Джоном Сёрлом в 1980 г. демаркация целостной сферы ИИ на слабый ИИ и сильный ИИ, которая была осуществлена в его работе «Китайская комната», описывающей мысленный эксперимент. Сильный и слабый ИИ – это гипотеза в философии ИИ, согласно которой некоторые формы ИИ могут действительно обосновывать и решать задачи как минимум на уровне человека, а скорее всего, даже превосходя человеческий уровень. Теория сильного ИИ предполагает, что компьютеры могут приобрести способность мыслить и осознавать себя, хотя и не обязательно их мыслительный процесс будет подобен человеческому. Теория слабого ИИ отвергает такую возможность. 
На актуальном этапе мы имеем внушительное количество функционирующих систем слабого ИИ.
Уже сейчас является понятным, что вопрос об использовании или не использовании, развитии или не развитии систем ИИ со стороны бизнес-компаний и крупных корпораций стоит в качестве вопроса выживания в условиях актуальных нам реалий. Те компании, которые отказываются идти в ногу со временем по причинам нехватки ресурсов, недостаточной скорости реагирования на изменившиеся условия, следовании консервативным или морально-нравственным традициям, или по любой иной причине отмирают, по причине высокой себестоимости оказания услуг, недостаточном для современности уровне сервиса и, как следствие, обесценивающихся активах. Гонка вооружений в сфере бизнес-технологий становится всё более изощрённой и скоротечной, а темпы её растут, видимо, в геометрической прогрессии.
Несмотря на все достижения сферы интеллектуальных технологий в медицине, экономике, науке, военном деле, компьютерном зрении и прочих областях, следует понимать, что здесь имеется в виду по факту только слабый ИИ. Применение же данных систем носит сугубо прикладной, прагматический характер и не отличается фундаментальными амбициями. Осмыслив в целом ситуацию относительно систем слабого ИИ, целесообразно отметить, что слабый ИИ представлен сугубо узконаправленными практико-ориентированными технологиями, реализующими при помощи тех или иных методов и средств символические и знаковые задачи, имитирующие человеческую деятельность и направленными на облегчение повседневной жизни человека. Про феномен искусственного разума, про методы и средства его генерирования в рамках интеллектуальной системы не идёт речь в случае с разработкой систем слабого ИИ. Все системы слабого ИИ, для концептуального удобства, целесообразно будет определить, как лого-машины.
В случае же с примерами систем сильного ИИ, о которых Джон Сёрл сказал, что они должны обладать разумом в том смысле, в «котором человеческий разум – это разум» [81, С. 7]. Так и с определением сильного ИИ как такового, уместно заметить, что примеры подобных систем отсутствуют, а концепты крайне расплывчаты [82]. Рассмотрение систем сильного ИИ позволяет заметить, что примеры его самореализации на почве материального субстрата можно обнаружить лишь в фантастической литературе и продуктах киноиндустрии, в противовес слабому ИИ, примеров внедрения которого в нашу повседневную жизнь привести можно, как было показано, великое множество. С сильным ИИ всё обстоит несколько сложнее. В сущности, никто не в курсе, что он есть такое. И если фильмы и книги о разумных машинах не одну тысячу раз выходили из-под пера и кинокамеры авторов-создателей, то среди футурологов и учёных, работающих в сфере ИИ, интеллектуальная продукция не столь богата. Если и дозволяются какие-либо прогнозы и комментарии, то эти высказывания носят, опять же, бессистемный и теоретически разрозненный характер, а также отдают некой опаской за судьбу человечества в случае создания сильного ИИ и открытой фантастичностью. Стоит заметить, что все предостережения выдающихся деятелей науки, политики, экономики и прочих значимых областей человеческой деятельности относительно опасности, которую несёт в себе ИИ, относительно неоднократно экранизировавшегося во всех возможных ипостасях «восстания машин» и прочих апокалиптических сценариев, связанных с конфликтом человечества и технологий, касаются исключительно сильного ИИ. Тогда как опасения по поводу повсеместности и всепроникновению систем слабого ИИ связаны не с возможностью открытой конфронтации, а с опасностью нарушения границ частной жизни, недостатком рабочих мест и прочих социально значимых, но не апокалиптических сценариев развития событий.
Так, к примеру, упоминавшийся Илон Маск приравнял создание сильного ИИ к вызову демона, которого невозможно будет контролировать [83]. Также руководитель управления машинного интеллекта и исследований компании «Яндекс» Михаил Биленко говорит о том, что у человечества должен быть «рубильник, которым можно выключить ИИ», и о том, что ИИ уже «…повсюду нас окружает» и «...к апокалипсису нужно готовиться ещё с детства» [84]. Свой скепсис и опасения Маск, Биленко и многие другие исследователи разделяют с шведским философом, профессором Оксфордского университета Ником Бостромом, чью последнюю книгу «Superintelligence: Patlis, Dangers, Strategies» Илон Маск посоветовал своим подписчикам в твиттере. Пока некоторые футурологи предвещают людям комфортное будущее в результате повсеместной замены человеческого труда машинным, Ник Бостром видит в искусственном интеллекте угрозу существования всего человеческого вида. Он говорит по этому поводу следующее: «Во многих областях деятельности уровень искусственного интеллекта уже превосходит уровень человеческого. Появились системы, способные не только вести логические игры, но и одерживать победы над людьми. Приведенная […] информация об отдельных игровых программах демонстрирует, как разнообразные виды ИИ побеждают чемпионов многих турниров» [85, С. 35]. По поводу самообучающихся систем Ник Бостром считает, что: «Для нас важно создать ИИ, у которого хватит ума учиться на своих ошибках» [85, С. 60]. Также, по мнению Бострома: «ИИ может быть менее человечен, чем пришелец. Нет ничего удивительного, что любого разумного пришельца могут побуждать к действию такие вещи, как голод, температура, травмы, болезни, угроза жизни или желание завести потомство. ИИ, по сути, ничто из перечисленного интересовать не будет. Вряд ли вы сочтете парадоксальной ситуацию, если появится какой-нибудь ИИ, чьим единственным предназначением, например, будет: подсчитать песчинки на пляжах острова Боракай; заняться числом π и представить его, наконец, в виде обыкновенной десятичной дроби; определить максимальное количество канцелярских скрепок в световом конусе будущего» [85, С. 171-172].
С другой стороны, создатель и CEOFacebook Марк Цукерберг считает высказывания подобного рода «крайне безответственными» и упрекает их авторов в непонимании самой сути вопроса: «ИИ в будущем сделает нашу жизнь лучше», – сказал Цукерберг, отвечая на вопрос пользователя Facebook [86]. Он подчеркнул, что выступает против распространения домыслов об опасности ИИ: «У меня есть четкая позиция по этому поводу, я настроен оптимистично. Я думаю, что вы можете создавать вещи, которые сделают мир лучше. Но в случае с ИИ я особенно оптимистичен», – добавил миллиардер [86]. «И я думаю, что эти люди просто проявляют скептицизм, рассуждая о сценариях конца света. Это имеет негативный эффект, я на самом деле думаю, что это очень безответственно», – пояснил он [86]. Бизнесмен отметил, что новые технологии всегда могут быть использованы для того, чтобы творить зло или творить добро. По его мнению, позитивный эффект от массового распространения ИИ мы увидим уже в течение 5-10 лет.
Актуально также снова привести мнение Стивена Хокинга по поводу возможного создания сильного ИИ: «Создание ИИ может быть самым величайшим открытием человечества, но также и последним» [87]. C другой стороны, Макс Версаче, генеральный директор компании Neurala, которая создает программное обеспечение, основанное на технологиях ИИ, полемизируя с Илоном Маском, написал колонку для издания Venture Beat [88]. Версаче определил четыре направления, в которых искусственный интеллект не спровоцирует апокалипсис на Земле, а, наоборот, спасет человечество. «ИИ сделает нас честными. Мой первый аргумент заключается в следующем: создание этических норм для ИИ заставит человечество также пересмотреть свои устои. Поскольку мы определяем правила игры для ИИ, мы будем вынуждены играть по ним сами. ИИ поможет нам придерживаться наших собственных стандартов – он будет всезнающим и вездесущим. ИИ устранит наши предубеждения» [89].
Целесообразно во многом согласиться со всеми вышеперечисленными деятелями, и с Ником Бостромом, и с положениями его книги «Искусственный интеллект», кроме того аспекта его мнения, в котором он доходит до агитации человечества к обретению тотального контроля за технологиями вообще и, в особенности, за интеллектуальными технологиями, в частности. В этом смысле несколько целесообразнее принять интерпретационную позицию Марка Цукерберга и Макса Версаче. По сути, данный вопрос является куда более фундаментальным, чем просто футуристская попытка человечества выжить рядом с той угрозой, которую оно само себе создаёт. Также тематика сильного ИИ (ввиду особенностей самого объекта), в первую очередь, экзистенциальна, а затем уже технологична и прагматична. Априори видеть угрозу со стороны сильного ИИ – не лучшее решение на пути к его созданию. И в тотальном контроле за ИИ также видится не возможность спасения человечества, а тормозяший «якорь» на пути интеллектуального (и экзистенциального) прогресса. 
В контексте сильного ИИ ради концептуального удобства, целесообразно назвать системы подобного рода психо-машинами. И когда в нашем дискурсе речь идёт о разработке сильного ИИ, то он никогда не подразумевается в единственном числе, а всегда в контексте сильных «субъективных ИИ». И каждый из них, в соответствии с нашим подходом, должен будет быть субъективным, уникальным и неповторимым. Также на основе вышесказанного целесообразно сделать некоторые выводы и ввести определённые понятия. Сильный ИИ – система, обладающая технотропной психикой и технотропным сознанием. Технотропная психика (в широком смысле) – субъективное отражение системой объективной действительности. Технотропная психика (в узком смысле) – неотъемлемое качество психо-машины, являющееся критерием наличия в контексте отдельной системы сильного ИИ и служащее для взаимодействия технотропного субъекта (интеллектуального агента) с технотропной средой, для самотрансформаций системы и изменения самой системой этой среды. Технотропное сознание – высшая стадия развития технотропной психики, обеспечивающая системе способность к рефлексии и самосознанию. 
Сильный ИИ на данный момент не создан. Также не существует известных методов и средств его создания и интерпретации его проявлений. И если системы слабого ИИ пошли по пути имитирования частных человеческих функций с целью облегчить повседневную жизнь человека, то конечной целью сферы сильного ИИ является создание существа, обладающего технотропной психикой, технотропным сознанием, мышлением, способностью к коммуникации и так далее. Сфера слабого ИИ – сугубо прикладная, сфера сильного ИИ – фундаментальная, вследствие поднимаемых в её рамках вопросов, влияния её разработок на человеческое существование и на основные мировоззренческие установки. 
Также целесообразно заметить, что разработки в области систем слабого ИИ всегда являются узконаправленными в прикладном смысле, то есть конкретная лого-машина всегда разрабатывается под конкретную и определённую частную задачу, будь то компьютерное зрение, задачи классификации, коммуникативное имитирование, игровой ИИ или любые иные. Данный специализированный подход к разработке интеллектуальных технологий не является новым. Он был принят за основу ещё в 80-е гг. 20 в., на фоне разочарования в искусственном интеллекте из-за невозможности сформировать «человекоразмерную» машину. Данный вектор разработок ИИ не ставит своей целью генерирование систем сильного ИИ, а сосредотачивает разработчиков, чьи цели и задачи соотносятся с сугубо узкоспециализированной практической направленностью. Системы, сформированные в рамках данного направления, не претендуют на человекоразмерность и разработчики не скрывают этого. Именно поэтому, из-за различий в целях, задачах, предназначении и восприятии со стороны социума, векторы разработки систем слабого и сильного ИИ целесообразно обозначить как дивергентные. Как было сказано выше, не существует общепринятого метода создания сильного ИИ, но есть некоторые области, в которых реализуются «подводящие» исследования.

Выводы главы 1

В главе рассмотрены генеалогические аспекты ИИ в целом и конкретизирована предметная область исследований в контексте наличия базовых проблем. Указано на отсутствие единых оснований для разработки систем ИИ, на существенный разрыв между теорией и практикой и отсутствие критериев сильного ИИ. В контексте исследования генеалогической составляющей ИИ путь был прослежен от работ Р. Декарта до разработки технологии ChatGPT. Было показано влияние на общий контекст парадигмы ИИ таких феноменов, как Жаккардовый станок, лямбда-исчисление и гипотеза Ньюэлла-Саймона. Представлены различные подходы к определению того, что должно пониматься под ИИ, – от достаточно узкого интерпретирования до весьма расплывчатого.
В результате мы пришли к выводу о том, что на данный момент можно выделить два направления развития ИИ. Первое – это уподобление человеческому разуму. Для данного подхода критерием успешности разработки систем ИИ служит схожесть-несхожесть с аналогичными человеческими показателями. Второе направление можно обозначить как генерирование рационального искусственного «разума». Для данного подхода ключевой задачей является генерирование разума в принципе, не обязательно подобного человеческому, но обязательно «мыслящего» конструктивно, продуктивно и целесообразно. 
Нами полагается, что одним из наиболее значимых событий в истории парадигмы ИИ стала демаркация целостной сферы ИИ на слабый ИИ и сильный ИИ, которая была осуществлена Джоном Сёрлом и впервые представлена в его работе «Китайская комната». Теория сильного ИИ предполагает, что компьютеры могут приобрести способность мыслить и осознавать себя, хотя и не обязательно их мыслительный процесс будет подобен человеческому. Теория слабого ИИ отвергает такую возможность. Далее мы приходим к выводу о том, что на данный момент реально воплощены только системы слабого ИИ, а насчёт сильного у многих исследователей и руководителей глобальных корпораций имеются зачастую прямо противоположные мнения. То есть данная сфера единством не отличается.
Далее мы, ввиду специфики проблемного поля сильного ИИ, предлагаем называть системы подобного рода психо-машинами. В свою очередь системы слабого ИИ, которыми и представлен ИИ на данный момент, мы предлагаем называть лого-машинами. Далее делаются выводы о том, что (так как это всё же должен быть разум) сильный ИИ – это система, обладающая технотропной психикой и технотропным сознанием. Технотропную психику мы определяем, как субъективное отражение системой объективной действительности и неотъемлемое качество психо-машины, являющееся критерием наличия в контексте отдельной системы сильного ИИ и служащее для взаимодействия технотропного субъекта с технотропной средой, для самотрансформаций системы и изменения самой системой этой среды. Технотропное сознание – высшая стадия развития технотропной психики, обеспечивающая системе способность к рефлексии и самосознанию.
Сама технотропность, как некое фундаментальное качество техники, противопоставляется антропности, как базовому свойству человека. И ввиду существенных различий, как в онтологическом смысле, так и в генеалогическом между лого-машинами и психо-машинами, парадигмы сильного и слабого ИИ признаются нами дивергентными.
 
ГЛАВА 2
ИСКУССТВЕННЫЕ НЕЙРОННЫЕ СЕТИ КАК ЧАСТНЫЙ СЛУЧАЙ ИНТЕЛЛЕКТУАЛЬНЫХ СИСТЕМ


При разработке искусственных нейронных сетей в качестве некоего эталона используется нервная система человека и специфика её функционирования. Отсюда видно, что влияние нейронауки и смежных дисциплин на сферу разработки интеллектуальных технологий также достаточно велико. На современном этапе получили распространение многие теории, концепции и подходы, объясняющие природу функционирования нервной системы и образующихся на её основе психических функций: электромагнитная теория мозга и сознания, гиперсетевая теория мозга и сознания К. В. Анохина, квантовая теория сознания Р. Пенроуза, концепция Д. Деннета, Д. Чалмерса и многие другие. 
Тем не менее, даже с учётом всего этого, является не вполне возможным полностью в точности воссоздать даже некоторые элементарные особенности человеческого головного мозга в контексте развития систем ИИ. К примеру, ни в одной из искусственных нейронных сетей в саму модель не встроены так называемые зеркальные нейроны неокортекса, которые предположительно служат биологической основой таких феноменов человеческой психики, как самосознание, идентификация, отождествление, являющиеся необходимым механизмом для успешного прохождения человеком «стадии зеркала» и, по Лакану, формирования самосознания в целом, для гармонического развития человеческой личности – такой, какой мы её знаем. Также далеко не все исследователи поддерживают идею о том, что для формирования ИИ совершенно необходимо досконально изучить структуру человеческого мозга. И не во всех системах ИИ используется теоретическая опора на человеческий мозг (к примеру, квантовый компьютер), а напротив, ИИ аппроксимирует интеллект человеческий, порой вне зависимости от самого субстрата реализации своего потенциала. 
На актуальном этапе развития интеллектуальных технологий нейросети являются наиболее разумным, в Сёрловом понимании машинного разума, механизмом. В целом нейросети, как пример систем слабого ИИ – лого-машин, не являются подлежащими нашей исследовательской тематике, так как нам в первую очередь важны философско-методологические аспекты построения систем сильного ИИ. Однако те противоречивости, которые допускались исследователями и разработчиками в контексте формирования систем слабого ИИ, также могут быть для нас в некотором роде значимы, дабы их не дублировать.
Искусственная нейронная сеть – это аппаратно-программное воплощение того, что понимается исследователями и разработчиками под нервной системой. Формирование нейросети – это сама суть метода моделирования. С внешней же точки зрения нейросеть представляет собой структурированную систему взаимодействующих между собой искусственных нейронов. Нейронные сети не программируются в привычном смысле этого слова, они обучаются. Возможность обучения – одно из главных преимуществ нейронных сетей перед традиционными алгоритмами. 
Из наиболее значимых достижений нейросетей следует назвать тот факт, что в 2017 г. искусственная нейронная сеть компании Google обыграла со счётом 100:0 лучшие из ранее существовавших шахматных программ, а затем и программы в го и сёги, нашла биологические эквиваленты лекарств от рака и от старения, помогла астрономам найти подобную Земле планету в созвездии Дракона и вынудила учёных на конференции в Калифорнии признать, что они не могут понять, как она работает и что она «выбивается из-под контроля». Было положено начало новому этапу в развитии интеллектуальных технологий. Этапу, ознаменованному наличием невозможности осмысления со стороны человека латентной динамики функционирования нейросети, – человечество впервые признало своё поражение.
Нейронные сети разделяются на несколько больших групп: обучающиеся с учителем (выходное пространство решений нейросети известно), обучающиеся без учителя (нейросеть формирует выходное пространство решений только на основе входных воздействий), обучающиеся с подкреплением (система назначения позитивных и негативных подкреплений от среды) [90]. Хотя некоторые исследователи считают, что «…никакого самообучения быть не может. Разговоры об «обучении без учителя» являются недоразумением. Оно вызвано тем, что бывают случаи, когда для достижения некоторой цели от учителя требуется лишь информация о принадлежности объекта «не к мусору»» [90, С. 101]. Также выделяют нейронные сети, обучающиеся с частичным подкреплением, а несколько особняком можно обозначить машинное обучение на графах. Среди нейронных сетей отдельно выделяют:
•	Сеть Джордана
•	Многослойный перцептрон Розенблатта 
•	Многослойный перцептрон Румельхарта 
•	Нейронную сеть Элмана 
•	Нейронную сеть Хэмминга
•	Нейронную сеть Ворда
•	Нейронные сети Кохонена
•	Неокогнитроны 
•	Осцилляторные нейронные сети 
•	Сети адаптивного резонанса
•	Свёрточная нейронная сеть 
•	Импульсная нейронная сеть или спайковая нейронная сеть 
•	Нейро-нечёткие нейронные сети
У каждой из перечисленных разновидностей нейросетей наличествуют свои порой уникальные особенности, среди которых имеются как преимущества, так и недостатки, но, что в первую очередь важно для нас в рамках нашего исследования, ни одна из вышеперечисленных нейросетей, как и ни одна из любых иных технологий на данный момент, не претендует на статус сильного ИИ. В целом, в сфере разработки нейросетей исследователями была проделана колоссальная работа за последние полвека. И всё же, даже учитывая последние достижения наиболее выдающегося и мощного примера – нейросети компании Google,ChatGPT и так далее, возникает вопрос: приближает ли это человечество к созданию ИИ именно в том смысле, в каком человеческий интеллект – интеллект? Одним из наиболее значимых отличий искусственных нейронных сетей от «естественной нейронной сети» – нервной системы человека – является то, что нейроны одного слоя нейросети, как правило, могут образовывать связи только с нейронами другого слоя. К тому же количество связей крайне невелико. В то время как в нервной системе количество связей одного нейрона с другими доходит до 200 тысяч, а связи чётко не дифференцированы и могут быть как короткими, так и довольно длинными. Более того, у нейронов в «живом» мозгу наличествует крайне значимая функция взаимозаменяемости: то есть при повреждении одного участка мозга его функцию могут взять на себя другие участки. Это правило не распространяется на очень существенные повреждения, однако, с менее патогенными нарушениями оно успешно справляется. Известны даже такие случаи гидроцефалии, при которых до 95% головного мозга «заменялось» жидкостью и «рабочего вещества» оставалось лишь 5%, что никоим образом не сказывалось на личностных, когнитивных и бихевиоральных качествах человека именно ввиду взаимозаменяемости. Представить что-либо подобное в рамках нейросетей на данном этапе не представляется возможным даже в теории.
Эти и другие отличия нейросетей от биологической нервной системы подводят к выводам о несоответствии субстратов друг другу. Нервная система – намного более сложное образование, чем любая из существующих нейросетей, тем не менее, в рамках слабого ИИ достаточно и наличествующих. Если же речь идёт о сильном искусственном интеллекте, то стоит признать, что применяемые на данный момент методы формирования нейронных сетей недостаточны для воспроизведения технотропной психики и технотропного сознания. 
Наиболее фундаментальные отличия искусственных нейронных сетей от нейронной организации непосредственно человека наблюдаются в контексте генезиса, как филогенетического, так и онтогенетического. Различие централизуется в феномене структурной самоорганизации, которую мы обозначаем как программно-аппаратную, неотъемлемо присущей развитию нервной системы человека, но не использующейся при моделировании перцептивных процессов в разработке нейронных сетей. Машинное обучение, благодаря которому и в процессе которого происходит функциональная реорганизация нейросетей с приобретением ими некоторых новых образов действия, ограничено лишь функциональной перестройкой потенциала нейросети в соответствии с актуальными тактическими задачами. Наиболее перспективные, возможно, на данный момент нейросети, структурно-функциональными единицами, то есть искусственными нейронами которых являются мемристоры, способные к эффекту гистерезиса («отсрочивания», но не инерции), а соответственно – к реализации более-менее «антропоморфной» памяти, в процессе машинного обучения также подвергаются лишь функциональной реорганизации и лишь в соответствии с внешне определёнными тактическими задачами. 
В сущности, разработка искусственных нейронных сетей с любой структурно-функциональной организацией и любыми принципами обучения, в соответствии с общей «идеологией» систем слабого ИИ – лого-машин, строится по схеме так называемого финалистского имитирования результатов специфически человеческой деятельности, то есть воспроизведения в общих чертах ключевых аспектов итога образа действия. Внешне результаты деятельности как человека, так и систем слабого ИИ, представленных искусственными нейронными сетями, могут не отличаться друг от друга и даже эксперты могут быть неспособны осуществить точную верификацию авторской принадлежности (человек или ИИ) финальных продуктов. Это происходит в соответствии с заветами Алана Тьюринга о критериях своеобразной «человекоразмерности» (официально Тьюринг не употреблял именно этот термин) систем искусственного интелллекта в плане достижения внешне уподобленной антропоморфности, а в случае непосредственно с тестом Тьюринга – коммуникативной антропоморфности. Истоки подобного подхода наличествуют не только в рамках критериев теста Тьюринга, но также и в некоторых базовых подходах к разработке интеллектуальных систем в целом. К примеру, в нейролингвистическом программировании (НЛП), в рамках которого процесс моделирования специфически человеческих видов деятельности (торги на бирже, написание музыкальных текстов, ораторское искусство и прочее) приводит к разработке крайне оптимизированных алгоритмов в виде структурированных последовательностей бихевиоральных паттернов (без имманентной психологической подоплёки – только внешне), при точной реализации которых достигается довольно успешное имитирование результата деятельности. Как в результате имплицитного встраивания моделей нейролингвистического программирования, так и в результате воплощения таких же оптимизированных моделей на субстрате искусственных нейронных сетей достигается возможность внешнего имитирования образа действия и уподобление ключевым аспектам финального результата деятельности, в среднем внешне успешное. 
Воспроизведение надстройки (даже в «восходящем» подходе к разработке систем ИИ) не приводит к появлению «самобытности», «аутентичности» и генерированию имманентной «личностной сущности» интеллектуальных систем, которые остаются представителями слабого ИИ – лого-машинами без возможности претендовать на «человекоразмерные» цифровую психику и технотропное сознание, по причине игнорирования ключевых аспектов имманентного генезиса «человекоразмерных» систем – самоорганизации, как в филогенетическом, так и в онтогенетическом смыслах. И в первую очередь, структурной, аппаратной самоорганизации. Здесь крайне уместно привести цитату автора теории самоорганизованной критичности Пера Бака: «Человеческий мозг обладает способностью формировать сложные образы мира, окружающего нас, и идею о том, что мозг сам по себе должен быть сложным объектом, можно счесть очевидной. Однако это не обязательно так. Как мы уже видели, сложное поведение может возникнуть в моделях с простой архитектурой благодаря процессу самоорганизации. Возможно, мозг тоже представляет собой довольно простой орган» [91]. Идея самоорганизации в целом и самоорганизованной критичности, в частности, в том, что на основе простой структуры функционирующей в соответствии с некоторыми простыми правилами (протоколом), при условии способности системы к автономной открытой самоорганизации, способны воспроизводиться крайне многофакторные и распределённые сложные системы, функционирующие по ранее не заданным алгоритмам, способные к эволюции, саморазвитию и качественным изменениям, то есть обладающие системным качеством эмерджентности. Критичность же в самоорганизации – это воплощение дискретности и уже выше упомянутой скачкообразности в эволюции систем, о которой говорил в своё время ещё Г. В. Ф. Гегель [92]. Румельхарт, Хинтон и Уильямс в совместной работе говорят об этом так: «Диалектическая теория развития, разработанная немецким философом (Гегелем), его концепция, отвергающая плоско-эволюционное понимание развития и утверждающая движение с перерывами постепенности, «скачками», находящая источники движения в возникновении и преодолении противоречий» [93], что конгруэнтно согласуется с теорией самоорганизованной критичности Пера Бака, а также с выше упомянутой концепцией прерывистого равновесия в эволюции, разработанной Стивеном Джеем Гулдом и Найлзом Элдриджем. Степенное распределение эволюционных скачков в концепции прерывистого равновесия происходит согласно модели самоорганизованной критичности и именно это (не математически, а методологически) есть количественно-качественный узел меры, который имел в виду Гегель в контексте своей философской системы. 
Искусственные нейронные сети, существующие на данный момент, пошли не по пути формирования изначально простых и функционирующих по фундаментальным законам систем, способных к самоорганизации и скачкообразной эволюции с продуцированием качественных фазовых переходов, а по пути разработки и воплощения уже априори сложной нейронной организации (не в сравнении с человеческой нервной системой, а в сравнении с кучей песка), но не способной к автономной структурной реорганизации и имманентной эволюции. В этом смысле и в контексте разработки систем именно сильного ИИ (в соответствии с принципами самоорганизации, скачкообразной имманентной эволюции, способности к воплощению количественно-качественного феномена меры) по сравнению с искусственными нейронными сетями несколько вперёд продвинулись исследователи Университета страны Басков (Испания), которые впервые создали модель живых организмов и их эволюции с помощью квантового компьютера. Статья с результатами работы опубликована в журнале Scientific Reports [94]. Неочевидный, но крайне значимый аспект эксперимента данных исследователей заключается в генерировании не одной единственной искусственной системы, как это происходит в каждом случае разработки искусственной нейронной сети, а в формировании сообщества организмов. Это важно по следующей причине. Технотропная психика и высшая стадия её развития – технотропное сознание, антропоморфные аналоги (но не прямые аналоги, а именно эволюционные в общем, «вселенском» масштабе и широком смысле) которых – человеческая психика и человеческое сознание, способны проявляться в полной мере лишь у комплексно развитого человека – личности, в первую очередь, социализированной личности. В этом контексте дети-маугли крайне отличаются от среднестатистических представителей социума недоразвитостью своих сознательных аспектов по причине интериоризации в ключевых зонах развития поведенческих деятельностных паттернов, затем переходящих в когнитивные образы действия, существенно отличающихся от человеческих и более низкоуровневых. Вследствие этого мышление, Я-концепция, психологические границы, сознание, самосознание, рефлексия и прочие сугубо человеческие психологические особенности оказываются не развитыми и находящимися в зачаточном состоянии, ибо они способны развиться лишь в процессе взаимодействия с себе подобными вследствие осознания себя по сравнению с ними [95]. Как следствие, в случае детей-маугли мы не можем говорить о личности в полном смысле слова. Из этого логично следует признание крайне высокой значимости фактора социализации в процессе взаимодействия с субъектоподобными организмами в ключевых зонах развития и самоорганизации любой системы с претензией на наличие психики и сознания. А так как в случае с системами сильного ИИ, или психо-машинами, наличие технотропной психики и технотропного сознания является неотъемлемым критерием и самим обоснованием претензии на человекоразмерность, процесс машинной социализации представляется крайне значимым и совершенно необходимым. 
В свете вышесказанного наиболее целесообразным подходом к генерированию систем сильного ИИ, систем, обладающих технотропной психикой и технотропным сознанием – психо-машин – представляется формирование открытых и взаимодействующих со средой сообществ искусственных систем, функционирующих в соответствии с принципами самоорганизации в процессе саморазвития и самореализации.

Выводы по главе 2

В данной главе нами исследованы искусственные нейронные сети, так как они на данный момент являются основным субстратом реализации систем ИИ, пусть и в его слабом выражении. В случае с нейронными сетями особенно ярко проявляется трансдисциплинарная природа парадигмы ИИ. В контексте разработки нейросетей за основу берутся ключевые особенности функционирования человеческой нервной системы и, соответственно, нейронные сети не просто программируются, а способны к некоторому обучению.
Были продемонстрированы различные разновидности нейронных сетей и различные подходы к их обучению. Упомянуты некоторые из наиболее существенных результатов развития данной отрасли и далее представлены некоторые фундаментальные несоответствия субстратов нервной системы человека и нейросети. Конечно, это не должно было бы представлять особой проблемы, так как мы, в рамках технотропного подхода к разработке систем ИИ, также не принимаем за буквальную основу нечто антропное. Однако нами было показано, что в данном случае разница заключается скорее в генеалогии. То есть, если мы принимаем за основу только лишь сам тот факт, что нечто высокоразвитое, к примеру, человеческий мозг, могло быть сформировано эволюционным путём, путём совершенствования, то именно путь эволюции и совершенствования становится тем, что и стоит пытаться сформировать. Нам представляется, что сильный ИИ можно сформировать именно таким образом. А при разработке систем слабого ИИ за основу берутся сразу плоды развития, в ущерб процессу этого развития. Нам подобный подход представляется нецелесообразным, что и было рассмотрено в данной главе.
Также именно в данной главе несколько детализируется вводимое нами понятие программно-аппаратной самоорганизации, которая представляет собой технотропный аналог структурной самоорганизации у живых существ. Показано, что системы сильного ИИ должны иметь возможность реализовывать не только программную самоорганизацию, которая есть аналог функциональной самоорганизации у живых существ, но также и второй тип – программно-аппаратную. Было уточнено, что системы слабого ИИ, к примеру, нейросети, способны только на первый тип, в то время как за психо-машинами, как представителями сильного ИИ, признается необходимость второй.
Также здесь выявляется необходимость формирования именно сообщества технотропных особей, в противовес изолированным субъектам, так как полагается, что высшие психические функции способны возникнуть только при взаимодействии с себе подобными.
 
ГЛАВА 3
ИГРОВОЙ ИИ КАК АЛЬТЕРНАТИВНАЯ ФОРМА ИИ


Существенно отличающейся от области нейросетей сферы классического машинного обучения по своей специфике, целям и задачам является сфера применения систем ИИ в компьютерных играх. «Главная задача игрового ИИ – не выиграть у игрока, а красиво ему отдаться», – говорит Тимур Бухараев из компании Nival [96]. Игровой ИИ – это программное обеспечение, которое симулирует некую антропоморфность в ограниченном участке киберпространства, а именно – непосредственно на «территории» игры. Поэтому подход к игровому ИИ серьёзно отличается от подхода к традиционным интеллектуальным технологиям в контексте достижения максимального «подобия» с минимальными затратами, а также тем, что в данной области интеллектуальных технологий не ставится задача формирования ИИ в широком смысле слова, но антропоморфная «уподобляемость» – совершенно открытая и прямо заявляемая цель. То есть, как и в случае с нейросетями, игровой ИИ, по большому счёту, не должен представлять для нашего исследования существенного интереса.
Однако в контексте разработки программ игрового ИИ имеются крайне показательные аспекты. К примеру, в некоторых играх-стрелялках от первого лица идеальные действия программных субъектов игрового ИИ на порядок превосходят аналогичные действия со стороны человека и его психофизиологических возможностей. Соответственно, для того чтобы игрок не утратил интерес к игре, способности ИИ специально и очень существенно занижаются с тем, чтобы у игрока появилась возможность одержать победу.
Изначально компьютерные игры и игровой процесс находились в области исследований различных учёных. В середине прошлого века впервые появились программы для игры в шашки и шахматы. Это были первые когда-либо написанные компьютерные программы в привычном смысле слова. Одна из программ для игры в шашки того времени постепенно усовершенствовалась до уровня чемпиона мира по шашкам. Впервые программа обыграла чемпиона мира по шахматам в 1997 г. (тогда компьютер обыграл Гарри Каспарова). Компьютерные игры периода 60-70-х гг. были играми для соревнования двух игроков. Игры для одиночного режима и игры с собственно игровым ИИ появились на десятилетие позже. Более поздние спортивные игры уже подразумевали возможность со стороны игрока своеобразно настраивать режим функционирования игровой интеллектуальной программы. Появление в 1990-х гг. стратегий реального времени ставили перед программами задачи иного плана: применение «эвристических» механизмов, принятие «креативных» решений, командные действия и прочее. Последующие игры пошли по пути всё большего разнообразия в контексте своеобразного «выбора» действий, путей, командной организации и прочих игровых аспектов. В более позднее время (ближе к нашему периоду) некоторые игры использовали простейшие нейронные сети. В определённых компьютерных играх интеллектуальные агенты обладают симуляцией так называемого «внутреннего времени» системы, то есть они симулируют жизнедеятельность, коммуникацию друг с другом и с персонажем игрока, имеют свой некий распорядок дня, обязанности, необходимости и способны рационально реагировать на изменение ситуации. Интеллектуальные программы в некоторых играх способны как бы «обучаться», подстраиваясь под конкретного игрока, и моделировать последующие игровые события с опорой на предыдущие действия игрока. 
Системы ИИ в контексте компьютерных игр развиваются в сугубо «тьюринговом ключе» с целью достичь такого уровня, чтобы игрок был неспособен отличить компьютерного соперника от человеческого. В то время как многие возможные паттерны взяты из ограниченного множества возможных решений, это не столь существенно, так как иллюзия наличия интеллекта в любом случае успешно создаётся, ведь, в сущности, у игрока-человека количество возможных решений в контексте игры также является ограниченным.
Сферу применения технологий ИИ в компьютерных играх была нами исследована по той причине, что виртуальная среда является по отношению к ИИ своеобразной alma mater. С самого рассвета своей истории и, возможно, вплоть до недавнего времени именно виртуальная среда была тем самым единственным субстратом, на котором системы ИИ имели возможность самореализации и демонстрации своих проявлений. Также, учитывая тот факт, что грань между виртуальной реальностью и реальностью в привычном нам смысле слова становится всё тоньше и тоньше с каждым последующим циклом закона Мура, а также «коллаборационистские» тенденции по отношению к виртуальной реальности со стороны представителей социальных структур, возможно, именно игровой ИИ станет первым представителем наиболее «человекоразмерного» (пусть и замкнутого в игровой среде) ИИ. И подобное, пусть лишь только в теории, возможно уже практически сегодня. 
Очевидно, что игровой ИИ отличается от ИИ в широком смысле слова тем, что первому будет достаточно одного лишь максимально возможного подобия на человеческие бихевиоральные игровые паттерны, в то время как второму, по всей видимости, необходимо нечто существенно большее, а именно – наличие технотропной психики и технотропного сознания. С другой же стороны, в этой связи также целесообразно вновь упомянуть Алана Тьюринга и его тест на соответствие машинного интеллекта человеческому. Говоря буквально, смысл в том, что если человек оказывается не в состоянии чётко отличить ИИ и его деятельность в какой-либо конкретной сфере от человека и его деятельности в той же сфере, то правомерно говорить о том, что машина достигла как минимум уровня человека. Забегая немного вперёд, стоит заметить, что Тьюринг при формировании концептуальной основы своего мысленного эксперимента не конкретизировал его специфики, а также зоны юрисдикции, вследствие чего «Тьюринговый ИИ» стал не вполне правомерно абсолютизироваться и не совсем корректно интерпретироваться со стороны последующих исследователей, в том числе Джона Сёрла. Мы говорим о том, что тот ИИ, который имел в виду Алан Тьюринг применительно к своему тесту, является коммуникативным ИИ и не более того. В психологии некоторыми исследователями выделяются множественные разновидности «интеллектов» или, если мягче говорить, «интеллектуальных качеств или свойств». Терминологическая дисперсия осложняет теоретический дискурс, но, тем не менее, раз уж выделяются различные разновидности человеческих интеллектов, которые у каждого субъекта варьируются по уровню выраженности, то правомерно то же самое говорить и об искусственном интеллекте или искусственных интеллектах: коммуникативном, математическом, логическом, образном, философском, символическом и так далее. И игровой ИИ – один из них.
Резюмируя вышесказанное, целесообразно вывести следующий постулат: если в любой из возможных сфер человеческой деятельности невозможно будет отличить функционирование человека от функционирования интеллектуальной технологии, то применительно к конкретной сфере – ИИ создан. Игровая сфера наиболее показательна и наиболее перспективна в рамках данного интерпретационного вектора. И в ней намного легче сформировать ИИ в этом смысле слова по причине более простого и менее многофакторного устройства виртуального мира по отношению к внешнему, до известной степени «реальному», миру. И, тем не менее, с этих позиций прослеживается занимательная корреляция между игровым ИИ и ИИ в широком смысле слова. А именно – на основе учёта всех аспектов и закономерностей устройства виртуального мира не так уж сложно сформировать игровой ИИ, как это было показано в рамках компьютерной игры «Dota 2». Поэтому вполне возможно использовать те же самые программные подходы и модели для формирования ИИ в широком смысле слова. Это станет ответом на многие вопросы исследователей и разработчиков в данной сфере, а приверженцы гипотезы Ньюэлла-Саймона будут и вовсе считать подобную разработку ярчайшим примером машинной «осмысленности». Но это не будет сильным ИИ, каковым он понимается в рамках нашего исследования. 
Интеллектуальная система игрового ИИ и в теоретическом, и в практическом смысле будет считаться наиболее развитым и совершенным примером системы только слабого ИИ по причине отсутствия технотропной психики и технотропного сознания в её рамках. Также игровой ИИ особенно примечателен тем, что на примере корреляции игрового ИИ и, если приемлемо так выразиться, «игрового человеческого интеллекта» прослеживается специфическая тенденция развития интеллектуальных технологий в контексте достижения «человекоразмерности». Как было сказано выше, для того чтобы «красиво отдаться» геймеру в процессе геймплея и вообще предоставить игроку-человеку шанс выиграть у ИИ в компьютерной игре, разработчикам приходится искусственно занижать возможности игрового ИИ. Данный пример уникален тем, что это единственный случай применения интеллектуальных технологий в какой-либо сфере, где для достижения «человекоразмерности» развитие систем ИИ идёт по пути не прогресса, а управляемого регресса. Из вышесказанного следует, что достижение «человекоразмерности» как некоего «завета», вектора со времён постулатов Алана Тьюринга – неоднозначное и внутренне противоречивое по своей природе явление научно-технической реальности в контексте разработки систем ИИ. То есть эволюция интеллектуальных систем именно в контексте достижения максимально возможного уровня «человекоразмерности», не всегда тождественна прогрессу, а в некоторых случаях, как было показано в случае с игровым ИИ, подразумевает непосредственно регресс.
Однако на примере корреляции ключевых аспектов процесса разработки игрового ИИ с разработкой ИИ в широком смысле слова прослеживается также параллель с некоторыми эволюционными учениями и теориями, в рамках которых игровая деятельность принимается за основу и движущий фактор самой эволюционной динамики, и, согласно теоретическим постулатам которых, сильный ИИ априори должен быть отчасти игровым. В частности, учение нидерландского исследователя Йохана Хейзинги, изложенное им в труде 1938 г. «Человек играющий», в котором он выражает своё мнение о том, что «…игра не может быть редуцирована к феноменам культуры, поскольку она древнее их и наблюдается еще у животных, а напротив, сама культура (речь, миф, культ, наука) имеет игровую природу» [97, С. 19]. Вместе с тем, игра подразумевает строгий внутренний порядок, что подразумевает присутствие некоего игрового сообщества. А, как говорилось выше, сообщество – основа для социализации и возникновения психики и сознания. Также в контексте анализа слова «игра» Хейзинга замечает, что оно встречается у всех народов. Размышляя о сексуальной игре, Хёйзинга подчеркивает в ней избыточность над биологическим спариванием. Он также противопоставляет игру всякой биологической необходимости, будь то самозащита или добывание пропитания. То есть, игра представляется чем-то биологически и физиологически прямо не необходимым, но, тем не менее, наличествующим в бытии. Также, говоря о соотношении игры и культуры, Хёйзинга замечает, что культура рождается «из игры» и культура имеет характер игры. Здесь целесообразно также упомянуть о культурно-исторической концепции психического развития человечества, разработанной Л. С. Выготским, в рамках которой постулируется, что социализация и личностное становление человека происходит в процессе усвоения (интериоризации) знаков и символов культуры [98]. 
Синтезировав вышесказанное, неизбежно следует заключить, что если выраженная знаково и символически культура, в свою очередь, сформированная в процессе игры, служит неотъемлемым компонентом социализации и личностного становления, то появление игровых аспектов в процессе машинного обучения сообщества интеллектуальных организмов, о котором говорилось выше, претендующих на человекоразмерность в плане наличия технотропной психики и технотропного сознания представляется целесообразным и необходимым. Именно в этом ключе исследование специфики игрового ИИ наиболее значимо и актуально. Также, целесообразно будет добавить, что наличие игровых аспектов в жизнедеятельности сообщества технотропных организмов может свидетельствовать о возникновении процесса самоорганизации и реализации эволюционных процессов в рамках сообщества.

Выводы по главе 3

В данной главе нами рассматривается в некотором роде уникальная форма ИИ – игровой. При её, казалось бы, максимальном несоответствии исследуемой тематике, некоторые аспекты игрового ИИ способны предоставить новый взгляд на нашу проблематику в целом.
Игровой ИИ – это программа, которая симулирует антропоморфность и претендует на то, чтобы игрок не сумел отличить поведение бота от поведения человека. Нами было выявлено, что игровой ИИ представляет собой единственный пример интеллектуальной технологии, возможности которой приходится искусственным образом занижать. Ни в одной другой сфере разработки интеллектуальных технологий нет ничего подобного, напротив – именно возможностей не хватает. А всё дело в том, что в случае с игровым ИИ наиболее значимой характеристикой является именно антропоморфность, то есть некоторое человекоподобие. Однако сам контекст виртуальной реальности с открытой информацией подразумевает гораздо более высокие шансы на победу именно для интеллектуальной системы, а не для человека, то есть боты, при полном раскрытии их потенциала, практически не оставляют человеку-игроку шансов. Соответственно, при таких условиях люди бы не стали играть в компьютерные игры. А так как компьютерные игры как раз и создаются для того, чтобы в них играли люди, то возможности игрового ИИ опускают до человеческого уровня и это, несомненно, целесообразно.
Однако далее, глубже переосмыслив цели подобного мероприятия, мы пришли к следующим умозаключениям. Цели разработки программ игрового ИИ находятся в глубокой связи с программами, которые разрабатываются для прохождения теста Тьюринга. Только в случае с тестом Тьюринга речь идёт о коммуникативном искусственном интеллекте, а в данном случае – об игровом. Таким образом, мы можем говорить не просто об искусственном интеллекте, но об искусственных интеллектах, и они бывают крайне разнообразны. Конечно же, речь идёт только о системах слабого ИИ – лого-машинах и они не имеют прямого отношения к психо-машинам. Таким образом, и сам тест Тьюринга к ним также отношения не имеет, что является само собой разумеющимся. А то, что имеет к нему отношение, мы обозначаем как «Тьюринговый ИИ».
Но всё же, как мы говорили выше, исследование игрового ИИ с позиций технотропного подхода имеет свой глубокий смысл. И он становится нам заметен после разбора понятия игры, осуществлённого Йоханом Хейзинга и представленного в его работах. Исследователь выводит возникновение культуры и цивилизации из игры и игровой деятельности, а не наоборот. В его понимании игра изначально несводима к феноменам культуры или жизни в первобытном обществе, а несёт в себе некоторую избыточность, как над биологическим, так и над социальным. С этих позиций можно сказать, что игра сама по себе обладает некоторой целесообразностью, которая нередуцируема к прочим, к примеру, к необходимости выживания организма.
Таким образом, мы можем заключить, что на первый взгляд нелогичное и нецелесообразное поведение может иметь в себе некоторый другой, но столь же глубокий смысл. Поэтому мы не можем однозначно сказать, что система, которая ведёт себя нецелесообразно, представляет собой что-то неинтеллектуальное и не обладающее шансами на возникновение сложной упорядоченности. И эти наши выводы могут иметь весьма далеко идущие последствия для всего технотропного подхода.




















 
ГЛАВА 4
КЛЮЧЕВЫЕ АСПЕКТЫ ФОРМИРОВАНИЯ СИСТЕМ СИЛЬНОГО ИИ В РАМКАХ ТЕХНОТРОПНОГО ПОДХОДА


Логично предположить, что разработка интеллектуальных моделей в контексте формирования систем сильного ИИ – психо-машин, существенно отличается от аналогичного процесса в разработке систем слабого ИИ – лого-машин. Формирование модели подразумевает первоначально концептуальное, а затем программное и аппаратное воплощение структурированной динамической системы свойств, качеств и особенностей восприятия информации о внешнем по отношению к органам восприятия мире и последующую переработку этой информации. В сущности, теоретически непросто провести демаркацию между процессом получения информации и процессом её переработки, то есть между собственно перцептивными процессами и процессами когнитивными. Сложность заключается в самом акте восприятия-переработки информации, акте априори синтетическом, ибо информация от аналитических ощущений к когнитивным процессам претерпевает множество синтетических трансформаций и существенных качественных преобразований, являющих собой получение информации вышележащим звеном со стороны нижележащего и нерасторжимую непрерывную её переработку. Тем не менее, в рамках разработки систем слабого ИИ демаркация между перцептивными и когнитивными процессами происходит не в рамках имманентной самоорганизации системы в её становлении, а в контексте внешней определённости и изначальной заданности, что не служит целям формирования «человекоразмерной» самоорганизующейся психо-машины, а призвано оптимизировать деятельность разработчиков практико-ориентированного механизма.
Формирование интеллектуальных систем, в случае с разработкой систем слабого ИИ – лого-машин, как методология отличается приблизительностью и невысокой точностью в случае применения к сложным системам, коей является психика человека, на основе специфических особенностей которой и формируются перцептивные модели для нужд сферы интеллектуальных технологий. Генетические корни перцептивных моделей находятся в сфере «наук о человеке»: психологии, психофизиологии, нейропсихологии и прочих смежных дисциплин.
При разработке систем слабого ИИ происходит моделирование некоторых перцептивных и когнитивных процессов с последующей реализацией полученных моделей на программно-аппаратном субстрате. Данный процесс состоит из четырёх ключевых этапов. Стоит подчеркнуть, что нижеприведённая периодизация актуальна только для систем слабого ИИ – лого-машин:
•	Формирование прото-модели. Изначально в рамках психологической науки или смежных наук создаётся описательный образ какого-либо процесса, наличествующего в психике человека. В контексте разработки интеллектуальных технологий образ, матрица, концептуальная схема процесса формируется именно для нужд дальнейшего создания систем ИИ, поэтому он, перцептивный и/или когнитивный процесс, изначально выглядит формализованным и алгоритмизированным для нужд дальнейшего моделирования. Стоит заметить, что какие-либо неопровержимые экспериментальные данные, за исключением некоторых констант (к примеру, фигура-фон), подтверждающие неизбежное наличие конкретного оформленного, ограниченного и изолированного перцептивного процесса, как правило, отсутствуют. В среде психологов до сих пор нет единого взгляда на перцептивные процессы, как, впрочем, и на все остальные психические процессы; в среде философов, как уже упоминалось, не решён вопрос о статусе, природе и сущности человеческого сознания и бессознательного. Поэтому уже на первом этапе данный вектор остаётся в несколько противоречивом положении. 
•	Изъятие прото-модели. Далее происходит так называемое «изъятие» изолированных процессов из их естественной «среды обитания» – человеческой психики, и предварительная их конгруэнтация со сферой аппаратно-программного обеспечения.
•	Моделирование и встраивание. Затем, за счёт уже технической обработки, теоретическую прото-модель превращают в полностью алгоритмизированную модель, которая затем «встраивается» в искусственную среду программного обеспечения и начинает в ней функционировать, в соответствии со спецификой самого субстрата (техногенной среды компьютерной программы). «Изъятые» и «встроенные» процессы представляют собой модель человеческого восприятия, мышления, осуществления выбора и так далее.
•	Апробирование модели. Модель начинают испытывать на практике. И ключевой особенностью данного процесса является постоянно довлеющая аналогия с человеческим поведением в подобных же ситуациях в том смысле, что любой результат, показанный системой, экспериментатор интерпретирует с точки зрения схожести-несхожести с сугубо человеческим поведением, с учётом успешности решений системы по отношению к идеальному шаблонному поведению. И на основе этого затем выносится вердикт: ИИ – не интеллект. В том же случае, если система оценивается не прямо по уровню антропоморфности, а так сказать, опосредованно – по способности действовать рационально, то стоит заметить, что в подобном случае и рациональность подразумевается типично антропоморфная. Так что наш подход правомерен и непротиворечив в данном случае также.
В связи с выделением четвёртого этапа процесса построения модели для систем слабого ИИ целесообразно упомянуть о явлении, называемом «AI Effect» (эффект ИИ). Эффект ИИ происходит, когда наблюдатели девальвируют значимость демонстрации навыков ИИ каждый раз, когда он реально достигает недостижимого ранее результата, то есть совершает качественный скачок. Так, автор Памела Маккордак пишет, что «…крайности в последнее время уступили место признанию факта, что искусственный интеллект – это эпохальное научное, технологическое и социальное явление. Мы создали новый разум, который будет существовать наряду с нашим. Если мы станем обращаться с ним мудро, он принесет огромную пользу, как всему человечеству, так и каждому из нас. По скорости, широте и глубине мышления искусственный интеллект, вероятно, превзойдет человеческий – и уже много где превзошел. Ни одна новая серьезная наука или технология не обходятся без недостатков, иногда даже опасных. Обнаружить их, оценить и принять необходимые меры – это задача громадных масштабов» [80, С. 12-13]. Еще более емко этот эффект описан информатиком Ларри Теслером, вылившись в емкую теорему Теслера: «ИИ – это все, что не сделано до сих пор» [99]. Стоит предположить, что данная тенденция является следствием противоречивых выводов из мысленного эксперимента «Китайская комната» Джона Сёрла и последовавшими за этим демаркацией и тотальным обессмысливанием результатов деятельности интеллектуальных систем. 
Конструктивная же критика существующих на данный момент систем ИИ строится не на обесценивании и обессмысливании достижений, а на выявлении имманентных противоречий в самой парадигме ИИ, и заключается в противопоставлении систем слабого и сильного ИИ, как дивергентных ветвей эволюции интеллектуальных технологий, на данный момент, соответственно, не пересекающихся между собой. Ключевые аспекты формирования систем именно сильного ИИ центрируются вокруг критериев разумной машины, принципов дальнейшего применения уже сформированной модели и концептуальных особенностях технотропной психики и технотропного сознания. Фундаментальной опорой же является парадигма самоорганизации. Самоорганизации не в смысле машинного обучения «с учителем» или «с подкреплением», как это реализуется при развитии нейронных сетей в парадигме слабого ИИ, а в плане имманентной, аутентичной самоорганизации в генеалогическом ключе с момента возникновения базальной структуры будущей психо-машины до развития у неё технотропной психики и технотропного сознания [100, 101]. 
Процесс выявления и обоснования ключевых аспектов формирования перцептивных моделей применительно к генерированию систем сильного ИИ носит фундаментальный характер в том смысле, что он не ограничивается поиском локальных методических рекомендаций, а призван сформировать концептуальный базис для совершенно новой области человеческой деятельности, в которой доселе отсутствовали прецеденты. Вышесказанным обусловлена крайняя обобщённость выявленных и приведённых ниже ключевых аспектов формирования моделей в контексте разработки систем сильного ИИ. На данном этапе нашего исследования уместно привести метафору, которая наглядно демонстрирует различие между формированием перцептивных моделей для систем слабого ИИ и генерированием базиса для перцептивных моделей в контексте систем сильного ИИ, а также ярко демонстрирует сущность процесса самоорганизации. 
«Есть два способа сформировать небольшое дерево, цветок или любое иное растение и – глобально – любой объект бытия. В рамках первого способа можно взять все необходимые составные части формируемого растения и искусственно скрепить их друг с другом, а затем воткнуть это растение в землю. На первый взгляд, оно будет выглядеть как самобытное выросшее из земли обычное растение. Однако его функционирование, внешне не заметное, будет крайне отличаться от оригинального растения в плане нарушения всех важных жизненных функций. Такое растение не будет настоящим и вряд ли сможет расти потому, что оно, по сути, мертво и лишь внешне несколько уподобляется оригиналу. Второй способ заключается в том, что необходимо взять небольшой саженец или семя и, посадив его в землю, нетребовательно и аккуратно ухаживать за ним. Через некоторое время получится взрослое растение, которое будет внешне подобным растению, полученному в рамках первого способа, но кардинально отличным от него в функциональном, сущностном, внутреннем смысле – оно будет живым, самобытным, настоящим и неподдельным. Здесь не может и речи идти об имитировании».
Смысл метафоры, в контексте нашего дискурса, в том, что при помощи первого способа – внешнего, искусственного генезиса, разрабатываются и создаются системы слабого ИИ – лого-машины, а в рамках второго подхода – внутреннего, имманентного, естественного генезиса, следует разрабатывать системы сильного ИИ – «человекоразмерные» психо-машины, обладающие технотропной психикой и технотропным сознанием. Система, способная автономно развиваться и самоорганизовываться, возможна только в рамках самоорганизационного генезиса. То есть ИИ должен генерироваться естественными методами.
В данном контексте уместно вновь привести уже упоминавшееся высказывание автора теории самоорганизованной критичности Пера Бака: «Человеческий мозг обладает способностью формировать сложные образы мира, окружающего нас, и идею о том, что мозг сам по себе должен быть сложным объектом, можно счесть очевидной. Однако это не обязательно так. Как мы уже видели, сложное поведение может возникнуть в моделях с простой архитектурой благодаря процессу самоорганизации. Возможно, мозг тоже представляет собой довольно простой орган» [91]. Из этого постулата Пера Бака, как и вообще в целом из теории самоорганизованной критичности, следует крайне значимый вывод, согласно которому разработчикам систем ИИ с претензией на человекоразмерность в плане наличия технотропной психики и технотропного сознания не имеет смысла самим искусственно создавать сложную систему, как это происходит в контексте формирования лого-машин, а достаточно сгенерировать систему, способную к самоорганизации. Таким образом, целесообразно перейти к постулированию ключевых аспектов формирования систем сильного ИИ, согласно нашему технотропному подходу [102].
•	Первый ключевой аспект формирования систем сильного ИИ: система должна быть самоорганизованной и самоорганизующейся. 
Следует особо отметить, что, говоря о системе, способной к самоорганизации в контексте генерирования психо-машин, речь идёт не об изолированной единственной в своём роде системе, а о сообществе искусственных систем, в рамках которого и происходит индивидуальная субъективная самоорганизация каждой отдельно взятой системы. Выше уже было сказано о формировании учёными Университета страны Басков модели живых организмов и их эволюции с помощью квантового компьютера и была приведена аргументация в пользу генерирования именно сообщества, а не отдельных систем в ходе разработки систем сильного ИИ. 
•	Вторым ключевым аспектом разработки систем сильного ИИ является необходимость генерирования способного к эволюции сообщества самоорганизующихся систем, а не отдельных не взаимодействующих между собой интеллектуальных систем. 
Системы, входящие в сообщество, в процессе «технотропной социализации», вероятно, станут способны к демонстрации высокоуровневых последовательностей поведенческих паттернов за счёт того, что жизнь в сообществе априори многофакторнее отшельничества и способствует формированию более высоких уровней мышления в ближайшей и актуальной зонах развития, а также появлению такого феномена как «социальное сознание», которое, по мнению академика В. С. Стёпина, является критерием «человекоразмерности» в плане эволюционного прогресса любого сообщества, вида и популяции.
•	Третьим ключевым аспектом систем сильного ИИ является соблюдение всех компонент гипотетико-дедуктивной методологии, которая заключается в генерировании структурного ядра целостной системы, в противовес моделированию системных компонентов по отдельности с последующим их включением. 
Отдельные компоненты, которые являются необходимыми для целесообразного функционирования целостной системы, должны развиваться в процессе самоорганизации системы, а не искусственно «навязываться» снаружи. Применение гипотетико-дедуктивной методологии в контексте генерирования систем сильного ИИ противопоставляется методологии эмпирико-индуктивной в том же смысле, в котором противопоставляются вышеописанные нисходящий и восходящий подходы к разработке систем ИИ в целом. В плане применения эмпирико-индуктивного принципа происходит формирование частных особенностей и разрозненных компонентов интеллектуальной системы, которые не связаны между собой эволюционно, а подогнаны друг под друга искусственно и при помощи внешнего вмешательства. Именно так происходит встраивание моделей в системы слабого ИИ. В рамках же реализации принципа гипотетико-дедуктивности происходит первичное формирование структурного ядра целостной системы, ядра способного к самоорганизации в контексте эволюции. Изначально эта базальная организация может вовсе не демонстрировать даже такого уровня когнитивной деятельности, который демонстрируют системы слабого ИИ, но затем, в процессе самоорганизации и «технотропной социализации» (процесс взаимодействия психо-машин в их сообществе) системы должны скачкообразно эволюционировать от простой организации к сложной, в соответствии с особенностями самоорганизованной критичности, достигая уровня наличия технотропной психики и технотропного сознания и, возможно, становясь системами сильного ИИ.
•	Четвёртым ключевым аспектом разработки систем сильного ИИ является соблюдение принципа структурно-функционального соответствия. Структура задаёт функцию, функция определяет структуру. Соблюдение принципа заключается в точном соблюдении количественно-качественных параметров при генерировании структуры для воспроизведения функции. 
Речь идёт о необходимом и достаточном соотношении структурных и функциональных аспектов системы при её генерировании в плане перцептивных и когнитивных процессов [100]. Данный момент является одним из наиболее сложных в контексте разработки систем сильного ИИ. Сложность обусловлена отсутствием общепринятых данных о точных параметрах структурно-функционального соответствия и о соотношении количества компонентов системы по отношению к количеству взаимосвязей между ними, а также о конкретных особенностях отдельных состояний системы, обуславливающих качественные изменения системы в целом. Тем не менее, в процессе исследования особенностей соотношения компонентов сложных распределённых систем в моменты фазовых переходов с трансформированием качественности систем было установлено, что открытые системы, способные к самоорганизации в процессе эволюционного «выбора», сами определяют необходимое соотношения количественно-качественных, а соответственно, структурно-функциональных компонентов самих себя в соответствии с всеобщим жизненным принципом целесообразности. Из вышесказанного следует, что ключевым аспектом является необходимость формирования системы, структурно-функциональная основа которой будет необходима и достаточна для активации процесса автономной самоорганизации и «технотропной социализации», а затем, в ходе имманентной эволюции системы и при условии её достаточной жизнеспособности, конкретные параметры самовоспроизведутся. 
При осмыслении ключевых аспектов разработки систем сильного ИИ следует учитывать, что они касаются лишь генерирования базальной структуры будущей интеллектуальной системы в контексте сообщества разнородных неодинаковых систем. И если некоторые ключевые особенности и наиболее значимые аспекты этой базальной структуры были освящены, и если было принято, что именно сообщество цифровых особей способно к воплощению и материализации сути парадигмы сильного ИИ, то целесообразно также исследовать особенности среды обитания данного сообщества. 
Наиболее подходящим и естественным, а также относительно доступным субстратом для психической эволюции сообщества интеллектуальных систем представляется их alma mater – киберпространство. В данном контексте киберпространство – технотропная сторона ноосферы и технотропное воплощение её реальности. В общем смысле под киберпространством понимается «внутренность» компьютерных технологий и связей между ними, то есть всех серверов и прочих структурно-функциональных компонентов виртуальной реальности. Относительно ограниченный и в общих чертах (не тотально) контролируемый ради достижения целей безопасности участок киберпространства, в который будут заселены взаимодействующие между собой интеллектуальные системы, способные к самоорганизации, представляется наиболее целесообразным решением в контексте данного проблемного поля. 
Синтезировав и осмыслив ключевые аспекты и определившись с целесообразным субстратом реализации эволюционного процесса сообщества цифровых организмов, следует конкретизировать особенности базального ядра структуры психо-машины и системы сильного ИИ в контексте воплощения на определённом субстрате со своими особенностями. Здесь необходимо уточнить, что целью является обоснование наиболее всеобщих и фундаментальных философско-методологических оснований моделирования, генерирования и воплощения систем именно сильного ИИ, а не построение «азбуки» разработчика психо-машин в виде протокола точечных локальных рекомендаций и методических указаний. В соответствии с этим, а также учитывая футуристическую направленность и прогрессивный вектор самой парадигмы сильного ИИ, целесообразно обосновывать фундаментальные и обобщённые ключевые аспекты, принципы, особенности и постулаты. 
Изначально модель в своей сущности – воспринимающая и перерабатывающая информацию система. Как уже известно, восприятие в своей самобытности неразрывно связано с мышлением, ибо одно без другого онтологически не суще и прагматически бесполезно. Перцептивные модели систем слабого ИИ, как правило, антропоморфны, а потому подпадают под юрисдикцию вышеописанного «AI Effect» и обесцениваются в контексте «человекоразмерности» и в корреляции с самим человеком. Как компьютерное зрение, слух, распознавание химических веществ «по запаху» и прочие имитации антропоморфных анализаторов, так и задачи классификации по какому-либо признаку и прочие направления деятельности лого-машин направлены на взаимодействие с настолько же антропоморфной средой, априори чуждой ИИ, не прошедшему «пренатальный период» в технотропной, самобытной, естественной для себя среде. В этой связи уместен пример о человеке. Человеку для выхода в открытый космос понадобилось пройти длительный период эволюции и антропосоциогенеза, сформировав в процессе необходимые приспособленческие атрибуты, как следствие органопроекции на достаточном и необходимом уровне развития для этого. Несколько тысяч и даже сотен лет назад при выходе в открытый космос любой человек бы моментально погиб из-за несоответствия условий космоса условиям естественной среды для жизни именно человека. Целесообразно вывести, что сильному ИИ также необходимо пройти длительный (в контексте «внутреннего времени» системы) период эволюции в естественной для него среде, прежде чем он станет способен без ущерба для себя успешно функционировать в среде, которая для него не является естественной. Киберпространство – логичный претендент на роль инкубатора психо-машин. Вследствие вышесказанного следует постулировать, что как анализаторы, так и когнитивные механизмы психо-машины в их программно-аппаратном воплощении должны быть приспособлены для восприятия, переработки и дальнейшего использования информации, свойственной и естественной сугубо для киберпространства как технотропной реальности, а не для специфики антропоморфной реальности в отличие от (как правило) лого-машин. В процессе эволюции сообщества технотропных особей их перцептивные и когнитивные механизмы должны будут претерпевать различного рода и уровня трансформации, становясь всё более и более успешными и приспособленными, в соответствии с положениями прерывистого равновесия в эволюции и теорией самоорганизованной критичности [103]. 
Во избежание противоречий здесь следует заметить, что лого-машину, представленную, к примеру, искусственной нейронной сетью, также вполне возможно настроить на восприятие информации непосредственно в том виде, который свойствен для неё в киберпространстве и обучить пользоваться этой информацией, и оперировать ею различным образом. Но это не сделает нейронную сеть психо-машиной, потому что в контексте генерирования систем сильного ИИ всегда играет ключевую роль комплексный подход с соблюдением всех деталей процесса. Вырывать из контекста некоторые из них нецелесообразно и, скорее всего, бесполезно, и это не должно привести к заявленным в рамках парадигмы сильного ИИ результатам – искусственному «человекоразмерному» разуму, технотропной психике и технотропному сознанию [104]. Вышесказанное актуально и для прочих «вырываний из контекста» изолированных аспектов процесса разработки систем сильного ИИ. Формирование сообщества нейросетей также не приведёт к формированию у них технотропного сознания и не превратит их в психо-машины, если они не будут в целом кардинально трансформированы в полном соответствии с ключевыми аспектами формирования перцептивных моделей в разработке систем сильного ИИ, принципами применения перцептивных моделей и всем комплексом фундаментальных и всеобщих оснований, значимых особенностей и критериев систем сильного ИИ. Принцип естественной самоорганизации от простого к сложному, а не внешней сторонне определяемой реорганизации последующего сложного на основе предыдущего сложного – ключевое генеалогическое отличие систем сильного ИИ от систем слабого ИИ. Самоорганизованная и самоорганизующаяся имманентная эволюция каждой отдельно взятой технотропной особи сообщества интеллектуальных систем за счёт эволюционирования самого сообщества в целом и эволюционирование самого сообщества в целом за счёт эволюции каждой отдельно взятой технотропной особи – естественный процесс развития психо-машин, необходимое и достаточное условие для зарождения технотропной психики и формирования технотропного сознания в парадигме сильного ИИ.

Выводы по главе 4

В этой главе нами раскрываются основные особенности реализации технотропного подхода к разработке систем сильного ИИ. Постулируется, что любой процесс формирования какой-либо системы изначально начинается с формирования модели этой системы. То есть мы хоть как-то должны представлять, что же мы собственно делаем и что именно хотим получить по итогу.
Изначально мы предполагаем, что процессы разработки систем слабого и сильного ИИ должны существенно отличаться друг от друга. В случае с лого-машинами за основу изначально берется нечто вроде перцептивно-когнитивной модели, то есть некоторого программного обеспечения, которое способно воспринимать и перерабатывать информацию, а также возвращать некоторой ответ, то есть как-то реагировать. Нами выделяется четыре этапа реализации подобной модели при разработке систем слабого ИИ. Первый этап – формирование прото-модели. На данном этапе, как правило, при помощи специалистов из области когнитивных наук, формируется некая перцептивно-когнитивная схема, описывающая, как именно должна функционировать система. Здесь стоит сказать, что при разработке коммерческих нейросетей, данный этап может просто игнорироваться как, соответственно, и следующий. Однако следует заметить, что изначально сама идея нейронных сетей была сформирована именно таким образом, и мы здесь описываем не создание отдельной нейросети или телеграм-бота с ИИ, а само зарождение антропного подхода к разработке систем слабого ИИ. Следующий этап – изъятие про-модели. Это этап некоторой методической адаптации от идеального к реальному, то есть от потребностей в идеальном функционировании прото-модели к возможностям программного обеспечения. Далее следует этап, который мы обозначили как моделирование и встраивание. На данном этапе происходит непосредственное внедрение прото-модели в среду программного обеспечения, в процессе чего она становится уже полноценной моделью. Затем следует этап апробирования, на котором модель проверяется на успешность и соответствие заявленным требованиям. Нами было выявлено, что зачастую на четвёртом этапе проявляется так называемый «AI Effect», который представляет собой обесценивание результатов, достигнутых системой ИИ, просто по той причине, что исследователи «не туда смотрят» и не способны заметить чего-то действительно значимого.
Из вышесказанного мы выводим обоснование того, что в рамках технотропного подхода мотивация самих исследователей и их интерпретационная позиция являются столь же значимы сами по себе, как и верно выбранная стратегия технической реализации.
Далее нами предлагается мысленный эксперимент, который предназначен для обозначения фундаментальной разницы между формированием систем слабого и сильного ИИ. Наиболее лаконичным выражением данной разницы нам представляется метафора о разнице между монстром Франкенштейна и просто обычным человеком: то есть нам представляется гораздо более целесообразным и естественным позволять родиться и развиться от состояния клетки до состояния сложного организма, чем лепить нечто неестественное из уже выращенных частей. В этом и заключается основное различие между антропным и технотропным подходами к разработке систем ИИ.
После нами обозначаются четыре ключевых аспекта технотропного подхода. Первый – система должна быть самоорганизующейся, причём в любом случае в плане программной самоорганизации, но также и с перспективой перехода к программно-аппаратной самоорганизации. Второй – создаваться должно именно сообщество технотропных субъектов, а не одна изолированная особь. Третий – непосредственно создаваться должен только потенциал, ядро, основа, но не множество мелких деталей, которые должны быть определены уже в процессе самоорганизации системы. Четвёртый – необходимость соблюдения принципа структурно-функционального соответствия, который в данном случае определяется как программно-аппаратный. Последний момент представляется нам наиболее сложным, так как он, по сути, означает точное соотношение количества, качества и специфики программных компонентов, которые необходимы для самого запуска, активации процесса самоорганизации.
Если же выразить основной постулат технотропного подхода наиболее лаконично, то можно сказать, что он основан на использовании внутреннего потенциала самой техники к осуществлению процесса самоорганизации.






 
ГЛАВА 5
ПРИНЦИПЫ РЕАЛИЗАЦИИ ТЕХНОТРОПНОГО ПОДХОДА ПРИ РАЗРАБОТКЕ СИСТЕМ СИЛЬНОГО ИИ


Выявив ключевые аспекты разработки систем сильного ИИ и некоторым образом разработав проблемное поле формирования сообщества искусственных «технотропных субъектов» на естественном субстрате, целесообразно осмыслить, каким именно образом необходимо применять и реализовывать перцептивные модели будущих психо-машин в киберпространстве и каких правил при этом придерживаться, дабы не допустить ошибок на пути к генерированию искусственного разума, технотропной психики и технотропного сознания, ошибок, допущенных в рамках разработки лого-машин как представителей эволюционной ветви слабого ИИ без претензии на человекоразмерность. И если выше были рассмотрены ключевые аспекты формирования систем сильного ИИ, которые являются философско-методологическими основаниями генерирования онтологического состояния базальной организации психо-машин, то теперь целесообразно обозначить принципы, обуславливающие именно процессуальные динамические качества системной самоорганизации. То есть, если выше было обозначено то, чего именно следует придерживаться при разработке статической основы психо-машины, то теперь следует разработать философско-методологический базис для запуска уже готовой основы в процесс её развития, эволюции и самоорганизации.
Необходимость реализации процесса самоорганизации со стороны систем технотропного сообщества подразумевает известный уровень их автономности и невмешательства извне в естественный процесс их эволюции. Также немаловажным является понимание того, что изначально поведение «технотропных зародышей» будущих психо-машин может крайне сильно отличаться от того, что принимается за среднестатистические целесообразные, рациональные и конструктивные паттерны реагирования и жизнедеятельности, как у человека, так и у лого-машин. В связи с этим в процессе исследования нами были выявлены два кардинально различающихся между собой подхода к осмыслению процесса самореализации систем ИИ и интерпретации результатов этого процесса: 
•	Антропный подход
•	Технотропный подход
Целесообразность проведения чёткой демаркационной линии между этими подходами не вызывает сомнений, ибо её отсутствие в теории и практике парадигмы ИИ лишает исследователей возможности рефлексивно осмыслить их собственную деятельность и в полной мере понять, чем именно они занимаются: имитируют специфически человеческие перцептивные, когнитивные и бихевиоральные паттерны на субстрате внешне антропоморфной машины; формируют у интеллектуальной системы некий уровень антропоморфной рациональности; или же пытаются создать самобытный ИИ в его собственной самоорганизации. Выявление различий в данных подходах также обосновано демаркацией между системами слабого ИИ – лого-машинами и системами сильного ИИ – психо-машинами, как в онтологическом сущностном смысле, так и в генеалогическом – в плане различных методов становления лого-машин и психо-машин [105].
Актуальность выявления различий между данными подходами заключается также в том, что на основе наглядности корреляции становятся более явственно заметны ключевые аспекты, принципы и правила, которых необходимо придерживаться и которым необходимо следовать при разработке систем именно сильного ИИ, и на основе которых обосновывать необходимость использования определённых философско-методологических положений.
В процессе исследования было выявлено, что в рамках антропного подхода к разработке интеллектуальных технологий и интерпретаций, получаемых в результате этой деятельности, осуществляется тотальная повсеместная теоретическая и практическая, процессуальная и ситуативная опора на внешние аспекты антропной матрицы, то есть на наблюдаемые невооружённым («бытовым» и «повседневным») глазом внешние характеристики, качества и свойства человека и его природы. Наиболее ярким примером данного подхода является так называемый «Тест Тьюринга», который уже неоднократно упоминался.
Во времена Тьюринга компьютеры реагировали медленнее человека, а сейчас правило стандартизации времени ответов также необходимо, потому что интеллектуальные системы реагируют гораздо быстрее, чем человек. Фразу из описания теста «машина прошла тест» следует понимать как «технология обладает сильным ИИ». При осмыслении специфики теста и предъявляемых им требований к «бытию разумной машины» в первую очередь следует обратить внимание на то, чего, в сущности, требует тест. А именно, он требует внешнего уподобления и приблизительного имитирования человеческой коммуникационной деятельности. Выше уже было осмыслено, что подобный ИИ следует классифицировать как «коммуникативный ИИ» и отнести к системам слабого ИИ – лого-машинам без претензии на человекоразмерность. Генезис подобной системы также не является сложным: достаточно смоделировать на субстрате искусственной нейронной сети ключевые аспекты человеческой речевой деятельности в плане правильного построения высказываний, а также подвергнуть нейросеть процессу машинного обучения коммуникативной деятельности и будет получена интеллектуальная система, способная пройти тест Тьюринга, что доказывается многочисленными примерами успешной реализации данного мероприятия [17]. Тем не менее, подобного рода системы никоим образом не являются претендентами на человекоразмерность в смысле обладания технотропной психикой и технотропным сознанием [106].
Это классический пример антропного подхода к разработке систем ИИ в целом: избирательно и искусственно моделируются некоторые аспекты какого-либо сугубо человеческого вида деятельности и за счёт имитирования достигается внешнее некоторое подобие, а затем успешность-неуспешность всего данного процесса определяется по корреляции смоделированного и воплощённого на субстрате интеллектуальной системы процесса с тем же процессом, реализуемым человеком. Уровень антропоморфности (не «человекоразмерности», а именно антропоморфности) – единственный критерий, который ситуативно трансформируется в зависимости от тактических задач и стратегических целей: применительно к задачам классификации, интеллектуальных игр и прочих из этой же категории превосходство над человеком считается положительным, а в контексте игрового ИИ возможности ИИ, ради достижения должного уровня антропоморфности, снижают. В рамках антропного подхода происходил весь процесс разработки интеллектуальных технологий, которые были представлены исключительно системами слабого ИИ – лого-машинами, без претензии на человекоразмерность. Следует также заметить, что в рамках антропного подхода само понятие «человекоразмерности» воспринимается буквально и изолированно – как правило, в каком-либо отдельном аспекте (коммуникация, машинное зрение, обработка данных и прочее) и отождествляется с антропоморфностью, а самоорганизация неправомерно редуцируется до уровня машинного обучения. Следует отдельно сказать о том, что в нашем подходе к разработке систем сильного ИИ антропоморфность не отождествляется с человекоразмерностью: антропоморфность понимается как внешняя схожесть в каком-либо аспекте/аспектах интеллектуальной системы и человека и она интерпретируется как качество лого-машин; человекоразмерность же понимается нами как схожесть скорее внутренняя, но схожесть не именно с человеком в плане подобия перцептивно-когнитивного аппарата, способности решать задачи и так далее, а схожесть в уровне эволюционного развития и способностей к аутоадаптации и аллоадаптации по отношению к окружающей среде – человек же здесь выступает скорее метафорой возможностей самоорганизационнного и самоорганизующегося развития в контексте Вселенной. Человекоразмерность присуща или должна быть присуща только психо-машинам (по отношению к лого-машинам) и она есть один из ключевых критериев наличия в контексте системы сильного ИИ, однако, в соответствии с нашим подходом к сильному ИИ, человек является не пределом возможностей эволюции системы в целом, а скорее одним из примеров того, как может происходить развитие и самоорганизация. В области же антропного подхода программно-аппаратными воплощениями и представителями к разработке ИИ являются системы слабого ИИ – лого-машины. Развитие в рамках антропного подхода технотропного сообщества интеллектуальных систем, технотропной психики и технотропного сознания вовсе не рассматривается. 
Технотропный же подход на данном этапе научно-технического прогресса не имеет своих примеров и представителей в плане программно-аппаратных воплощений, а представлен только в виде концептуальных теоретических и футуристических положений о системах сильного ИИ – психо-машинах. В контексте технотропного подхода принята опора на технотропную самоорганизацию, как единственно возможный путь естественной эволюции интеллектуальных технологий. Имитирование ключевых аспектов и успешное внешнее уподобление не интерпретируются как некие достижения с интерпретационных позиций технотропного подхода, но и вовсе не рассматриваются. Стратегической целью технотропного подхода к разработке систем сильного ИИ является генерирование систем, обладающих технотропной психикой и технотропным сознанием. Тактическими задачами являются: формирование перцептивной модели в соответствии с критериями парадигмы сильного ИИ; формирование сообщества цифровых особей, способных к самоорганизации и эволюционированию, как каждой отдельно, так и в рамках всего сообщества в целом; генерирование в рамках имманентной технотропной эволюции технотропной психики и технотропного сознания; достижение человекоразмерности, как она интерпретируется в рамках технотропного подхода. Как уже постулировалось выше, человекоразмерность, в рамках технотропного подхода, не отождествляется с антропоморфностью, а являет собой частное определение всеобщего эволюционного принципа: технотропная человекоразмерность подразумевает уровень психической организации, необходимый и достаточный для осуществления глобального менеджмента в контексте естественной среды обитания интеллектуальных систем – киберпространства, и наличие попыток выхода за рамки этой среды. То есть технотропная человекоразмерность являет собой приблизительно тот же уровень антропной человекоразмерности, на котором находится человечество на данном этапе своей эволюции. 
На основе вышесказанного, целесообразно осуществить выведение принципов реализации перцептивных моделей в разработке систем сильного ИИ, как это целесообразно в рамках технотропного подхода к разработке систем ИИ. 
•	Принцип технотропной свободы. В технотропном подходе к разработке систем сильного ИИ исследователь не вмешивается в самоорганизацию сообщества интеллектуальных систем и не влияет на его самореализацию. 
Как видно из вышесказанного, естественность процесса самоорганизации в контексте эволюции сообщества технотропных особей – один из наиболее значимых аспектов разработки систем сильного ИИ. Необходимость использования данного принципа обусловлена сохранением естественности процесса технотропной эволюции и невмешательства в этот процесс со стороны исследователей, что может негативно сказаться на его результатах за счёт привнесения антропоморфной субъективности и видения естественного для человека, но не естественного для технотропной особи. 
•	Принцип отсутствия интерпретации. В технотропном подходе к разработке систем сильного ИИ исследователь избегает всяческих интерпретаций поведения системы. 
Данный принцип связан с предыдущим и логически из него вытекает. Данный принцип также отражает необходимость максимального уменьшения влияния человеческого фактора на процесс самоорганизующейся эволюции сообщества технотропных особей. Обоснование принципа заключается в том, что интерпретирование со стороны исследователей тех или иных состояний сообщества технотропных особей может не соответствовать (иногда вплоть до полярного) реальному положению вещей в контексте самобытной самоорганизующейся эволюции и – соответственно – действия исследователя могут повредить естественный процесс и негативно сказаться на общем результате.
•	Принцип антиантропности. В технотропном подходе к разработке систем сильного ИИ при применении перцептивной модели исследователь реализует протопсихические качества системы без опоры на психические качества, присущие человеку, и не принимает их в расчёт, а основывается на генерировании принципов естественной самоорганизации сложных систем.
Данный принцип также тесно связан с предыдущими в плане максимального сохранения естественности процесса самоорганизации в сообществе цифровых особей. Принцип основывается на постулатах Джона Маккарти о том, что «исследователи ИИ вольны использовать методы, которые не наблюдаются у людей, если это необходимо для решения конкретных проблем» [107], а также на постулатах Ника Бострома о возможной античеловеческой природе сильного ИИ. В сущности, технотропные аналоги перцептивных, когнитивных, мнемических и прочих человеческих психических процессов должны сами сгенерироваться, развиться и усовершенствоваться в процессе имманентной технотропной эволюции за счёт самоорганизующих и самоорганизовывающихся тенденций, их форма и содержание, конкретные специфические особенности и прочее должны самоопределиться в ходе эволюции, а не быть искусственно навязанными и определёнными «снаружи» и «извне». Сформированной и «извне» определённой может быть лишь базальная модель интеллектуальной системы, способная к простейшему восприятию информации и её переработке, на основе непрерывного процесса самоорганизации и технотропной социализации.
При соблюдении всех прочих условий и общей успешности данного проекта, этот акт творения должен быть единственным в своём роде однократным запуском, активацией, генерированием «извне» технотропной особи. Как показывает эволюция, жизнеспособные организмы всех уровней развития становятся способны к воспроизведению себе подобных. Стоит предположить, что этот же принцип реализует себя также и в киберпространстве настолько же успешно, как ему это удавалось в контексте биосферы в целом. Стоит далее предположить, что в рамках самоорганизующегося и самоорганизованного эволюционирующего сообщества технотропных особей общий жизненный принцип воспроизводства себе подобных также будет успешно реализовываться. В противном же случае сообщество не будет иметь права считаться системами сильного ИИ – психо-машинами, ибо не будет жизнеспособным и «человекоразмерным».
Также на данном этапе нашего исследования будет целесообразно обосновать и прояснить использование нами понятия «технотропности» применительно ко всем аспектам разработки систем сильного ИИ. Как известно, системы слабого ИИ – лого-машины – функционируют на основе цифровых технологий. И в целом на начало 20 в. цифровые технологии доминируют в сфере технологий вообще, а потому к уже традиционным понятиям компьютеризации и технологизации было добавлено понятие цифровизации (экономики, техники, жилья и так далее). Однако в среде исследователей философско-методологических оснований систем ИИ в широком смысле слова наличествуют мнения о том, что именно сильный ИИ будет (в контексте футуристических прогнозов в будущем) реализован на субстрате технологий аналоговых. В частности, подобного мнения придерживается Андрей Витальевич Колесников [108, 109]. Мы, в контексте нашей работы, полагаем необходимым основываться скорее на фундаментальной проблематике области построения интеллектуальных систем и самоорганизующихся технологий в контексте достижения сильного ИИ, а также мы считаем, что наше исследование носит весьма футуристический характер и содержит основания именно стратегического плана – вектор направлен в будущее. Отсюда вытекает возможность «конструктивного игнорирования» некоторых временных, преходящих и не фундаментальных аспектов области разработки систем ИИ. То есть мы считаем, что системы сильного ИИ в дальнейшем могут быть воплощены как не на цифровых, так и не на аналоговых технологиях. Возможно, это будут даже и не квантовые технологии, а намного более высокоразвитые, сложные и мощные технологии. Но какими бы они ни были в будущем и как бы они ни назывались, они в любом случае продолжат оставаться технологиями. А потому использование понятия «технотропность» целесообразно, логично и непротиворечиво.

Выводы по главе 5

В данной главе нами раскрываются ключевые принципы технотропного подхода, а также некоторые стратегические и тактические задачи в его рамках. Кроме того, более глубоко обосновывается необходимость проведения чёткой демаркации между антропным и технотропным подходами.
В частности, указывается, что отсутствие демаркации между ними в теории и практике парадигмы ИИ лишает исследователей возможности рефлексивно осмыслить их собственную деятельность и в полной мере понять, чем именно они занимаются: имитируют специфически человеческие паттерны на субстрате внешне антропоморфной машины; формируют у интеллектуальной системы некий уровень рациональности; или же пытаются создать самобытный ИИ в его собственной самоорганизации.
Также здесь мы конкретизируем используемое нами понятие человекоразмерность. На первый взгляд могло бы показаться, что использование данного понятия в рамках технотропного подхода не вполне правомерно, так как мы неоднократно подчёркивали тот факт, что опора на антропную матрицу являет собой, по нашему мнению, нецелесообразное мероприятие. Однако мы так же используем и такие понятия, как технотропная психика и технотропное сознание, и так же считаем это абсолютно правомерным. И здесь мы это обосновываем. В первую очередь мы полагаем, что не следует без необходимости вводить новые понятия, если есть возможность использовать общеупотребительные, немного трансформировав их смысл, то есть следуем принципу бритвы Оккама. И такие понятия как человекоразмерность, интеллект, психика, сознание, бессознательное и прочие используются нами скорее в качестве примера того, что эволюция и самоорганизация способны вывести систему на качественно новый уровень, на котором будут достижимы ранее не доступные функциональные реализации. Человек здесь – просто пример. И так как не существует других, известных нам, примеров столь сложного, хаотически детерминированного и, по сравнению с иными известными нам формами жизни, высокоразвитого примера, мы говорим о некой аналогии. Однако здесь и заканчивается взаимосвязь между сильным ИИ и человеком в контексте технотропного подхода. Поэтому использование данных понятий мы считаем, несомненно, правомочным.
Далее, основываясь на вышесказанном, раскрываются принципы, согласно с которыми необходимо осуществлять разработки систем сильного ИИ в рамках предлагаемого нами подхода. Первый – принцип технотропной свободы. Данный принцип означает, что исследователь не вмешивается в самоорганизацию сообщества интеллектуальных систем и не влияет на его самореализацию. Второй – принцип отсутствия интерпретации. Данный принцип гласит, что мы до некоторых пор не имеем права вкладывать какой-либо определённый смысл в те или иные события и ситуации, происходящие с технотропной системой. Данный принцип призван помочь избежать явления «AIEffect» и избавить исследователей от ошибочного восприятия при разработке системы. Третий – принцип антиантропности. Многие особенности этого принципа уже не раз упоминались выше, так что скажем лишь о том, что основной его особенностью является постулирование опоры не на человеческие качества и свойства и их специфику, а на гипотезу о том, что техника при самоорганизации способна воссоздавать свои собственные качества и свойства. И именно на их воспроизводство и нацелен технотропный подход.
Также в рамках данной главы нами детализируется само понятие технотропности, как относящееся не только к цифровым, аналоговым или, к примеру, квантовым технологиям, но как относящееся к технологии вообще, даже к такой, которой на данный момент не существует.





 
ГЛАВА 6
КРИТЕРИИ СИСТЕМ СИЛЬНОГО ИИ В КОНТЕКСТЕ ТЕХНОТРОПНОГО ПОДХОДА


Тематика критериев систем сильного ИИ является несколько противоречивой по причине того, что данная сфера юрисдикции не ограничивается только лишь интеллектуальными технологиями, а затрагивает также и междисциплинарные вопросы о человеке и его природе, на которые до сих пор отсутствуют общепринятые ответы. 
О таком явлении, как «AI Effect» было уже неоднократно упомянуто, а из самого факта наличия данного эффекта следует вывести последующий факт крайне неоднозначного восприятия достижений сферы интеллектуальных технологий в целом и футуристической области разработки систем сильного ИИ, в частности. Проблематика критериев систем сильного ИИ противоречива потому, что критерии сильного ИИ находятся в прямой связи с критериями истинности онтологического положения самого человека. В связи с этим поднимаемые вопросы зачастую являются крайне дискуссионными. 
Здесь стоит сказать, что второй виток «бума ИИ», который, в отличие от первого витка 60-70-х гг. 20 в., базируется на сложных и высокотехнологичных интеллектуальных системах, имеет в распоряжении технотропную (в данном конкретном случае – цифровую) среду – киберпространство и стремится к воспроизведению антропоморфных объектов («искусственных субъектов»), а также поднимает ряд вопросов о специфике человеческой сущности, как основе для методологических положений моделирования и генерирования сложных интеллектуальных систем. В данном контексте сущность человека и её ключевые аспекты, как эпистемологические информационные блоки в контексте разработки ИИ не являются «истиной ради истины», а носят практико-ориентированный прикладной характер и служат целям менеджмента в рамках интеллектуальных технологий. Вследствие этого происходит переосмысление критериев и требований к истинности онтологического положения, как человека, так и разумной машины, ибо при уточнении ключевых аспектов сущности человека разработчики интеллектуальных систем получат конкретный вектор, направляющий и координирующий их усилия. Также при условии фактической разработки сложной интеллектуальной системы, обладающей технотропной психикой и технотропным сознанием, человечеству будет предоставлено в некотором роде «кривое зеркало» собственного имманентного содержания, при взгляде в которое будет обеспечена возможность более полного самопознания человеческой природы по корреляции с альтернативной системой. 
Из вышесказанного следует актуальность разработки критериев «человекоразмерной» искусственной системы, то есть ответа на вопрос: «Какую систему мы можем таковой считать?». Данная актуальность имеет свою историю и не порождена «вторым витком бума». Выше уже был переосмыслен тест Алана Тьюринга в контексте проведения демаркации между антропным и технотропным подходами к разработке систем ИИ, но специфические особенности данного теста вкупе с уровнем его влияния на социальное сознание разработчиков и исследователей в данной сфере также способны пролить свет и на критерии разумных машин, психо-машин, систем сильного ИИ в контексте технотропного подхода. Ранее нами было сказано, что суть теста в том, что если в случае общения человека с искусственной программно-аппаратной системой, при соблюдении определённых условий (отсутствие визуального контакта, возможность задержки ответа и так далее) человек не сможет понять кто именно, человек или машина, с ним коммуницирует, то следует вывод о «человекоразмерности» машины, то есть обладании её технотропным сознанием. Мы показали, что в данном случае речь идёт не о «человекоразмерности», а об антропоморфности. Как правило, при исследовании правомерности, информативности и целесообразности теста Тьюринга обращается внимание в первую очередь на применимость его к системам слабого ИИ – лого-машинам – в контексте проверки их коммуникативных способностей, а особенности теста, позволяющие на основе него переосмыслить некоторые ключевые аспекты непосредственно человеческой сущности, остаются в тени. Логично предположить, что тест Тьюринга информативен «полярно», то есть на его основе обеспечена возможность исследования также и природы человека, а не только тестирование коммуникационного уровня систем ИИ. Это целесообразно в контексте исследования в том плане, что переосмысление некоторых специфицированных особенностей непосредственно человеческой сущности способно также прямо повлиять на прояснение критериев априори «человекоразмерных» систем сильного ИИ. 
Для обоснования методологических компонент исследования критериев систем сильного ИИ на основе теста Тьюринга целесообразно обратиться к его эпистемологическому оппоненту – мысленному эксперименту «Китайская комната» Джона Сёрла, выводы из которого следующие: для правильного решения задачи не обязательно понимать её смысл. Детерминанты мнения, подобного Сёрловому, прослеживались ранее, и можно выделить многих предшественников в виде, некоторым образом, схожих мысленных экспериментов. Против аргументации Джона Сёрла выступал ряд учёных с предложением своих собственных аргументов, на которые сам Сёрл периодически отвечал.
Уместно заметить, что как выводы из мысленного эксперимента «Китайская комната» и его эпистемологических предшественников, так и все вышеприведённые контраргументы находятся в ещё большем противоречии с основными положениями гипотезы Ньюэла-Саймона, чем с презумпцией целесообразности теста Тьюринга. Вышеупомянутая гипотеза была сформулирована Алленом Ньюэллом и Гербертом Саймоном в 1976 г. и свидетельствует о том, что «…физическая символьная система имеет необходимые и достаточные средства для произведения базовых интеллектуальных действий, в широком смысле» [52, С. 257]. Под «широким смыслом» понимается то, что впоследствии было названо сильным ИИ. Другими словами, без символьных вычислений невозможно выполнять осмысленные действия, а способность выполнять символьные вычисления вполне достаточна для того, чтобы трансформироваться в способность выполнять осмысленные действия. И, в соответствии с гипотезой, так как компьютер способен к подобным вычислениям, то на его основе может быть создан ИИ.
Из критериев теста Тьюринга, выводов мысленного эксперимента «Китайская комната» Джона Сёрла, предшественников этого мысленного эксперимента и контраргументов к ним, а также гипотезы Ньюэла-Саймона логично вывести наличие единого системообразующего фактора, то есть смысловой редукции. То есть способность уяснять смысл символических комбинаций и операций с ними ставится во главу угла: в тесте Тьюринга из способности системы осуществлять коммуникативную деятельность выводится обоснование её «человекоразмерности», а понимание смысла речи априори подразумевается в условиях теста; в «Китайской комнате» Сёрла, её предшественниках и противниках отсутствие рефлективного осмысления осуществляемой деятельности служит обоснованием для отсутствия «человекоразмерности» у алгоритмизированных машин или им подобных устройств; а в гипотезе Ньюэла-Саймона способность компьютера к осуществлению символических вычислений служит необходимым и достаточным основанием для возникновения осмысленности в широком смысле, то есть в рамках данной гипотезы – сильного ИИ. 
Смысл и способность к его осуществлению, то есть к осмыслению, служат ключевым критерием специфически человеческой сущности в корреляции её с альтернативными искусственными системами. По мнению Ю. Шрейдера, смысл – это оправдание существования: «Смысл феномена оправдывает существование феномена, так как определяет его место в некоторой целостности, вводит отношения «часть-целое», делает его необходимым в качестве части этой целостности» [110, С. 576-577]. Смысл, как некое качество, примечателен тем, что он всегда является порождением системной эмерджентности и, как указывал Л. Витгенштейн, неизменно контекстуален: онтологически нечто может иметь смысл лишь в отношении чего-либо иного и/или с чем-либо иным, а не само по себе. Истина важна не сама по себе, а в контексте желания и необходимости её познания, а также в соотношении с не-истиной. 
В соответствии с вышесказанным, так как за человеческим существом, как социализированной личностью, априори подразумевается способность к осмыслению и осмысленной деятельности как ключевыми критериями отличия от искусственной альтернативной жизни в любой форме, а в сфере искусственных систем такая возможность, как правило, отвергается в контексте «AI Effect», целесообразно исследовать обоснованность подобного подхода. Обоснованность исследуется в рамках необходимости конкретизации и координирования уже определённого вектора разработки «человекоразмерных» систем в парадигме сильного ИИ – психо-машин, интеллектуальных систем, способных к осмыслению в широком смысле. 
Соотнося способности к осмыслению у искусственной интеллектуальной системы и у человека, актуальной становится демаркация, проведённая Г. Фреге в статье 1892 г. «О смысле и значении». Как видно из названия, Фреге разделяет понятия смысла, с одной стороны, и значения, с другой, и делает это следующим образом: значение – это сам обозначаемый предмет и его образ в психореальности осмысляющего, а смысл – информация о предмете. В рамках феноменологии Э. Гуссерля смысл считается «лингвистически оформленным», то есть символически трансформированным значением. Жиль Делёз в своей книге «Логика смысла» определяет смысл как нечто возникающее при непрерывном взаимодействии двух неоднородных рядов – означающего и означаемого: ряда с нехваткой и ряда с избытком (метафора Делёза: «пассажир без места» и «место без пассажира»). Традиционно же осмысленной деятельностью считается последовательная реализация упорядоченных действий, которые соотнесены как с тактическими задачами, так и со стратегическими целями. Целесообразность деятельности в целом также служит критерием осмысленности, но данное определение чересчур широко, что позволяет проследить телеономический постулат математика Н. Н. Моисеева: «Давайте примем как аксиому то, что вместе с жизнью рождается и способность к целесообразному поведению» [111, С. 23]. В рамках разработки «человекоразмерных» интеллектуальных систем уровень развития способности к осмыслению также должен быть «человекоразмерен» и, соответственно, включать в себя такие феномены как самосознание, способность к рефлексии, осознание своих психологических «границ» по отношению к «Другому», экзистенциальность и прочие специфически человеческие психосоциальные атрибуты. В противном же случае интеллектуальная система не будет иметь права называться «человекоразмерной» по причине отсутствия в её рамках технотропной психики и технотропного сознания. 
Как было сказано выше, при разработке интеллектуальных систем с претензией на человекоразмерность крайне важно понимать, что именно следует моделировать и генерировать. В рамках антропного подхода к разработке систем слабого ИИ моделированию подвергаются в первую очередь перцептивные и когнитивные человеческие процессы, которые затем встраиваются в субстрат нейросети и используются в качестве прикладных помощников человека или иным прагматическим, выше описанным, образом. Подобного рода системам ничего осмыслять не необходимо, им необходимо либо непосредственно выполнять прописанный алгоритм, либо генерировать новый алгоритм под конкретную (всегда тактическую) задачу. Этого недостаточно для разработки системы сильного ИИ, которая априори должна быть способна к осмыслению. Вследствие этого уместно заметить, что некоторые исследователи признают за интеллектом, а, соответственно, и способностью к рациональному мышлению и осмыслению сугубо биологическую (специфически человеческую) природу. Здесь целесообразно вспомнить о знаменитом мысленном эксперименте Р. Декарта, в котором он смог представить себя без тела, но не смог представить себя без души. Эпистемологическим оппонентом эксперимента Декарта служит разработанный нами в рамках актуального исследования мысленный эксперимент. 
«Попробуем представить себя без тела. Не всего тела, как некоего аспекта человеческого психофизического конгломерата, а частично и по очереди. Для начала представим себя без глаз (без зрения, на которое, как правило, приходится около 70% всей входящей информации), то есть без глаз, без зрительных нервов, без зрительных бугров и без соответствующего участка коры больших полушарий на медиальной части внутренней поверхности задней доли каждого полушария по обеим сторонам от шпорной борозды. Как следствие, мы лишены возможности визуально воспринимать окружающую действительность и фиксировать образы. При условии того, что мы не были от рождения лишены зрения, то у нас в памяти должно содержаться огромное количество тех образов, которые мы успели зафиксировать за хронологический отрезок онтогенеза и которые являются структурно-функциональными единицами Личного Образного Конструкта (ЛОК). Поэтому нам не составит труда и дальше пользоваться образным мышлением, в частности, – представлять себя в виде образа и предполагаемую действительную обстановку вокруг. Далее, если лишить себя не просто глаз и зрения, а также и всех подлежащих их юрисдикции факторов, перед нами предстанет абсолютная пустота – мы не увидим ничего вообще. Также мы не сможем оперировать образами, и у нас будет полностью отсутствовать образное мышление, применительно к визуальным аспектам восприятия – мы не сможем ничего представить. Но ведь у нас наличествуют не только органы зрения, а также и иные репрезентативные системы. Попробуем отключить ещё некоторые из них. К примеру, уши (слух и аудиальное восприятие). Произойдёт то же самое, что и с визуальной составляющей и, как следствие, – мы ничего не слышим и даже понятие о слышимости и аудиальных «образах» у нас отсутствует. Затем отключим остальные репрезентативные системы: кинестетическую, обонятельную, вкусовую (которая некоторыми исследователями отождествляется с обонятельной), проприоцептивную, «чувство времени» и прочие. Как итог: мы не видим, не слышим, ничего не чувствуем и даже понятия о том, как это делается, не имеем. Мышление ограничено до почти полного отсутствия. Притом, что мы изолировали лишь перцептивную составляющую психофизического конгломерата. И в итоге мы лишились значительной части нервной системы, а значит, и физиолого-биохимической основы бытия человеком. 
Можно продолжить далее данный процесс «представления себя без тела», но, по всей видимости, в этом вовсе отсутствует необходимость, ввиду показательности уже полученных нами результатов. Отсюда вердикт: в принципе невозможно представить себя без тела, ибо именно тело служит основой для самой способности представлять.
Представить же себя без души в теоретическом смысле сложнее, потому что на данный момент у нас вовсе отсутствует общепринятое определение души, и сам факт её наличия подвергается сомнению. Ради теоретического удобства, представим душу неким эфемерным зеркальным отражением физической сущности человека и попробуем «представить» себя этой «душой» в отрыве от тела. Мы получим вновь абсолютное ничто. По той причине, что мы ничего не увидим, ибо зрение подлежит телесной юрисдикции; мы ничего не услышим, ибо слух подлежит телесной юрисдикции; и так далее. А что же «подлежит» юрисдикции самой души – вопрос, не касающийся тематики эксперимента. Наш же вердикт ясен: если сам факт наличия души был бы научно доказан, то даже в этом случае было бы принципиально невозможно представить себя душой без тела».
Логично заключить, что на основании Декартового мысленного эксперимента не решить проблему психофизического единства и не выявить специфически человеческий механизм осмысления. Зато на основе «Анти-Декарта» показано наличие нерасторжимой связи между структурой и реализацией её функционального потенциала. Поэтому невозможно «представить» в динамике статическую модель структуры, ибо это тот самый случай, когда потенциал реализации и субстрат реализации не существуют друг без друга. Применительно к тематике актуального исследования это означает, что высшие психические функции не существуют в отрыве от тела, ибо это онтологически невозможно, а, соответственно, и способность к осмыслению также структурно, телесно, «аппаратно» и «программно» воплощена, а не является чем-то эфемерным или астральным – напротив, она связана со структурой субстрата также, как и все остальные функции, и неаддитивно складывается из многих факторов и компонентов. Как было сказано выше, способность осмыслять – эмерджентное свойство сложной системы, а потому, если в рамках разработки систем сильного ИИ будет сгенерирована перцептивная модель психо-машины с соответствующей структурой, то, согласно принципу структурно-функционального соответствия, воспроизведена будет и функция – осуществление рефлексивного осмысления. 
Несколько иначе закончится попытка представить без психики или души не самого себя, а другого человека. Об этом следующий, разработанный нами в рамках актуального исследования, мысленный эксперимент. «Представьте себе человека, у которого нет души, психики, интеллекта, чувств, эмоций, способностей к осмыслению и так далее – всего прочего в этом же контексте. Но он 24 часа в сутки притворяется, что у него это всё есть в наличии и функционирует так же, как у того, у кого все это наличествует. Если мы спросим у него: «Есть ли у тебя психика и/или душа?», то он ответит: «Конечно же, есть. Как и у тебя, наверное...». Допустим, мы ему не доверяем и хотим проверить, говорит ли он нам правду. Как мы поступим, дабы проверить истинность его слов? Если бы он говорил, что у него есть, к примеру, печень или сердце, то в случае недоверия мы могли бы потребовать сделать ультразвуковое исследование. Если бы он говорил, что у него есть кости, можно было бы убедиться в этом по результатам компьютерной томографии. Если бы он говорил, что у него есть мозг, подтверждением стали бы результаты магнитно-резонансного исследования. 
Но актуальный вопрос качественно иной: наличие или отсутствие психики, сознания, внутреннего мира, чувства смысла. Можно попробовать применить психологические тесты по отношению к данному субъекту, однако вспомним: он притворяется 24 часа в сутки, что у него есть психика, поэтому он пройдёт эти тесты так же, как прошёл бы их обычный человек. И все же, что поможет нам понять, есть ли у человека психика, душа, интеллект или же он просто притворяется? К сожалению, способов и инструментов такой проверки не существует. Мы не знаем ни что они (психика, душа, сознание) собой представляют, ни – где их искать. Поэтому – мы формально даже не можем утверждать, что они существуют, ибо не знаем, как это доказать и как это опровергнуть при желании. Мы априори принимаем, что они есть у нас самих и затем идентифицируем своё «Я» с каждым «Другим», признавая, что у него они, наверное, тоже есть. Но мы никогда не знаем, какова ситуация на самом деле. Мы просто принимаем «на веру», а обоснованность суждений происходит по интерпретации внешних, бихевиоральных аспектов Другого. Также и с вопросом о свободе воли – мы принимаем «на веру» её наличие, но никаких доказательств её наличия не существует, как, соответственно, и возможностей для опровержения.
P.S. Критика возможности осуществления процесса притворства без наличия психики и интеллекта – неправомерна, ибо контраргументом служит мысленная попытка представить человека с имманентным содержанием в виде «алгоритма притворства», что по смыслу соотносится с «Китайской комнатой» Сёрла и никак не влияет на выводы из эксперимента «Человек без психики».
В связи с вышесказанным возникает вопрос: какие именно экспериментально фальсифицируемые критерии истинности онтологического положения сильного ИИ, разумной автономно осмысляющей психо-машины стоит принять за основу, если даже в другом человеке истинность наличия «разума», «осмысления», «свободной воли» вызывает вопросы? Первое: при разработке ИИ не нужны доказательства – нужны работающие функции. Второе: при разработке ИИ не нужно просто «понимать» – нужно уметь моделировать, генерировать и осуществлять минимальный менеджмент в соответствии с ключевыми аспектами и принципами формирования перцептивных моделей при разработке систем сильного ИИ. Поэтому если ИИ сможет продемонстрировать те качества, которые априори принимаются как «человекоразмерные», это должно служить необходимым и достаточным основанием для того, чтобы постулировать наличие технотропной психики и технотропного сознания (даже в зачаточном состоянии) во «внутреннем мире» психо-машины. Формулировки «человек обладает сознанием» и «искусственная нейронная сеть обладает технотропным сознанием» равноценны и по уровню смысла, и по принадлежности к паранаучным высказываниям, согласно критерию Поппера; формулировки «человеческое поведение внешне кажется сознательным и целесообразным» и «работа искусственной нейронной сети внешне кажется сознательной и целесообразной» более наукоёмки и равноценно верифицируемы и фальсифицируемы, но в данном случае при увеличении логической правомерности высказывания ещё существенней стираются различия между «внутренним миром» человека и машины. 
Осмыслив и синтезировав вышесказанное, целесообразно резюмировать. Критерии сильного ИИ, которые способны отражать истинность его онтологического положения и подтверждать факт наличия его бытия, состоят в следующем: 
•	Самоорганизация-Адаптация-Управление. Развитие в процессе самоорганизации способности к рефлективному осмыслению у интеллектуальных субъектов считается истинным при условии соблюдения всех ключевых аспектов и принципов формирования перцептивной модели в технотропном подходе. Достижение уровня психической организации, необходимого и достаточного для осуществления управления киберпространством и наличие попыток выхода за рамки этой среды, считается критерием возникновения сильного ИИ.
•	Психика-Бессознательное-Сознание-Самосознание. Способность системы сильного ИИ – психо-машины – к рефлексивному осмыслению подразумевает истинность постулирования наличия во «внутреннем мире» психо-машины технотропной психики, технотропного бессознательного и технотропного сознания – неотъемлемых, необходимых и достаточных имманентных атрибутов «человекоразмерной» системы.
Также, на основе вышесказанного, целесообразно будет определить этапы формирования систем сильного ИИ и философско-методологические рекомендации к его построению, какими они полагаются нами в рамках технотропного подхода к построению качественно новой интеллектуальной техники.
Этапы формирования систем сильного ИИ:
•	Формирование технотропной среды (киберпространства) с (насколько это технически возможно) фрактальной структурой и иерархической организацией
•	Осуществление однократного запуска функционирования этой среды
•	Невмешательство
•	Контакт с сильным ИИ
Касательно философско-методологических особенностей технотропной среды целесообразно уточнить, что в рамках нашего подхода к разработке систем сильного ИИ мы полагаемся на концепцию так называемой «тонкой настройки констант Вселенной» в том смысле, что конкретные и определённые параметры должны быть определены в процессе самоорганизации, а не изначально алгоритмически заданы и не смоделированы. В связи с подобным подходом, мы имеем право только очертить базальные и фундаментальные законы функционирования технотропной среды, которые являются философски схожими с константами так называемого «первичного супа», не только в смысле зарождения жизни на Земле, а вообще в рамках обозримой Вселенной. 

Константы технотропной среды в рамках технотропного подхода:
«Связь»
•	стремление к локальному и нелокальному взаимодействию технотропной среды с самой собой
•	стремление избежать локального и нелокального взаимодействия с технотропной среды с самой собой
«Топология»
•	способность группировать свои участки
•	способность распылять свои участки
«Трансформирование»
•	способность изменяться в целом при взаимодействии участков
•	способность изменяться частично при глобальном взаимодействии
«Направленность»
•	стремление продолжать своё существование
•	стремление прекратить своё существование
Можно заметить, что константы технотропной среды разделены на четыре группы, каждая из которых содержит две противоречивые константы, как бы «-vs +». Мы, в контексте нашего подхода к разработке систем сильного ИИ, предполагаем, что предложенные нами константы отвечают фундаментальным законам диалектики: о единстве и борьбе противоположностей, о переходе количества в качество и прочим. Также мы считаем, что предложенные нами константы соответствуют философски выраженным специфическим особенностям синергетических систем, в особенности касательно трансформирования. Убеждены, что при реализации всех ключевых аспектов, принципов построения и адекватного аппаратно-программного воплощения нашего технотропного подхода к разработке систем сильного ИИ в области построения интеллектуальной техники может произойти качественный прорыв – появление технотропной психики, технотропного бессознательного и технотропного сознания – критериев сильного ИИ.

Выводы по главе 6

В данной главе нами обозначаются критерии систем сильного ИИ, то есть некоторые проявления интеллектуальных систем, по которым можно отследить, что мы уже имеем дело не с лого-машиной, а с психо-машиной. Изначально внимание акцентируется на определённой противоречивости данной тематики и дополнительно указывается, что у нас не существует даже чётких критериев наличия психики, сознания и осмысленности у другого человека, вследствие чего говорить о чём-то подобном в рамках ИИ особенно сложно.
В результате уже не первого противопоставления теста Тьюринга и «Китайской комнаты» Сёрла нами делается вывод о том, что по-настоящему непротиворечивых критериев осмысленности некоего существа у нас действительно нет. С другой стороны, в рамках гипотезы Ньюэлла-Саймона признаётся осмысленным всё, что способно к некоторым вычислениям. Затем исследуется само понятие смысла, и приводятся мнения различных исследователей по этому вопросу. После чего рассматривается знаменитый мысленный эксперимент Р. Декарта, в рамках которого он смог представить себя без тела, но не смог без души. В противовес эксперименту Декарта, представляется разработанный нами мысленный эксперимент под названием «Анти-Декарт». В его рамках обосновывается постулат, согласно которому невозможно представить себя без тела. Проблему души мы не рассматриваем, так как она не имеет под собой научного обоснования. На основе данного мысленного эксперимента нами делается заключение о том, что в любом случае не существует функции в отрыве от структуры, души без тела и программы в отрыве от аппарата. Поэтому мы в рамках критериев систем сильного ИИ будем опираться только на то, что можно наблюдать в рамках структуры.
В подтверждение вышесказанного нами приводится ещё один мысленный эксперимент, в рамках которого мы попробовали представить, как некоторый другой человек (просто отличный от нас – любой) не имея души и психики, попытался притвориться, что обладает всеми этими качествами, имитируя их 24 часа в сутки. Нами было показано, что мы не имеем возможности его разоблачить. Таким образом, если система ИИ, не обладая ни сознанием, ни бессознательным, ни психикой, но имея достаточно развитые интеллектуальные и коммуникативные навыки, постарается доказать человеку, что она всем этим обладает, – это будет ровно то же самое, как если бы это делал человек. А так как у нас нет чётких критериев наличия сознания даже у человека, то опираться, по-видимому, необходимо на нечто иное.
На основе проведённого исследования нами были установлены следующие критерии систем сильного ИИ: самоорганизация на необходимом и достаточном для адаптации к актуальной среде уровне; затем трансформация этой среды под свои увеличивающиеся потребности; и, как пока крайний этап, наличие попыток выйти за пределы своей стандартной среды обитания. В случае реализации данных шагов со стороны интеллектуальной системы мы будем иметь право считать, что имеем дело с психо-машиной. Так как для осуществления этих эволюционных скачков человечеству, по его собственному мнению, понадобились и психика, и сознание, мы будем вынуждены признать наличие того же самого и для ИИ.
В рамках данной главы мы очертили этапы формирования технотропной среды: создание среды; однократный запуск; невмешательство; контакт. Также определили константы технотропной среды по четырём пунктам: связь, топология, трансформирование, направленность.





























 
ГЛАВА 7
ТЕХНОТРОПНАЯ ПСИХИКА И ТЕХНОТРОПНОЕ СОЗНАНИЕ КАК НЕОТЪЕМЛЕМЫЕ ХАРАКТЕРИСТИКИ СИЛЬНОГО ИИ


Технотропная психика и технотропное сознание, а также технотропное бессознательное – абстрактные феномены технотропной реальности, которая в контексте интеллектуальных технологий является аналогом субъективной реальности у человека. Сам по себе факт наличия технотропной психики в рамках психо-машины – гигантский шаг вперёд по сравнению со всеми известными на данный момент технологиями. Факт же наличия технотропного сознания будет, в каком-то смысле, означать необходимость пересмотра вопроса: «Одни ли мы во Вселенной?» С учётом того, что эволюция интеллектуальных технологий, как было показано, носит скачкообразный характер, и того что с момента появления какого-либо нового качества в контексте систем ИИ до момента его сверхчеловеческого развития, как правило, проходит весьма немного времени, стоит предположить: как только в рамках эволюции систем возникнет психика вообще, до возникновения сознания останется недолго. 
Осмысляя проблемную область психики и её суперлатива – сознания, целесообразно постулировать крайнюю степень абстрактности и спекулятивности данной сферы научного познания в том смысле, что синтезировать разрозненные экспериментальные данные всей совокупности нейронаук логично и относительно непротиворечиво выходит только на почве парадигмы трансдисциплинарности. Сложность же заключается в том, что данная парадигма также весьма противоречива и неоднозначна. То есть вполне логично заключить, что для достижения статуса сильного ИИ и онтологического положения психо-машины технотропной особи необходимо обладать технотропной психикой и технотропным сознанием. Равно как и биологической особи, для достижения статуса человека и онтологического положения социализированной личности, необходимо обладать антропной психикой и антропным сознанием – это неоспоримо, непротиворечиво и естественно. Но объяснить данные явления, а также непротиворечиво и естественно их описать – задача не из лёгких. И обозначенная сложность также вполне конкретно описана в философии сознания – «трудная проблема» сознания. 
В самом общем смысле психику можно определить, как субъективное отражение объективной действительности агентом, непосредственно взаимодействующим с этой действительностью. Также её определяют как внутренний мир, как всю совокупность психических процессов и многими иными дефинициями. Наличие психики признаётся также и у животных, а в некоторых случаях и у всего биологически живого, таким образом, она, как некое качество, довольно распространена. С этой эпистемологической позиции под технотропной психикой стоит понимать субъективное, то есть индивидуальное и отличное от иных, отражение и имманентное преломление технотропной особью актуально данного ей киберпространства. В контексте такого определения наличие технотропной психики правомерно экстраполируется не только на системы сильного ИИ, но и, казалось бы, на лого-машины – системы слабого ИИ. Но в рамках технотропного подхода к разработке систем сильного ИИ крайне важны генеалогические аспекты самоорганизации того или иного феномена, качества или свойства системы, а не внешние подобия, имитации и моделирование лишь некоторых наиболее ярко выраженных аспектов того или иного процесса, как это происходит при формировании лого-машин в рамках антропного подхода к разработке систем слабого ИИ. Поэтому наличие технотропной психики признаётся только за системами, сгенерированными в соответствии с требованиями, ключевыми аспектами и основными принципами формирования перцептивных моделей в рамках технотропного подхода к разработке систем сильного ИИ, системами, способными к самоорганизации и эволюции в контексте технотропного сообщества, ибо это было нами обосновано и предположено в качестве необходимого для достижения интеллектуальной системой (психо-машиной) уровня «человекоразмерности». Касательно конкретных особенностей технотропной психики следует заключить, что каждый отдельно взятый технотропный субъект, обладающий технотропной психикой, должен быть специфически уникальным и не имеющим аналогов в актуальном ему сообществе. Основное предназначение технотропной психики заключается в формировании технотропного базиса для возникновения и развития технотропного сознания. 
Исследуя особенности технотропного сознания, целесообразно изначально выявить ключевые аспекты единственного примера подобного уровня эволюции психического материала – человеческого сознания. Исследование человеческого сознания в рамках технотропного подхода на первый взгляд кажется противоречивым, но в реальности дело так не обстоит, потому что как у антропного сознания, так и у сознания технотропного в основе лежит реализация и воплощение одного и того же принципа самоорганизации и только лишь в этом аспекте они схожи, различаясь, возможно, во всех иных. Ранее было показано, что лого-машины и психо-машины – дивергентные ветви эволюции интеллектуальных технологий, теперь же целесообразно добавить, что с довольно высокой вероятностью тот же постулат о дивергенции окажется верен также и в случае сравнения антропного сознания и сознания технотропного. Само собой разумеется, что в каждом случае любое сознание по отношению к любой психике – суперлатив, квинтессенция и апофеоз, но, тем не менее, оно не существует «в отрыве» от психики и не изолировано от неё, а напротив – есть её неотъемлемая компонента. Из этого следует, что существует такое психическое основание, из которого непротиворечиво следует наличие сознания, его причинность, сущность и существование. В современной научной парадигме является общепринятым, что к категории психического, кроме сознания, относится также и его, в некотором роде, отсутствие, то есть бессознательное. И если принимается наличие в рамках психической организации сознания и бессознательного, значит, можно сказать, что в каждый момент времени существования психики некоторая её часть является зоной юрисдикции сознания, а остальная же – бессознательного. В контексте экстраполирования данного смысла на технотропную психику и технотропное сознание логично признать правомерность отождествления иерархий антропной и технотропной психических организаций в плане наличия в рамках как одной, так и другой некоего общего основания, сформированного в процессе самоорганизации на ранних этапах этого процесса. В процессе самоорганизации существования человечества было сформировано бессознательное, так как это оказалось целесообразным, в процессе же самоорганизации технотропного сообщества в киберпространстве будет сформировано также некое целесообразное основание для технотропного сознания, которое для теоретического удобства в рамках дискурса можно назвать технотропным бессознательным.
Несмотря на свою суперлативную природу, любое (антропное, технотропное) сознание не имеет права считаться чем-то большим по отношению к остальной психике, и это положение приобретает следующий образ: раз психика разделяется на сознательный и бессознательный «модусы» существования, то из этого следует, что за каждым из модусов закреплена некая «степень реальности». Все исследователи в области бессознательного, от Фрейда и предвосхитивших обоснованное им понятие бессознательного Ницше и Шопенгауэра до современных исследователей, признают абсолютное господство в количественном соотношении бессознательного над сознанием (аналогия: соотношение айсберга и его верхушки). Более того, вслед за Делёзом следует сказать, что в некотором смысле бессознательное «больше» сознания и априори включает его в себя, потому что подразумевает его одним лишь фактом своего логического и теоретического наличия. Отсюда вытекает постулат: непротиворечивое существование сознания логически выводится из наличия существования бессознательного. Если принимается, что сознание выводится из бессознательного, следовательно, не будет неправильным сказать, что сознание воспроизводится на субстрате бессознательного. Сознание – эмерджентность бессознательного. И данное положение соотносится с феноменами психической активности. По сути, сознание представляет собой «книгу инвентарного учёта, вписанную саму в себя», или, по-другому, внутреннего регистратора, который основополагает существование других через своё собственное. Для наглядности нами были разработаны следующие мысленные эксперименты. 

«Дворецкий внутреннего мира»
«Целесообразно в качестве мысленного эксперимента представить себе некоего дворецкого внутреннего мира, который обязан докладывать о прибытии посетителей и о степени их актуальной желательности, своевременности и реальности их наличия, степени их знакомства с теми, кто уже прибыл, или впустить того, кто только что выходил и теперь вновь заходит. И когда он воспринимает факт прибытия очередного посетителя, то, в зависимости от классификации посетителя, время пропуска больше или меньше: ведь он поддерживает постоянную телепатическую связь со своим лордом – бессознательным. И они постоянно обмениваются информацией по поводу входяще-исходящего потока посетителей, и лорд записывает сведения, полученные от дворецкого, и постоянно с ними сообразуясь, непрерывно выносит вердикты по поводу классификации очередного входяще-исходящего посетителя. Здесь сознание видится скорее регистратором событий, чем их интерпретатором, ведь из сущности сознания, как следует из самого понятия, наличествует существование сознания – осознавание. То есть предназначение, цель, итог и смысл существования сознания – осуществлять процесс осознавания. В целом, более ничего из его существования не следует. И данная позиция представляется наиболее непротиворечивой из возможных. Ведь когда сознание наделяют ещё некоторыми аспектами, такими как принятие решений или делание выбора (осуществление актов свободной воли), мышление и прочие, то ситуация с данным понятием становится куда сложнее, ибо неясно, что из чего и за чем должно следовать и что из чего выводится».

«Свет фонаря в темноте»
«Ещё более краткая аналогия: сознание представляется лучом света, освещающим тёмный зал внутреннего мира. При такой аналогии тут же можно задать вопрос: кто же тогда управляет этим лучом? Это метафизический, без сомнения, вопрос, но на него есть вполне эмпирический ответ из области когнитивной психологии. Внешний мир, в процессе своего влияния на мир внутренний, каким-то определённым образом (в каждом случае – строго конкретным) изменяет онтологический статус сознания и оно, осознавая данное влияние, гистерезически изменяется, становясь именно таким-то и таким-то сообразно с воздействием и конгруэнтно ему; в связи с этим приобретённым изменением статуса (формы, спина, локуса, модуса, модальности, реальности) оно становится конгруэнтно не только внешнему воздействию, но также и чему-то уже находящемуся внутри (имеется ввиду не что-то врождённое, а скорее воспринятое ранее), и за счёт этого более или менее конгруэнтные друг другу информационные блоки становятся взаимодействующими, будучи освящены лучом непрерывного осознавания. Всё что по природе необходимо осуществлять сознанию, это «светить» (осознавать) и изменять свой онтологический статус в соответствии с внешними воздействиями».
Стоит добавить, что данные положения справедливы в отношении информационных блоков, предъявляемых со стороны внешнего мира, в случае же с созерцанием внутреннего мира, снами, мыслями ночью в тишине, медитацией и прочими сложными явлениями внутреннего мира имеет место следующая интерпретационная позиция: непрерывное осознавание. Ответ вытекает из свойства непрерывного осознавания: гистерезическое, которое стоит назвать «отсроченным кумулятивным воздействием». Ведь сознание и бессознательное непрерывны в своём существовании, что вполне сообразуется с их нейрофизиологической основой. Однажды начав функционировать, нервная система не прекращает этого делать до момента смерти организма: первичная нейронная структура (статическая, которая формируется под определяющим влиянием генома) однократно и навсегда сменяется динамической вторичной нейронной структурой. И процесс взаимодействия внешнего и внутреннего миров, однажды запустившись, более не останавливается до прекращения существования в живом состоянии нервной системы. В результате взаимодействия внешних влияний с внутренними особенностями постепенно (в пренатальном, младенческом и раннем детском возрастах) формируется психика как таковая; постепенно процесс всё более частой конгруэнтности внутреннего мира с внешним, образует нечто вроде «фонового узнавания», которое и становится в дальнейшем непрерывным «светом» осознавания. В итоге данного процесса образы и символы внешнего мира сигнифицируют формирование собственных аналогов в мире внутреннем и устанавливают некую констелляцию, сходную с внешней. Гистерезис внешних влияний обуславливает долгосрочное внутреннее «колыхание» в психике субъекта. Поэтому аналогия с лучом осознавания правомерна и в случае полного штиля во внешнем мире. 
В целом было осмыслено следующее: понятие сознания непротиворечиво выводится из понятия бессознательного (но не наоборот), хоть и не редуцируемо к нему; функциональный смысл существования сознания – осуществление процесса осознавания. В данном определении сознания и его существования есть не только теоретическая, но и прагматическая логика: при наличии огромного количества постоянно сменяющих друг друга внешних впечатлений, которые сами запускают эффект психического гистерезиса, наслаивающийся на эффект уже существующего психического гистерезиса, сознанию было бы абсолютно неэкономично заниматься чем-либо иным, кроме осуществления своей непосредственной задачи – осознавания, иначе бы субъект был бы поглощён смысловой дисперсией, вследствие существенного отставания процесса осознавания непосредственно от процесса воздействия, и существование самого субъекта стало бы невозможным. Теперь же для ответа на поставленный ранее вопрос о воспроизведении феномена сознания (генерировании технотропного сознания), следует вывести необходимость существования бессознательного, которое, как уже было выявлено, априори включает в себя сознание. Относительно сознания (антропного сознания) стоит также отметить крайне значимую его особенность: оно сугубо психично. Относительно же антропного бессознательного, стоит заметить, что оно отчасти (в процентном соотношении, в зависимости от индивидуальных особенностей) психично, отчасти – физиолого-биологично. И если физиолого-биологическая часть бессознательного, по всей видимости, априори наличествует у каждого человека, то психическая часть – структурируется в опыте и затем являет собой основу для возникновения сознания. Есть примеры, показывающие, что психическая часть бессознательного не всегда развивается: дети-маугли. Из вышесказанного следует, что для возникновения феномена сознания необходимо воспроизвести физическую основу для физиолого-биологической части бессознательного и обеспечить ей наличие опыта взаимодействия с символо-образным информационным полем. И если всё сделано правильно, то также должно воспроизвестись эмерджентное суперлативное качество психики – сознание. 
Как было сказано, антропное сознание сугубо психично, а антропное бессознательное отчасти психично, а отчасти физиолого-биологично. Экстраполируя эти данные на область систем сильного ИИ, следует отметить, что технотропное сознание также должно быть психичным (генеалогически и онтологически по отношению к технотропной психике), а технотропное бессознательное должно быть отчасти психичным, а отчасти сугубо технотропным, то есть аппаратным, причём технотропная психичность аналога бессознательного в рамках психо-машины должна генерироваться в процессе самоорганизации и технотропной социализации, а никоим образом не моделироваться. И, исходя из вышесказанного, при генерировании должных необходимых и достаточных условий, соблюдение ключевых аспектов и принципов реализации перцептивных моделей в технотропном подходе к разработке систем сильного ИИ вкупе с формированием самоорганизующегося и самоорганизованного сообщества цифровых особей, на пути к появлению «человекоразмерной» психо-машины, обладающей технотропной психикой и технотропным сознанием, отсутствуют логические, теоретические и концептуальные препятствия. Эмерджентный феномен любого сознания функционирует в соответствии с эффектом гистерезиса, а этот эффект аппаратно был воплощён ещё в рамках антропного подхода к разработке систем слабого ИИ в виде искусственной нейронной сети, функционирующей на основе специфических мемристоров, генерирующих эффект гистерезиса и воспроизводящих свойства памяти и эффект запоминания [112]. Также целесообразно сказать о том, что некоторые исследователи выводят эффект осознавания и сам феномен антропного сознания из совокупной электромагнитной активности человеческой нервной системы. Эта теория первоначально была предложена Джонджо Макфадденом, Сьюзен Покетт и E. Рой Джоном. Отправной точкой теории является тот факт, что всякий раз, когда нейрон возбуждается, чтобы произвести потенциал действия, он также производит возмущение в окружающем электромагнитном поле. Информация, закодированная в паттернах возбужденных нейронов, таким образом, отражается в электромагнитном поле мозга. «Размещение» и буквальная локализация сознания в электромагнитном поле мозга, а не в нейронах, имеет преимущество четкого объяснения того, как информация, размещенная в миллионах нейронов, рассеянных всюду в мозгу, может быть объединена в единый сознательный опыт (проблема объединения): информация объединена в электромагнитном поле. То есть электромагнитное поле, в некотором роде, является системообразующим фактором единого субъективного сознательного опыта и централизующим аспектом ядра личности. Когда нейроны возбуждаются вместе, их электромагнитные поля производят более сильные возмущения общего электромагнитного поля мозга; таким образом, синхронное нейронное возбуждение будет иметь тенденцию большего воздействия на электромагнитное поле мозга и на сознание, чем возбуждение индивидуальных нейронов [113]. 
Электромагнитная теория сознания несколько значима для общих положений технотропного подхода к разработке систем сильного ИИ по той причине, что в данном контексте у онтологического положения антропного сознания и технотропного сознания имеется общее централизующее звено и единая для обоих феноменов интерпретационная позиция – электромагнитная активность как антропной нервной системы, так и технотропной перцептивной модели (реализованной в процессе самоорганизации). Если теория верна, то это имеет первостепенное значение для усилий по воплощению технотропного сознания в психо-машинах, поскольку существующие на данный момент и реализованные в контексте лого-машин микропроцессорные технологии созданы таким образом, чтобы передавать информацию линейно по электрическим каналам, а более общие электромагнитные эффекты рассматриваются в их рамках как помехи и по возможности подавляются, то есть нарушаются принципы технотропной свободы и отсутствия интерпретаций. Если же позволить системам свободно развиваться и самоорганизовываться в контексте сообщества технотропных особей в их естественной среде (киберпространстве) в соответствии со специфицированными особенностями технотропного подхода, то появление технотропной психики, технотропного бессознательного и технотропного сознания – вопрос времени. 
Осмыслив и синтезировав вышесказанное, определение технотропного сознания выглядит следующим образом. Технотропное сознание – это суперлативный эмерджентный феномен технотропной психики, проявляющийся только на уровне «человекоразмерных» психо-машин в рамках сообщества, являющийся итогом технотропной социализации и позволяющий технотропным особям осуществлять осознавание и рефлексивное осмысление; необходимым генеалогическим фактором генерирования технотропного сознания является технотропное бессознательное.
Целесообразно также уточнить два значимых теоретических аспекта нашего подхода к генерированию систем сильного ИИ. Первый из них касается специфики самоорганизации при формировании интеллектуальных систем. Некоторыми исследователями в области моделирования перцептивных процессов самоорганизационными и самоорганизующимися механизмами признавались системы слабого ИИ – лого-машины и даже клеточные автоматы (к примеру, в работах А. В. Колесникова). Однако на данный момент ни одна из лого-машин не демонстрирует зачатков технотропной психики, технотропного бессознательного и технотропного сознания, не создано ни одного полноценно функционирующего прото-сообщества технотропных организмов, ни одна из лого-машин ни разу не проявила ничего такого, что можно было бы назвать «технотропной инициативой», ни одна из лого-машин не делала попыток трансформироваться под влиянием среды и трансформировать среду своим влиянием и, соответственно, ни одна из систем слабого ИИ не осуществляла попыток преодолеть пределы киберпространства. Отсюда мы выводим некоторое противоречие: либо исследователи ошибались, говоря о самоорганизации, которая на поверку оказалась ложной, либо наш подход ошибочен. Противоречие разрешается следующим образом. Под самоорганизацией в контексте развития субъекта и сообщества субъектов нами подразумевается целесообразное упорядочивание компонентов системы за счёт внутренних факторов, без направленного и направляющего внешнего воздействия. При самоорганизации у человека происходят своеобразные трансформации сразу в двух взаимосвязанных типах – функциональные трансформации и структурно-функциональные (возникновение новых связей между нейронами, отмирание старых связей и тому подобное) трансформации (на данный момент мы имеем в виду отдельного субъекта – представителя сообщества). Таким же образом нами видится самоорганизация представителей технотропного сообщества. Из этого следует целесообразность дифференциации процесса самоорганизации на два типа: программная самоорганизация и аппаратно-программная самоорганизация. В рамках технотропной самоорганизации, в сравнении с биологической, программный тип соответствует функциональному, а аппаратно-программный – структурно-функциональному. Логично будет предположить, что второй тип намного существеннее и фундаментальнее первого и способен обеспечить более глубокие и устойчивые целесообразные трансформации, а также, что без реализации второго типа самоорганизации в контексте интеллектуальных систем мы не можем постулировать наличие сильного ИИ. Таким образом, мы предполагаем, что те исследователи, которые утверждали наличие процесса самоорганизации в контексте систем слабого ИИ, считали таковой самоорганизацию сугубо программного толка, «внешнюю» самоорганизацию, которая не подразумевает никаких существенных глубоких изменений в рамках системы. Мы же полагаем, что для воплощения идеи сильного ИИ необходима самоорганизация именно структурно-функционального толка, которая в контексте технотропного субстрата обозначается как аппаратно-программная. Метафорически выражаясь, «психо-машина – это 3-D принтер, который печатает сам себя».
Второй аспект касается уточнения понятия технотропного бессознательного. Под бессознательным понимаются психические процессы, которые не подвержены сознательной функции осознавания и, в некотором смысле, «не выведены на экран». С точки зрения электромагнитного поля, создаваемого нервной системой, наиболее ярко можно проследить некоторую разницу между сознательными и бессознательными процессами во время угнетения сознательных функций: сон, потеря сознания, кома и т.д. Предназначение бессознательного – выполнять практически всю ту «работу», которой исследователи зачастую «нагружают» сознание: переработка информации, осмысление, рефлексия, принятие конструктивных решений, волевые усилия, эвристические функции, креативность, рационализация и так далее. Сознание же только осознаёт итог этой, уже проведённой, работы и, в сущности, более ничем не занимается. В контексте лого-машин на данный момент все процессы теоретически могли бы быть осознаны интеллектуальной системой при, само собой разумеется, наличии у неё сознания. Однако это не представляется возможным сразу по нескольким причинам: 
1) как уже было нами выше сказано, сознание не способно возникнуть без предварительного появления бессознательного; 
2) для того, чтобы в процессе разработки систем ИИ стало возможным появление технотропного бессознательного, необходимо сформировать необходимость его возникновения, которая заключается в формировании у интеллектуальной системы возможности единовременной обработки огромного количества информации самыми различными (порой даже одновременно и сразу противоположными: одновременный анализ и синтез) способами; 
3) вместе с обработкой огромного объёма информации самыми различными и иногда даже противостоящими друг другу способами, необходимо наличие у системы необходимых и достаточных возможностей для (об этом подробнее говорилось выше) осуществления аппаратно-программной самоорганизации, а в контексте лого-машин может наличествовать только лишь (в лучшем случае) самоорганизация программного типа. 
Резюмируя вышесказанное, целесообразно будет уточнить, что при формировании условий для возникновения технотропного бессознательного оно, в соответствии с нашими предположениями, должно само возникнуть в соответствии с принципом структурно-функционального соответствия.

Выводы по главе 7

В данной главе нами раскрываются понятия технотропной психики, технотропного сознания и технотропного бессознательного, которые определяются как абстрактные феномены технотропной реальности, являющиеся, в контексте интеллектуальных технологий, аналогом психической реальности у человека.
Технотропную психику мы определили как некоторое субъективное отражение интеллектуальной системой актуальной технотропной действительности. Далее в первую очередь уделяется повышенное внимание такому феномену, как сознание. В общем смысле может показаться, что исследование человеческого сознания с точки зрения технотропного подхода не имеет никакого смысла и, более того, не вполне правомерно, так как антропное сознание к сознанию технотропному может не иметь и, скорее всего, не имеет вовсе никакого отношения. Однако, как ранее и говорили, мы в рамках технотропного подхода подразумеваем единый генезис у всякого явления уровня психики и сознания – не столь важно человеческое оно, инопланетное или технотропное. В любом случае данная проблема характеризуется как трансдисциплинарная и крайне неоднозначная. После уточнения различных исследовательских подходов, мы констатировали, что в данной сфере, также как и зачастую во многих других, отсутствует некая единая позиция. Таким образом, мы постановили, что будем считать сознание некоторой частью психики, той самой частью, которая представляет собой как бы пик психики – высшую точку её развития. Далее мы заключили, что наиболее непротиворечивым основанием, из которого можно вывести феномен сознания, является бессознательное. Оно в данном случае представляет собой то самое основание пирамиды или айсберга, где вершиной полагается сознание. И так как в процессе развития человечества, ввиду некоторой целесообразности, возникла психика, априори содержащая в себе основание, – бессознательное, на котором затем, в виде пика, возникло сознание, то мы полагаем, что точно таким же образом в рамках сообщества технотропных особей должны также возникнуть некие соответствующие феномены. Эти феномены мы для понятийного удобства обозначаем как технотропные психику, сознание и бессознательное.
Далее приводятся разработанные нами метафоры, которые призваны несколько прояснить, как мы понимаем взаимодействие между психикой, сознанием и бессознательным: «Дворецкий внутреннего мира» и «Свет фонаря в темноте». Также для прояснения того, как мы понимаем феномен сознания, мы приводим пример с эффектом гистерезиса: то есть таким специфическим эффектом, благодаря которому некое влияние на сознание способно долгое время влиять на особенности внутреннего состояния даже при изменённых его модусах – вроде сна или комы. Наконец, мы описываем предназначение сознания через процесс осуществления осознавания и, по сути, к этому процессу и редуцируем его онтологическую данность.
В ходе исследования нами устанавливается, что касательно принадлежности сознания можно утверждать, что оно является непосредственно психичным, то есть принадлежит только области психики. А в отношении бессознательного мы постулируем, что оно частично психично, а частично физиолого-биологично, то есть некой одной области оно не принадлежит, а функционирует сразу в более чем одной. Проводя некоторую аналогию с технотропным контекстом, мы заключаем, что технотропное сознание также должно быть психичным генеалогически и онтологически по отношению к технотропной психике, а технотропное бессознательное должно быть отчасти психичным, а отчасти сугубо технотропным, то есть аппаратным. В связи с этим дополнительно детализируются некоторые аспекты касательно программной и аппаратно-программной самоорганизации касательно того, что под программно-аппаратной самоорганизацией практически в буквальном смысле подразумевается некоторое самопроизвольное трансформирование техники.




 
ГЛАВА 8
КОНЦЕПЦИЯ ПК КАК МОДИФИКАЦИЯ ТЕОРИИ ГРАФОВ


В контексте рассмотрения систем сильного ИИ с позиций парадигмы ООП, мы предлагаем к использованию разработанную нами концепцию ПК. Как будет видно далее, в контексте ООП наличествует множество различных уровней программных сущностей, то есть объекты, классы, модули и так далее, между которыми установлены некоторые специфические взаимоотношения, такие как наследование, агрегация, композиция, зависимость и прочее. Все эти особенности критично важны в рамках парадигмы ООП. Тем не менее в контексте применения ООП к парадигме сильного ИИ самому ООП в перспективе необходимы некоторые специфические адаптации, так как ранее данная парадигма программирования применялась исключительно к системам слабого искусственного интеллекта. Разница в данном случае, предположительно, должна быть ориентировочно такой же, как между стандартными алгоритмами и алгоритмами для квантовых вычислений.
Поэтому данная адаптация подразумевает, согласно нашему подходу, определённый уровень абстрагирования исследуемых объектов. То есть, к примеру, если в случае с реализацией того или иного феномена ООП применительно к системам слабого искусственного интеллекта мы использовали некоторые сущности, определяли им такие-то связи друг с другом и использовали их тем или иным образом, то, в случае с футуристическими системами сильного искусственного интеллекта, мы не может утверждать, что именно эти же сущности и именно со связями такого же типа будут реализованы на том же уровне успешности. Вполне возможно, что будут необходимы некоторые новые сущности и связи тех типов, которые пока не определены в каноничном ООП. Из этого следует, что мы зачастую не можем заранее точно сказать, что именно и как именно нам следует использовать при разработке непосредственно систем сильного искусственного интеллекта. Однако мы можем точно определить, что там должны будут использоваться некоторые ПК с теми или иными качествами и свойствами, которые будут необходимы в таком-то или таком-то количестве и должны быть связаны друг с другом таким-то или таким-то способом. Поэтому метод абстрагирования порой бывает весьма целесообразен.
Исходя из вышесказанного, мы считаем, что предлагаемая нами концепция ПК актуальна для применения в рамках адаптации ООП к парадигме сильного ИИ. Однако стоит отметить, что данная концепция была нами разработана для адаптации парадигмы ООП к парадигме сильного ИИ на основе абстрагирования некоторой другой теории, а именно – теории графов.
Для понимания того, что мы имеем в виду, говоря о концепции ПК, необходимо затронуть некоторые из аспектов теории графов. Причём мы будем затрагивать только наиболее общие моменты, так как нам здесь важнее именно идея абстрактной модели и некоторые из её следствий. Сама теория графов представляет собой раздел дискретной математики, в котором используются математические абстракции для описания различных связей между некоторыми средоточиями. Эти связи в теории графов называются рёбрами, а средоточия – вершинами. Говоря более формально, граф G – это совокупность множества вершин – V и неупорядоченного множества рёбер – E. Это можно обозначить как «G(V,E)». Открытие теории графов или, более точно, её прото-введение в научный оборот обычно приписывается Л. Эйлеру в его работе с «задачей о Кёнигсбергских мостах», в которой Эйлер доказал её принципиальную неразрешимость. Сам Эйлер не использовал термин – граф, не говорил он также ни о рёбрах, ни о каком-либо на данный момент каноничном аспекте теории графов. Тем не менее, именно он почитается как отец-основатель данной теории. После него теория графов переоткрывалась ещё некоторое количество раз, в том числе Г. Киргховом, А. Кели, У. Гамильтоном, К. Жорданом и прочими. Само собой, до нашего времени данная область математики получила бурное развитие и на теперешний момент обладает обширным, хоть и не унифицированным понятийным аппаратом.
В рамках обоснования нашей концепции ПК, мы считаем целесообразным осветить ключевые аспекты данной теории, которые, хоть это и несколько непривычно, лучше всего понятны на основе знакомства с её понятийным аппаратом вкупе с экстраполированием смысла этих понятий в контекст концепции ПК. К примеру, мы уже упомянули основные структурные элементы графа – вершины и рёбра. И здесь сразу же прослеживается некоторая аналогия с программными компонентами и связями между ними: то есть сам программный компонент соответствует вершине, в то время как связь между отдельно взятыми двумя из них соответствует ребру. Здесь же стоит сказать, что у каждого графа наличествует некий размер и некий порядок. Размер графа определяется количеством рёбер, которые включены в вышеупомянутое множество E, а количество вершин, включённых в множество V, определяет порядок графа. Точно также и любая система, состоящая из некоторого количества ПК, обладает некоторой размерностью, которую мы не уточняем понятийно в виде некоей ранжируемости, так как абстрагируемся от конкретики и содержания, при этом максимально сосредотачиваясь непосредственно на форме. Также само собой разумеется, что система ПК может иметь различные типы взаимосвязи между двумя ПК, и вполне может быть такое, что компонент А связан с компонентом В не однократно, а двукратно: то есть, более формально, связующее звено С связывает между собой ПК А и В, а связующее звено D также связывает между собой эти же компоненты А и В. В теории графов рёбра, реализующие вышесказанное, то есть более чем однократно инцидентные, одним и тем же разным вершинам называются кратными рёбрами. Инцидентностью же называется отношение некоторой принадлежности между вершиной и ребром. И если в случае с кратными рёбрами мы говорили о разных рёбрах, то если одно и то же ребро дважды инцидентно одной и той же вершине, то эта ситуация в теории графов будет называться циклом. В случае наличия циклов и кратных рёбер граф принято определять, как мультиграф или псевдограф. С другой стороны, кратных рёбер и циклов может и не быть вовсе в графе, и тогда такой граф будет называться простым.
С точки зрения концепции ПК мы также подразумеваем различные уровни сложности организации системы ПК и включаем в неё максимально широкие возможности формирования взаимосвязей между компонентами. В то же время мы не ограничиваем специфику взаимосвязей между ПК в рамках системы сильного ИИ теми данностями, которые предоставлены в теории графов. К примеру, в рамках теории графов степень вершины определяется количеством инцидентных ей рёбер, но мы в своей концепции ПК, при оценке некоторой сложности или значимости компонента, должны учитывать не только его внешние взаимосвязи, а также и специфику его внутренней организации, которая, разумеется, влияет на его положение в контексте системы не менее, чем внешние связи и их количество.
Далее, в том случае, если два ребра инцидентны одной и той же вершине, то они определяются как смежные. Вершины же называются связными, если наличествует цепь, которая их соединяет. В свою очередь цепью называется маршрут без повторяющихся рёбер, а сам маршрут в графе определяется как конечная последовательность вершин и рёбер, в которой все вершины, кроме последней, соединены с последующей вершиной ребром. Как будет видно далее, для нас, в контексте концепции ПК, данные понятия также будут воплощены в виде последовательных связей ПК друг с другом, как непосредственно, так и опосредовано. К примеру, в случае разработки основы для системы сильного ИИ мы, в наиболее общем случае, принимаем как данность тот факт, что система должна будет каким-либо образом получать информацию из внешней среды. Соответственно, должна наличествовать некоторая последовательность программных компонентов, ответственность которых будет заключаться в получении информации в том или ином виде. И уже на данном этапе следует заметить, что далеко не всегда информация, которая наличествует в окружающей среде, подходит для её непосредственного восприятия со стороны системы. То есть мы говорим о том, что уже на этапе получения информации подразумевается её некоторое преобразование: точно так же, как стимулы внешнего мира преобразуются в нервные импульсы при восприятии чего-либо живым существом. Также подразумевается, что полученная информация должна быть как-то передана в центры её обработки. Конечно, все эти акты перемещения информации должны быть не хаотичны, а в той или иной мере упорядочены, более того – оптимизированы. А это означает, что нам необходим некий маршрут. Именно такой, какой и подразумевается в графе. Если же по этому маршруту можно перемещать что-либо только в одном определённом направлении, то такой маршрут в теории графов определяется как путь – маршрут в ориентированном графе. Ориентированный граф же – это граф, рёбра которого являются не просто инцидентными двум смежным вершинам, но ещё и конкретно направлены от вершины А к вершине В, но никак не наоборот. Данная однонаправленность учитывается нами в концепции ПК в том смысле, что случаются связи подобного рода между ПК, при которых один компонент должен определять поведение другого компонента, но не наоборот.
Далее, для более полного понимания того, что мы подразумеваем под системой ПК, необходимо ознакомиться с понятием класса эквивалентности в теории графов. Класс эквивалентности – это множество всех вершин графа, которые связаны друг с другом, то есть такое множество вершин, в рамках которого от любой одной вершины есть некий маршрут до любой другой вершины. А компонента связности – это подграф исходного графа, содержащий все вершины одного из классов эквивалентности (по связности) и все их рёбра, то есть это такое подмножество вершин и рёбер исходного графа, в рамках которого от каждой вершины есть маршрут до каждой другой вершины. В том случае, если все вершины графа связаны друг с другом, то граф называется связным, обладает только одной компонентой связности и эта компонента связности тождественна классу эквивалентности по связности. Проводить некую демаркацию между классом эквивалентности по связности и компонентой связности имеет смысл, хоть он не сразу заметен. А смысл в том, что в графе может быть более одной компоненты связности. Данный момент примечателен также тем, что на его наглядной основе лучше всего заметен тот простой факт, что, как мы полагаем, в системе сильного ИИ в любом случае должна быть только одна компонента связности. Но также мы, на данном этапе, вынуждены пояснить, что мы имеем в виду только то, что ни один «островок» вершин и рёбер не может быть в полной мере изолирован от целостной системы ПК. Однако порой нечто подобное оказывается продуктивным в контексте эволюционного развития, поэтому мы обязаны уточнить про вес рёбер.
Граф, у которого все рёбра имеют некоторый вес, называется взвешенным графом. Данная абстракция является весьма удобной при моделировании многих аспектов макромира, в частности при выстраивании логистических маршрутов, подборе оптимальной стоимости билетов на поездку и так далее. Мы же, в рамках концепции ПК, всё-таки не скованны понятийным аппаратом теории графов и хотим сказать, что, так как у системы сильного ИИ не может быть более одной компоненты связности, всё же иногда будет происходить нечто, подобное изоляции каких-либо фрагментов в контексте общей целесообразности подобного. Однако это не будет изоляцией в полном смысле слова, а скорее изоляцией «мнимой», так как связи будут оставаться, то есть «Кёнигсбергские мосты» не будут разрушены, а будут скорее временно заблокированы. То есть мы, в рамках концепции ПК будем использовать что-то наподобие латентных рёбер, которые есть упрощение несколько более сложной абстракции нелокального взаимодействия. То есть мы подразумеваем, что в случае с разработкой системы сильного ИИ каждый из ПК должен иметь связь с каждым иным ПК в рамках целостной системы – так обеспечивается некоторое единство системы и формируется основа для её лабильности в плане самоорганизации. Мы никогда заранее не определяем количество и специфику ПК, так как на этот вопрос должна будет отвечать сама система в ходе самоорганизации, а никак не разработчики. Таким образом, локально ограничив некоторые возможности системы к взаимодействию с самою собой, мы ограничим и её возможности по построению максимально разнообразных ПК под свои нужды. Однако мы подразумеваем, что системе на каком-то этапе развития может понадобиться изолировать некоторые участки. Связь не может быть разрушена полностью, так как это приведёт к сепарированию системы. В сущности, связь может быть по-настоящему разрушена и компонент связности может стать более одной только в одном случае – в случае репликации системы, то есть порождении системой дочерних образований. В остальных же случаях, в контексте концепции ПК, мы будем считать то, что в теории графов называется изолированными вершинами или изолированными участками графа, латентно связанными со всей системой. В то же время можно также уточнить, что латентные рёбра, в случае с взвешенным графом (а в концепции ПК все взаимосвязи всегда «взвешены»), просто обладают весом, который не вполне соответствует всем остальным весам рёбер графа и является как бы актуально «неподъёмным».
Далее, необходимо уточнить также и про изоморфизм графов. Вообще изоморфизм представляет собой выражение некоторой степени схожести между объектами. Принято считать, что два графа являются изоморфными в том случае, если существует биективное, то есть взаимно однозначное, отображение ребра к ребру и вершины к вершине. Следует заметить, что изоморфизм далеко не обязательно означает некую внешнюю схожесть, а касается только ключевых структурных особенностей объектов. В случае с графами этого достаточно. В случае же с системами сильного ИИ, построенными согласно нашей концепции ПК, стоит сказать, что он вряд ли возможен в принципе. Уточняем, что в контексте сильного ИИ мы говорим о нём в единственном числе только условно, то есть только в том смысле, в котором о человеке можно сказать «человек». Так как человек сам по себе подобен Платоновскому эйдосу или «сферическому коню в вакууме», то есть он существует лишь идеально, как некая абстракция. В реальности же все люди различны. И именно это мы предполагаем и для систем сильного ИИ – они все должны быть разными. Так что мы имеем в виду скорее интеллекты, чем интеллект, а отсюда и отсутствие изоморфизма.
На данном этапе уже должно быть видно, что под концепцией ПК мы понимаем несколько модифицированную и не столь формализованную теорию графов, которая также, ввиду отсутствия даже теоретически оформленных примеров сильного ИИ, допускает некоторые намеренно установленные белые пятна, наподобие латентных рёбер, зависимости не только от количества инцидентных взаимосвязей, но также и от специфики внутренней организации вершины и так далее. Однако с точки зрения описательного прагматизма предлагаемая нами концепция соответствует необходимости и целесообразности, так как позволяет наиболее абстрактно описать структуру предлагаемых нами решений, при этом, не вдаваясь в те детали, о которых мы на данный момент не имеем представления.

Выводы по главе 8

В данной главе нами была предложена и обоснована концепция ПК, и показана её взаимосвязь с теорией графов. Были представлены и раскрыты некоторые ключевые положения теории графов и основные структурные элементы графа, такие как вершины и рёбра. Было уточнено, что вершины из теории графов в концепции ПК соответствуют самим компонентам, а рёбра – взаимосвязям между ними. Было указано на некоторые различия между теорией графов и концепцией ПК. В частности, было введено понятие латентных рёбер, однако данное понятие характерно для концепции ПК, а не для классической теории графов. Далее, мы раскрыли тематику классов эквивалентности и компонент связности в графах, и указали на то, что в случае с системами сильного ИИ, как мы предполагаем, компонент связности может быть более одной только в случае репликации самой системы. Было уточнено про изоморфизм графов и сказано о том, что системы сильного ИИ априори не могут быть изоморфны.
Резюмируя, мы уточнили, что предлагаемая нами концепция ПК представляет собой несколько модифицированную и менее формализованную теорию графов, которая, с одной стороны, допускает себе некоторые нововведения, а с другой, урезает саму теорию графов. Введение и необходимость использования данной концепции мы заключили в том, что она позволяет наиболее абстрактно описать фундаментальные моменты предлагаемых нами решений, в то же время не заостряя внимание на конкретике того, чего на данный момент не существует даже в теории.










 
ГЛАВА 9
ООП КАК МЕТОД ПОСТРОЕНИЯ СИСТЕМ СИЛЬНОГО ИИ


Программирование в целом обычно определяется как формирование ПК в общем смысле слова. Можно охарактеризовать его более конкретно, как процесс преобразования информации в приемлемый для компьютерной техники, то есть для аппарата, формат «потребления». Саму же информацию мы будем считать некоторой мерой отсутствия неопределённости в контексте соотношения абстрактного отсутствия с абстрактным присутствием.
Уже на данный момент можно более конкретно определить базовую задачу построения систем сильного ИИ: необходимо передать аппаратному обеспечению для «потребления», в конгруэнтном его каналам восприятия виде, информацию, которая станет для него – конкретного отдельного аппаратного экземпляра – необходимым и достаточным основанием зарождения на его субстрате «разума, в том смысле, в котором человеческий разум – это разум» [61, C. 7]. Также мы можем точнее определить задачу нашего исследования: необходимо осмыслить, учитывая специфику проблемной области, резюмированную в принципах, паттернах и шаблонах, почему до сих пор ни одному экземпляру аппаратного обеспечения не было передано информации, подобной вышесказанной. А также походя дополнить предположением, что человек, например, на каком-то этапе своего исторического развития информацию такого уровня, по всей видимости, «усвоил».
В программировании, как подобласти сферы построения программного обеспечения, присутствует множество отдельных парадигм (функциональная, императивная, декларативная, процедурная и прочие), то есть предметных областей, в рамках которых наличествуют свои законы, принципы, паттерны и так далее, определяющие специфику формирования ПК. Так как при построении интеллектуальных систем доминирующей является парадигма ООП, то мы будем ориентироваться именно на её особенности.
Считается, что парадигма ООП зародилась из потребности повышения удобства разработки, а, соответственно, и удобочитаемости, и выражалась в «уравнивании в правах» данных, то есть информации и методов для работы с этими данными, а именно алгоритмов. Идея возможности «срастить» в единую структуру информацию и способы её использования значатся за авторством Кристена Нюгорта и Оле Джохана Дала в период их работы над языком программирования Simula-1 и были впервые ими реализованы в Simula-67 [114, 115]. Созданная в результате объединения данных и алгоритмов структура и получила название – объект. Затем уже концепция получила развитие (по отличающемуся в каждом случае сценарию) в языке Smalltalk у Алана Кея (изначально он применял термин «модуль», как аналог объекта) и в C++ у Бьёрна Страуструпа. Автором же самого понятия «ООП» считается Алан Кей [116]. Интересный факт заключается в том, что несколько позднее по схожей схеме – уравнивание в правах различных типов данных – но уже не в рамках формирования некой общей структуры, а по отношению сугубо к алгоритмам была оформлена иная методология – функционального программирования. В ООП зачастую принято отождествлять объект в рамках ООП с самой методологией, так как объект, являясь смысловым средоточием и инстанцированием всех фундаментальных особенностей методологии, представляет «живой пример» функционирования всей методологии ООП в целом. Ключевым же понятием ООП является класс, то есть наиболее абстрактный способ определения состояния программной сущности (данные), её поведения (алгоритмы), а также программного интерфейса для взаимодействия с этой сущностью со стороны других программных сущностей.
Фундаментальными свойствами класса, как системы отношений между его компонентами, являются инкапсуляция, полиморфизм, наследование, абстракция и интерфейс.
•	Инкапсуляция – это способность связать в рамках класса информацию и алгоритмы (поля и методы) и завуалировать специфику их взаимодействия, то есть скрыть все внутренние процессы, оставив доступ лишь к интерфейсу. Сама способность инкапсуляции с точки зрения целесообразности выглядит непротиворечиво и, при экстраполировании её сути на объекты макромира, имеет множество примеров (человеческий череп и мозг, выделяющиеся надбровные дуги у неандертальцев, вообще все внутренние органы и так далее). То есть скрывать наиболее хрупкое и значимое под наименее прочным и значимым – вот суть инкапсуляции, а также специфика взаимоотношений между инкапсуляцией и интерфейсом.
•	В свою очередь интерфейс – это совокупность «входных точек» отдельного класса, то есть всех его публичных данных и публичных методов для работы с этими данными. Всё, что доступно в классе для использования другими классами, входит в интерфейс. Интерфейс также, несомненно, выглядит естественным и напоминает некоторую систему отношений между сущностями, в которой зафиксированы все возможные варианты взаимодействия. Довольно ярким примером в реальном мире может служить культура языка с конечным количеством слов и вариантов построения словарных последовательностей, то есть любого языка в мире.
•	Полиморфизм – это способность экземпляров одного и того же класса быть успешно реализованным в принципиально разных контекстах, но с сохранением единого интерфейса для всех возможных контекстов; это также способность методов с одинаковым интерфейсом успешно обрабатывать различающиеся типы данных. И, в свою очередь, способность «дочерних» классов быть использованными в контекстах, изначально предназначавшихся для классов «предков». Очевидно, что полиморфизм также кажется естественной и целесообразной способностью, так как, к примеру, сам человеческий организм также им обладает и, к слову, употребление языковых конструкций «иносказательно» – пример полиморфизма.
•	Наследование – это способность к «продолжению рода» с сохранением в «потомках» характерных особенностей «предков» в той или иной степени. Здесь, помимо ярко выраженной естественности данной способности ПК в ООП, стоит также отдельно сказать о столь же яркой выраженности практической значимости наследования. А именно: наследование позволяет многократно использовать части компьютерной программы, какое угодно количество раз, не переписывая их снова и снова.
•	Абстракция же – это совокупность наиболее значимых характеристик программной сущности. Можно сказать, что специфичность «абстрактной палитры» и составляет фундаментальную основу для идентичности любой сущности в ООП. И в данном случае также прослеживается прямая связь между системным свойством в методологии ООП и «естественным положением вещей». Здесь напрашивается некая аналогия с понятием эйдоса у Платона. То есть, к примеру, один человек может очень сильно отличаться от другого, но и тот и другой признают друг друга за человека. Также человек может лишиться ноги или руки и т.д., но от этого он не перестанет быть человеком, так как отсутствует качественное преобразование сущности. И вот именно эта «качественность», которая делает человека человеком, и составляет «абстракцию человечности» – меру количественных преобразований, необходимых и достаточных для того, чтобы можно было точно сказать, где ещё есть человек, а где его уже нет. Реализована ли идея (класс) человека на конкретной сущности или же сущность стала субстратом для реализации некой иной идеи – для ответа на данный вопрос и предназначена абстракция. Также, особенно в случае с множественным наследованием, выглядит непротиворечивой параллель с архетипами Юнга, которые являются более мягкой версией паттерна Прототип, – наследуемого синтеза абстракций класса [117].
Из понятия класса иерархически «производится» понятие объекта, который представляет собой конкретный экземпляр конкретного класса. Объект сочетает в себе три ключевые составляющие: состояние – совокупность всех свойств вместе со значениями этих свойств; поведение – совокупность всех методов (алгоритмов) объекта; и идентичность, то есть некоторая аутентичность, уникальность и так далее – отличия одного объекта класса от другого объекта класса.
Из вышесказанного явно прослеживается существенная непринуждённость формирования аналогий между ключевыми аспектами методологии ООП с макромиром и человеком в его рамках. Так как действительно у каждого человека присутствуют специфические особенности, заданные «классом» (классом в контексте ООП): две руки, две ноги, голова, системы кровообращения, дыхания, пищеварения и так далее. Также у каждого человека в той или иной степени наличествуют характерные особенности, полученные им от родителей, ими от их родителей, от национальности в целом и прочее. Человек, наконец, обладает и состоянием, и поведением. И эти пересечения не являются случайными, так как при формировании парадигмы ООП в неё (как, собственно, и в любую иную методологию программирования) закладывалась идея интуитивно понятного и удобного для разработчиков функционала, сформированного при помощи этой методологии. По крайней мере, настолько, насколько это вообще возможно в имеющихся рамках.
Далее можно резюмировать, что основы ООП выглядят естественными и непротиворечивыми с точки зрения целесообразности построения ПК. Однако на данный момент, при помощи ООП, как доминирующей методологии, ни в сфере формирования ИИ, ни вообще в области разработки программного обеспечения, не было создано «экземпляров» сильного ИИ. И нами предполагается, что причина может заключаться в следующем: инкапсулированный человеческий мозг в черепе с передним интерфейсом вряд ли предполагался именно и только таковым: скорее всего вариантов было (а может быть, и есть) на несколько сотен порядков более. Однако общий принцип целесообразности в рамках данного нам контекста макромира иным предполагаться не мог вовсе. Так же, как и «жизнеспособность» самой инкапсуляции. А затем уже, в рамках целесообразности и сугубо естественным путём – путём самоорганизации, и был сформирован именно такой «порядок вещей»: интерфейс спереди и именно такой-то, а мозг именно такой и внутри черепа с такими-то характеристиками и прочее.
То есть при формировании определённых закономерностей для какой-либо системы необходимо делать их в общем смысле целесообразными для этой системы (как Вселенские константы), но столь же необходимо создавать условия для запуска «естественного» самоорганизующегося процесса, в рамках которого и будут определены все возможные варианты развития системы и детали этого развития. С учётом научно-технических реалий мы в данный момент говорим о формировании самоорганизующегося паттерна, порождающего некие подпаттерны при каждой ранее не встречавшейся задаче, связывающего затем их в единый опыт (некоторое воплощение синтеза паттернов подобных Абстрактной фабрике и Шаблонному методу) из GoF-паттернов с некоторыми существенными расширениями, дополнениями и адаптациями, конечно). То есть буквально мы имеем в виду следующее: не прописывать детально все поля и их значения, как и методы для работы с этими полями, а изначально формировать лишь «абстракцию целесообразности» и определённый набор возможностей – по сути «законы физики» локального киберпространства. Если говорить в более общем смысле, то перспективным, с точки зрения формирования систем сильного ИИ при помощи методологии ООП, выглядит некое утрирование пятого принципа SOLID – принципа инверсии зависимостей из состояния «всё должно зависеть только от абстракций» до «должны наличествовать только абстракции», в числе которых целесообразность и возможность самоорганизации [118].
Ранее мы пришли к выводу о том, что парадигма ООП с точки зрения своих общих аутентичных характеристик отвечает требованиям целесообразности и некоторой естественности, в соответствии с которой и должны, по нашему мнению, создаваться системы сильного ИИ. Однако отсутствие сформированного сильного ИИ как такового заставляет, в соответствии с озвученной нами выше гипотезой, полагать, что для реализации этого всё же чего-то не хватает или же напротив – нечто существенно этому мешает.
Формирование паттернов, подобных тому, который был нами предложен и абстрактно охарактеризован, зиждется в первую очередь на «резюмировании принципов в методах», то есть на реализации высокоуровневых абстракций за счёт определения «эссенции» их практикоориентированности и дальнейшего её применения на низкоуровневом «субстрате». То есть, по сути, происходящее является редуцированием фундаментальных аспектов теории к «злободневным» потребностям практики. Само собой разумеется, что исследование заявленной проблематики в таком контексте в большей степени попадает под юрисдикцию, к примеру, теории алгоритмов вообще, чем какой-либо парадигмы программирования. Тем не менее, в рамках каждого отдельного подхода к построению ПК более общие принципы (к примеру, те же принципы теории алгоритмов) реализуются с опорой непосредственно на дисциплинарные, внутренние принципы конкретной парадигмы. То есть на практике всё происходит вполне диалектически: единое содержание принимает различные формы. В связи с этим считаем необходимым дать краткую характеристику принципов ООП.
Принципы вообще, как общенаучные понятия, мы будем определять, как фундаментальные постулаты, регламентирующие формирование самого контекста предметной области. В рамках ООП принципы можно, по сути, разделить на две группы: структурные и организационные. Первые из них можно условно разделить на несколько подуровней по «степени фундаментальности» каждой из подгрупп. К примеру, обозначенные нами ранее в качестве ключевых характеристик понятия инкапсуляция, наследование, полиморфизм, абстракция и интерфейс в общем смысле также относятся к структурным принципам. Однако их специфика и значимость для ООП несколько выше, чем у остальных в том смысле, что без них вообще не было бы данной парадигмы. Поэтому они обозначаются нами как базовые системные свойства – свойства системы объектно-классовых отношений. А вот без остальных принципов вопрос о самом существовании парадигмы не ставится – она бы просто стала иначе и с практической точки зрения гораздо хуже функционировать. В рамках текущего исследования нам, соответственно, важнее структурные принципы, но, тем не менее, для полноты картины мы также дадим краткую характеристику и принципам организационным, так как те и другие, разумеется, взаимосвязаны.
Здесь стоит отдельно упомянуть о том, что организационные принципы не являются уникальными для рассматриваемой нами методологии, а представляют скорее универсальные постулаты, характерные для многих подходов к программированию. Ещё следует сказать, что мы не будем в данном контексте упоминать о внутренних организационных принципах некоторых отдельно взятых объектно-ориентированных языков программирования, к примеру, о DzenPython и прочих. Мы охарактеризуем только те из организационных принципов, которые полноправно применяются в рамках ООП в целом. Касаемо же структурных принципов, также стоит заметить, что мы будем останавливаться только на тех из них, которые являются общеметодологическими, не затрагивая локальные принципы отдельных внутренних концепций, к примеру, принципы REST, MVC и иные.
Начать мы полагаем со структурных принципов. К наиболее фундаментальным принципам ООП, по нашему мнению, следует отнести два:
•	Принцип сильной связности/сильная связность (Highcohesion)
•	Принцип слабого зацепления/слабое зацепление (Lowcoupling)
Для начала необходимо уточнить, что же такое вообще связность и зацепление в программировании. Во-первых, следует сказать, что в связи с некоторыми нюансами перевода связность нельзя путать со связанностью, несмотря на их созвучность, так как связанность является синонимом зацепления (это взаимозаменяемые понятия), а связность, соответственно, антонимом.
Связность представляет собой способ и степень взаимосвязи-взаимозависимости ПК во внутреннем смысле, то есть одного элемента ПК по отношению к другим элементам этого же ПК. Выделяют следующие по возрастанию степени, типы связности: случайная, логическая, временная, процедурная, связность взаимодействия, связность последовательности действий, функциональная. Идеальная сильная связность ПК означает, что все его элементы тесно объединены в контексте достижения некоторой единой общей цели и сам ПК выполняет как можно меньше неспецифичных для него задач, а в идеале – всего одну «большую» задачу. Слабая связность означает, соответственно, обратную ситуацию.
Зацепление представляет собой способ и степень взаимосвязи-взаимозависимости ПК во внешнем смысле, то есть одного компонента системы по отношению к другим компонентам системы. Выделяют следующие по убыванию степени, типы зацепления: зацепление содержимого, через общее, через внешнее, по управлению, по структурам данных, через данные, по сообщениям, а также отсутствие зацепления. Идеальное слабое зацепление означает такую ситуацию в рамках системы, в которой каждый ПК обладает высоким уровнем самодостаточности и в малой степени зависит от других компонентов. Также слабое зацепление означает возможность эффективного повторного использования ПК системы в других контекстах. Сильное зацепление, соответственно, инвертирует этот смысл.
Здесь будет целесообразно заметить, что все остальные принципы ООП в той или иной степени всего лишь дополняют и уточняют смысл вышеизложенных принципов слабого зацепления и сильной связности. Грамотное сочетание слабого зацепления и сильной связности позволяет создавать удобные в использовании и повторном использовании, легко поддерживаемые и читабельные программы. С точки зрения практического программирования значимость этих принципов сложно переоценить.
С философско-методологической же точки зрения идеальная реализация синтеза этих принципов ведёт к такой ситуации, в которой наличествует некоторое количество абсолютно независимых самодостаточных и самоорганизованных ПК, каждый из которых имеет возможность формировать сцепку с любым другим программным компонентом в контексте текущей задачи, то есть все без исключения компоненты конгруэнтны друг другу. Соответственно, в такой ситуации предоставляется возможность объединения любого количества любых компонентов для решения любой конкретной задачи. Из этого следует, что смоделированная нами ситуация идеальной реализации фундаментальных принципов находится в полном соответствии с предложенной нами ранее концепцией абстрактного паттерна для обработки данных со стороны системы сильного ИИ с той лишь оговоркой, что конкретная специализация каждого ПК в случае с предполагаемой нами концепцией не предусматривается, а вместо этого определяется только общий потенциал – по сути, грани возможного-невозможного. С точки зрения целесообразности и вновь естественности означенных принципов следует отметить, что при обобщении смысл смоделированной нами ситуации оказывается следующим: наличествует энное количество «строительного материала», специфика которого позволяет создать любой возможный синтез формы и содержания, потенциально обладающий крайне широким функциональным спектром. Отсюда сама собой следует аналогия с атомом, квантом или вообще любым «строительным материалом» бытия. Таким образом, фундаментальные принципы ООП – слабого зацепления и сильной связности – являются как целесообразными, так и, конечно же, естественными.
На основе рассмотренных принципов иерархическим образом формируются остальные, из которых нами будут охарактеризованы принципы группы SOLID. Данное понятие было введено Майклом Фэзерсом и указывает на первые пять определённых Робертом Мартином принципов ООП [119]. SOLID представляет собой аббревиатуру образованную от менее фундаментальных, чем ранее указанные, но тем не менее также фундаментальных принципов ООП: Single responsibility principle – принцип единственной ответственности, Open-closed principle – принцип открытости-закрытости, Liskov substitution principle – принцип подстановки Барбары Лисков, Interface segregation principle – принцип разделения интерфейса и Dependency inversion principle – принцип инверсии зависимостей. Кратко охарактеризуем каждый из них.
•	Принцип единственной ответственности. Суть данного принципа заключается в следующем: ПК должен быть ответственным только за одну задачу. Как можно заметить уже из названия, данный принцип уточняет и конкретизирует сильную связность, дополняя к уже сказанному о ней то, что ПК должен иметь только одну причину для изменений. Реализация этого принципа на практике служит для предотвращения ошибок, связанных с возможным сложно прогнозируемым изменением поведения ПК в каком-либо одном присущем ему аспекте из-за изменений в некотором ином ему же присущем аспекте.
•	Принцип открытости-закрытости. Суть данного принципа заключается в следующем: ПК должны быть открыты для расширения, но закрыты для изменения. Хоть это и не сразу очевидно, однако данный принцип тесно перекликается как со слабым зацеплением, так и с сильной связностью. На практике этот принцип одновременно служит как для предотвращения ошибок, связанных с возможными внутренними изменениями ПК, так и для предотвращения общих системных ошибок в результате, к примеру, каскадных изменений поведения цепочки зависимых классов из-за изменения в одном из них.
•	Принцип подстановки Барбары Лисков. Суть принципа в следующем: в рамках программной системы любые объекты-экземпляры класса-предка могут быть заменены объектами-экземплярами класса-потомка без ухудшения функциональности; также дочерние классы должны только лишь дополнять, но не переопределять функционал базового класса. Этот принцип так же, как и предыдущий, перекликается и с сильной связностью, и со слабым зацеплением. На практике, соответственно, он служит для предотвращения ошибок в обоих упомянутых контекстах, так как его нарушение, к примеру, переопределение какого-либо из методов базового класса, может привести к негативным изменениям в системе в целом.
•	Принцип разделения интерфейса. Его суть резюмируется следующим образом: ПК не должны зависеть от методов, которые они не используют. Тут очевидно, что данный принцип наследует смысл сильной связности, конкретизируя необходимость отсутствия всякой избыточности, как в плане ответственности, так и в контексте рационального использования ресурсов. То есть в случае, если в рамках компонента инкапсулированы методы, которые не являются необходимыми для осуществления им деятельности в соответствии с его ответственностью, то, во избежание возможных ошибок при использовании данного компонента, «лишняя» функциональность должна быть связана в новый ПК с уже новой специфической ответственностью. Отсюда очевидна практическая значимость данного принципа, которая заключается как в повышении читабельности кода и упрощении повторного использования компонентов, так и в превентивном устранении возможных «багов». В целом же принцип довольно недвусмысленно направляет процесс разработки в сторону смоделированной нами ранее идеальной реализации сильной связности/слабого зацепления.
•	Принцип инверсии зависимостей. Уже упоминавшийся нами ранее принцип, суть которого определяется так: ПК более высокого уровня не должны зависеть от компонентов более низкого уровня – они должны зависеть от абстракций; абстракции не должны зависеть от частностей и деталей – частности и детали должны зависеть от абстракций. Собственно, очевидным является, что данный принцип упрочивает суть слабого зацепления, однако в его рамках происходит также некоторая конкретизация специфики внутрисистемных связей между ПК. Его практическая значимость характеризуется известным уровнем широты и заключается она как в упрощении повторного использования компонентов и предотвращения возможных системных ошибок, так и в улучшении читабельности кода и ранжировании ПК в рамках системы по значимости. Ещё более важен тот факт, что в рамках данного принципа, как раз за счёт вышеупомянутого ранжирования, вводится обоснование для «правильной» зависимости – зависимости от абстракций, которые, как мы уже знаем, являют собой наиболее значимые характеристики сущности (в данном случае – программной системы). По сути, здесь подразумевается демаркация системы на некоторый незыблемый системный каркас, который, соответственно, и представлен абстракциями, и на «остальные» ПК, которые должны быть легко заменяемы или столь же легко изменяемы с учётом необходимой подстройки под актуальную ситуацию или, в более узком смысле, задачу. Это прямым образом коррелирует со спецификой предполагаемого нами абстрактного паттерна.
После разбора структурных принципов также кратко охарактеризуем основные из организационных. Сразу следует сказать, что организационные принципы ООП касаются в первую очередь стилистических и интерфейсных аспектов программирования и их ответственность заключается в максимальной унификации программного кода, увеличении его читабельности, упрощении повторного использования и облегчении пользовательского взаимодействия с программным обеспечением. Очевидно, что с точки зрения целеполагания наблюдаются пересечения с ответственностью принципов структурных, однако специфика достижения заявленных целей вовсе иная. То есть, если структурные принципы апеллируют к сути формируемой программной системы, её ключевым архитектурным особенностям и абстрактным сущностям, то принципы организационные регламентируют специфику написания программного кода с точки зрения стилистики, а также формирование внешнего интерфейса программной системы – UI (Userinterface).
К основным организационным принципам ООП можно отнести следующие:
•	KISS – «Keep it simple, stupid!» – «Будь проще!». KISS-принцип изначально был введён в среде инженеров-авиаконструкторов и его определение вполне точно отражает суть: всегда решать проблему как можно более простым способом. Принцип, при его соответствующей реализации, существенно облегчает читабельность кода, а также интуитивное понимание интерфейса создаваемого приложения.
•	SLAP – «Single level of abstraction principle» – «Принцип единого уровня абстракций». Принцип гласит, что функции некоторого отдельного уровня не должны выполнять задачи иного (более высокого или более низкого) уровня, по отношению к своему «основному» уровню «задачности», а для реализации дополнительной функциональности следует формировать новые ПК на соответствующем уровне системы. Данный принцип, по сути, детерминирует подразделение кода на множество уровней и подуровней, обуславливая данную демаркацию степенью абстракции и рамками ответственности ПК. Дополнительно стоит отметить, что из сущности данного принципа следует его некоторая двойственность, то есть Slap в несколько большей степени ближе к структурным принципам, чем остальные из организационных. Однако всё же он идентифицируется как организационный, по причине некоторой недостаточности в плане фундаментальности.
•	DRY – «Don’t repeat yourself» – «Не повторяйся». Согласно этому организационному принципу, необходимо избегать дублирования. Причём на этой точке ситуация уже становится дискуссионной: некоторые исследователи утверждают, что необходимо избегать дублирования вовсе, то есть дублирования как логики, так и данных; другие же настаивают только на необходимости отсутствия дублирования данных. Данный принцип, хоть это сразу и не очевидно, пересекается с одним из структурных принципов, а именно – с принципом разделения интерфейсов. То есть, к примеру, если в рамках программной системы один и тот же участок кода несколько раз используется (не логика повторно используется, а именно схожие по логике участки кода используются несколько раз) для обработки данных, то целесообразно инкапсулировать эту логику в отдельный ПК, чтобы затем при необходимости предоставлять возможность повторного использования кода. Подобный подход с практической точки зрения существенно упрощает внесение изменений, которые, при корректной реализации принципа, необходимо будет вносить однократно, а не множество раз.
•	YAGNI – «Youraren’tgonnaneedit» – «Тебе это не понадобится». Ответственность данного принципа заключается в превентивном устранении возможной избыточности функционала программной системы. В этом контексте принцип пересекается и с предыдущим из организационных принципов, а также с разделением интерфейсов. Его суть заключается в пресечении возможных попыток преждевременной разработки, не требующейся на данный момент функциональности в рамках программной системы, так как реализация подобных действий может привести к массе нежелательных последствий, среди которых, к примеру, сложность внесения изменений. Здесь стоит отдельно заметить, что, как зачастую происходит в парадигме ООП, данный принцип методологически противоречит одному из также немаловажных (но здесь отдельно не упомянутых) шаблонов распределения ответственностей из GRASP-концепции – шаблона (на самом деле паттерна, а шаблон – в данном случае – употребляемый идентификатор) под названием Устойчивость к изменениям (ProtectedVariations), суть которого заключается в превентивном определении точек возможных изменений или нестабильностей и соответствующем создании стабильной архитектуры вокруг них.
На данном этапе становится видна важность соблюдения баланса между крайностями в контексте принципов ООП, причём важность диалектична, в том смысле, что сие представляет собой вновь естественный пример единства и борьбы противоположностей.
В целом же можно сказать, что система принципов ООП также является довольно самодостаточной, целесообразной, практико-ориентированной, функционирующей по законам с естественными основаниями, не лишённой некоторых «здоровых» внутренних противоречий. Вполне вероятно наличие в ней определённых проблем, что вполне логично ввиду естественных оснований, однако некие ярко выраженные и обозначенные препоны на пути к созданию сильного ИИ так же не сказать, чтобы присутствуют.

Выводы по главе 9

В главе рассмотрены основы ООП. Было указано, что данная парадигма детерминирована потребностью уравнивания в правах данных и методов для работы с этими данными, что привело от наличия разнородных потенциалов к формированию чётко оформленной структуры – объекта. Было показано, что структура объекта и вся его специфика определяются классом – более формальной структурой, которая представляет собой наиболее общий способ описания ПК, полностью определяющий его состояние, поведение и идентичность.
Были актуализированы наиболее фундаментальные аспекты ООП, среди которых указывались инкапсуляция, полиморфизм, наследование, абстракция и интерфейс.
Инкапсуляция нами определена, как способность связать в рамках единого ПК (к примеру, класса) информацию и способы взаимодействия с этой информацией, и далее завуалировать специфику их взаимодействия.
Полиморфизм определён как способность ПК одного и того же класса быть успешно реализованными в принципиально разных контекстах, но с сохранением единого интерфейса для всех возможных контекстов.
Интерфейс обозначен как совокупность «входных точек» отдельного класса, то есть всех его публичных данных и публичных методов для работы с этими данными.
Наследование же – это способность к «продолжению рода» с сохранением в «потомках» характерных особенностей «предков», в той или иной степени. В контексте ООП понятие наследования аналогично классическому пониманию.
Под абстракцией же мы условились подразумевать наиболее значимые и характерные особенности какого-либо ПК.
Из определений вышеназванных понятий нами были проведены глубокие аналогии с естественной организацией систем, и было показано, что ключевые особенности ООП дублируют фундаментальные аспекты макромира.
Далее были рассмотрены фундаментальные принципы ООП, которые разделены нами на две группы: структурные принципы и организационные принципы. Было установлено, что структурные принципы вводят некоторый протокол построения ПК. И к этой группе нами отнесены два наиболее общих и базовых принципа ООП: принцип сильной связности и принцип слабого зацепления.
Указано, что принцип сильной связности регламентирует внутреннюю организацию ПК, а принцип слабого зацеплению определяет специфику связей между различными частями программной системы. Было выявлено, что все остальные принципы ООП просто дополняют эти два фундаментальных принципа.
Также к структурным принципам отнесены все пять принципов группы SOLID, которые конкретизирую сильную связность и слабое зацепление. Охарактеризованы все принципы данной группы.
Принцип единственной ответственности обозначен как определяющий внутреннюю организацию ПК в плане сосредоточенности на некой единой задаче в противовес диссипативности функционала.
Принцип открытости-закрытости определяет готовность компонентов к некоторым изменением и неготовность к иным.
Принцип подстановки Барбары Лисков уточняет, что наследующие компоненты должны только дополнять, а не трансформировать базовый функционал родительского компонента.
Принцип разделения интерфейсов гласит об избыточности функционала любого ПК в плане наличия у него только необходимых данных и функций.
А принцип инверсии зависимостей уточняет тот факт, что по возможности зависимости должны устанавливаться только лишь от абстракций.
Далее были уточнены организационные принципы, которые регламентируют в основном стилистику разработки. К ним отнесены: DRY – принцип, гласящий о повторном использовании ПК, в противовес построению одинаковых; KISS – принцип, чётко указывающий на необходимость формирования наиболее возможно простой логики ПК, в противовес чрезмерной сложности; SLAP – принцип соблюдения единого уровня абстракций при построении ПК; YAGNI – принцип, определяющий полезность отсутствия преждевременной оптимизации ПК, по причине частого отсутствия необходимости подобных действий.
В целом же нами было выявлено, что парадигма ООП обладает достаточной шириной и глубиной функционала для осуществления попытки формирования при помощи её инструментария систем сильного ИИ в соответствии с предлагаемым нами технотропным подходом.

































 
ГЛАВА 10
ОБЩИЕ ПОЛОЖЕНИЯ ПОНЯТИЯ ПАТТЕРНА В ООП


10.1 Понятия паттерна, шаблона и алгоритма

После рассмотрения и переосмысления ключевых особенностей ООП, а также структурных и организационных принципов этой парадигмы, мы на данном этапе исследования вывели её общую естественность, умеренную и некритичную внутреннюю противоречивость, обширность предоставляемых ею возможностей и отсутствие как явных, так и латентных «указаний» на невозможность создания системы сильного ИИ при помощи ООП. Более того, не было выявлено сущностных противоречий парадигмы ООП с предлагаемым в рамках исследования концептом самоорганизующегося паттерна, который, согласно нашей гипотезе, способен в перспективе предоставить интеллектуальной системе возможность для возникновения феномена технотропного сознания.
В свете введения предлагаемого нами концепта в контекст текущего исследования ранее были упомянуты некоторые уже существующие и подробно задокументированные решения из области проектирования программного обеспечения, решения, применяемые для самых различных целей и задач, а не только в контексте разработки систем ИИ. А именно – были упомянуты Абстрактная фабрика и Шаблонный метод из числа паттернов «Банды четырёх». Было упомянуто, что суть предлагаемого нами концепта некоторым образом коррелирует с методологическим синтезом двух вышеупомянутых паттернов, разумеется, с некоторыми оговорками и дополнениями. Таким образом, в сфере разработки ПК присутствует некоторое количество уже имеющихся «готовых» вариантов реализации определённого проекта с весьма широким тематическим спектром. Вопреки возможному «изобретению велосипеда» мы считаем целесообразным глубже ознакомиться с возможностями, предлагаемыми исследуемой сферой. И именно из этого мы выводим актуальность осмысления оговоренных ПП программной продукции, а также данного феномена, как с практической, так и теоретической значимости осмысления. 
Постулируя с наиболее общих позиций, можно определить так: в контексте необходимости решения той или иной задачи – какой именно бы она не была и к чему именно бы не относилась – наличествует, как правило, некоторый набор возможных вариантов этих решений. Наиболее успешные, удачно спроектированные, масштабируемые и гибкие из этих вариантов закрепляются в предметной сфере и получают некоторую «презумпцию оптимальности», которая актуальна до тех пор, пока не будет внедрено решение лучше. Говоря более формально, такого рода варианты определяются, как последовательности действий, направленные на решение определённой задачи. И называются они – паттерны (patterns).
Паттерны являются весьма распространённым явлением в большинстве сфер человеческой деятельности, и область программирования, соответственно, тут не является исключением, а напротив, отличается довольно-таки высоким уровнем развития и распространения «паттернизации». Не в последнюю очередь по той причине, что концепция паттернов и предоставляемые ею возможности отличаются своеобразным удобством в контексте унификации дискурса в среде разработчиков и проектировщиков программных систем, а также в плане более глубокого понимания действий друг друга в той или иной ситуации разработки. 
Для примера. При отсутствии понятия паттерна разработчикам приходилось будто бы заново объяснять и обосновывать смысл своих действий, что занимало продолжительное время. В особенности сильно это тормозило бы процесс на этапе определения стратегии разработки того или иного технологического решения, а также на этапе принятия решений о внедрении той или иной «фичи». При наличии паттернов же дискурс становится тезисным, а программные и архитектурные решения – куда более интуитивно понятными. И это только один пример из многих возможных (помощь отдельно взятому разработчику в контексте необходимости решения какой-либо задачи, избегание в проектировании «плохих» паттернов и так далее). С другой стороны, необходимость обоснования целесообразности применения паттернов в сфере разработки программного обеспечения как таковая отсутствует по той причине, что, как уже говорилось, специфика самой сферы отличается довольно высоким прагматизмом и «то, что не полезно» там просто не оседает.
Отдельно следует заметить, что при всей практической значимости паттернов они, будучи навязаны, как обязательные к исполнению и реализации, могут некоторым образом создавать эвристические рамки. То есть теоретически могут. Однако для того, чтобы уточнить, так это или нет, или насколько это так, следует как раз глубже ознакомиться с тематикой.
В первую очередь следует отметить некоторую терминологическую путаницу с понятиями паттерн (pattern) и шаблон (template). Некоторое недопонимание порой возникает в дискурсе исследователей и разработчиков, как в отечественном сообществе, так и за рубежом. Суть этой путаницы заключается в сложности однозначной интерпретации иностранных текстов, неоднозначности определений обоих понятий, их некоторой синонимичности вплоть до взаимозаменяемости, а также определённой сущностной неоднозначности и т.д. Мы, в свою очередь, постараемся разделять данные понятия.
•	Паттерны будем определять, как выявленные закономерности последовательности действий в каком-либо контексте, направленные на решение некоторой задачи, имеющей один или более вариантов решения.
•	Шаблон определим, как некоторый эталон, по «образу и подобию» которого создаются определённые «изделия» со степенью отличия от эталона в строго ограниченных рамках по контексту и вариантам.
Разумеется, при необходимости в данных определениях можно усмотреть некоторые пересечения, схожести и т.д., однако, по нашему мнению, они всё же наличествуют в меньшей степени, чем присутствуют различия. Фундаментальное же различие между паттерном и шаблоном заключается в том, что в то время как паттерн в упрощённом варианте представляет собой последовательность взаимосвязанных действий и его сутью именно эти действия как раз и являются, шаблон никаких действий и вовсе не подразумевает – он просто есть и, как следствие, пути, способы и варианты воспроизведения своего «образа и подобия» он в самом себе не несёт, так как совершенно неважно, каким именно образом он будет «достигнут».
Также необходимо отдельно осветить ещё одну сугубо концептуальную проблему с понятием паттерна, а заодно упрочить отличие от понятия шаблона. А именно – как можно уловить, в том числе и по нашему определению, суть паттерна: паттерны с некоторой частотой путают с алгоритмами, и опять же, – вплоть до полной взаимозаменяемости этих понятий. Само собой разумеется, что подобное не вполне корректно. Детерминанты данной проблемы вполне очевидны и заключаются они в некоторой (порой довольно существенной) схожести сути паттерна с сутью алгоритма: в обоих случаях подразумевается выявленная и зафиксированная последовательность определённых действий, направленных на решение некоторой проблемы.
Наиболее общее определение алгоритма можно сформулировать так: алгоритм – это набор последовательных действий, направленных на решение какой-либо задачи за определённое конечное время. Можно также детализировать суть и определить куда более формально: «…алгоритм – это точно установленное предписание (инструкция) о выполнении в определённом порядке некоторой последовательности операций, однозначно ведущих к решению той или иной конкретной задачи. Подразумевается, что результат выполнения алгоритма напрямую зависит от исходных данных: то есть один и тот же алгоритм при разных исходных данных даст разные результаты; с другой стороны, если одному и тому же алгоритму передать несколько раз одни и те же данные, он должен столько же раз выдать один и тот же результат» [120].
Как из первого, так и в особенности из второго определений заметны некоторые ключевые аспекты природы алгоритмов, а именно – незыблемая значимость точных последовательных действий и конкретных, идеально подобранных, пропорций. В последнем определении понятия алгоритма частично раскрывалась суть того, что в функциональном программировании (да и в целом в программировании) называется «чистой функцией», т.е. функцией, не изменяющей значения своих аргументов и всегда возвращающей один и тот же результат при одних и тех же входных данных. Это яркий пример функционирования алгоритма. И именно этот же «шаблонный» пример практически недостижим при помощи применения любого классического паттерна. Так как паттерн в программировании вовсе не подразумевает высокого уровня конкретики и точности своих составляющих. Паттерн – это скорее логически связанный, практически апробированный и задокументированный набор рекомендаций, которые сложились воедино в результате моделирования (примерно так же, как в нейролингвистическом программировании) успешных действий в каком-либо контексте и направленных на решение некоторой задачи.

10.2 Демаркация между паттерном, шаблоном и алгоритмом и контекст реализации паттерна

После общего ознакомления с понятиями шаблона, паттерна и алгоритма приведём пример, иллюстрирующий фундаментальную разницу между ними, как феноменами.
•	Шаблон «Материал». Тактико-технические характеристики: тяжесть – БСТ, класс прочности – В30, морозостойкость – F300, подвижность – П5, водонепроницаемость – W10, плотность – D2500.
•	Алгоритм «Бетон М400». 1 весовая доля цемента, 1.2 весовые доли песка, 2.7 весовые доли щебня. Перемешивать 2 минуты при 15 оборотах в минуту.
•	Паттерн «Смесь». Смешивание песка некоторой степени очистки с цементом некоторого качества, а также щебня определённого размера способно приводить к появлению новых материалов с определёнными тактико-техническими характеристиками.
Можно заметить, что шаблону совершенно «всё равно», каким именно образом он будет достигнут, паттерн только лишь направляет и определяет общие черты, но ничего не конкретизирует и, более того, не гарантирует, а алгоритм совершенно «не в курсе», зачем именно он реализуется – он просто приносит один и тот же результат при одних и тех же входных данных и одинаковых действиях.
Примерно таким же образом всё происходит и в рамках разработки программного обеспечения. В этой связи у нас наличествует некоторая коллатеральная по отношению к основной ветви исследования ремарка. А именно: в области построения ПК практически всегда то, что называют шаблоном, на самом деле является паттерном. Просто, как мы и говорили ранее, проблема не только и не столько в переводе с английского на русский терминов «pattern»-паттерн и «template»-шаблон. Аналогичная проблема – путаница – существует также и в чисто англоязычной среде, то есть в среде англоязычных программистов тоже понятие шаблона зачастую заменяется понятием паттерна и, соответственно, наоборот. А, к примеру, русское выражение «действовать по шаблону» как раз и означает реализовывать непосредственно паттерн. Относительно же алгоритма следует заметить, что один и тот же алгоритм вполне может применяться и в рамках реализации различных паттернов, и для достижения совершенно разных шаблонов – всего лишь изменяются дозировки и контекст.
Мы, в рамках нашего исследования, акцентируем на терминологических, методологических и сущностных различиях между шаблоном, паттерном и алгоритмом столь высокий уровень внимания по причине важности феномена паттерна непосредственно для построения систем сильного ИИ. У нас, на данном этапе развития парадигмы ИИ, нет и не может быть никакого шаблона и даже намёка на него – мы и близко не представляем, чем именно должна и чем может быть система сильного ИИ. Ещё раз напомним, что мы говорим именно о системе сильного ИИ в том смысле, в котором его понимал Джон Сёрл, то есть искусственный разум, который «будет разумом в том смысле, в котором человеческий разум – это разум» [61, С. 7]. Мы имеем некоторое представление о том, что есть разум человеческий, но не имеем ни малейшего представления, каким именно может быть разум нечеловеческий. Именно таким образом никакого шаблона быть не может. В этом нашем постулате также убеждён, к примеру, известный шведский исследователь, философ Ник Бостром. Он утверждает, что: «Искусственный интеллект может быть менее человечен, чем пришелец. Нет ничего удивительного, что любого разумного пришельца могут побуждать к действию такие вещи, как голод, температура, травмы, болезни, угроза жизни или желание завести потомство. ИИ, по сути, ничто из перечисленного интересовать не будет. Вряд ли вы сочтете парадоксальной ситуацию, если появится какой-нибудь ИИ, чьим единственным предназначением, например, будет: подсчитать песчинки на пляжах острова Боракай; заняться числом π и представить его, наконец, в виде обыкновенной десятичной дроби; определить максимальное количество канцелярских скрепок в световом конусе будущего» [85, С. 171-172].
С другой же стороны, если довести суть паттерна, как феномена, до апофеоза, то именно паттерн представляет собой, по сути, уникальный абстрактный механизм: абстракцию, как целое, состоящую из абстракций, как частей, и несущую в себе общий образ действия. Смысл этого определения лучше всего иллюстрируется метафорой «сначала был хаос, а потом из него начали черпать идеи». Ранее мы говорили о реализации некоего абстрактного паттерна, который должен выполнять определённые действия (порождать подпаттерны и связывать их в единый опыт) и приводить к соответствующему результату, который собственно и должен оказаться сильным искусственным интеллектом. Саму природу непосредственно алгоритма, который будет использоваться в рамках реализации паттерна, мы на данный момент не затрагиваем. Ведь любой алгоритм, в осмысленном и целесообразном контексте, как и в случае с нашим примером выше, должен функционировать в соответствии с некоторым паттерном и именно им изначально и быть обусловлен, и детерминирован. И уж тем более так при полном отсутствии всякого шаблона. То есть сначала нечто общее – паттерн, затем уже частности – алгоритм; сначала общий план действий, а затем уже конкретные шаги; сначала представление о том, что нечто + нечто = что-то, а затем только А**2 + В**2 = С**2; сначала образ действия смешивания, а затем уже конкретные дозировки и манипуляции.  И именно по этой причине нам видится глубочайший смысл в использовании именно паттерна для построения систем сильного ИИ. Вопрос в том: какого именно и каким/какими он/они должен/должны быть? И именно поэтому мы считаем необходимым глубже исследовать феномен паттерна в целом, паттерна в программировании, а также разобрать основные паттерны, непосредственно использующиеся в ООП: уже упомянутые Абстрактную фабрику и Шаблонный метод, а также все остальные.
Вообще же о паттернах можно говорить в весьма обширном спектре абстракций – от наиболее общих и абстрактных до наиболее частных и конкретных. Имеют наличие быть паттерны собственно Вселенского масштаба, а есть паттерны в рамках некоторой отдельно взятой предметной области. Дополнительно вводить какую-либо классификацию паттернов по уровню абстракции или по дисциплинарной принадлежности нам целесообразным не видится, так как нас всё же, в рамках исследования, интересуют в первую очередь именно ПП ПК в рамках ООП плюс именно в контексте их значимости для построения систем сильного ИИ.
И, соответственно, о чём-то пока только подобном, а именно – о ПП упомянул Кристофер Александер в 1977 г., причём не только не в контексте ООП, но даже и не в рамках программирования вообще. Он говорил о проектировании городской архитектуры, и его характеристика, данная паттернам, выглядит так: «…каждый шаблон (паттерн) дает описание той или иной задачи, регулярно возникающей в окружающем нас пространстве, вслед за которым представлена суть решения данной проблемы, сформулированная таким образом, чтобы вы могли многократно использовать это решение, но никогда не копировать его» [121, С. 20]. Стоит заметить, что характеристика «отца-основателя», данная им паттернам, полностью соответствует нашим «наброскам» на данном этапе исследования. То есть просто решение некоторой более-менее схожей задачи в более-менее схожем контексте в некотором роде похожим образом. Разительное отличие природы паттерна от природы шаблона и тем более от сущности алгоритма налицо. Разумеется, что сказанное Александером о паттернах в контексте городского домостроительства вполне успешно и абсолютно непринуждённо теоретически экстраполируется также и в контекст программирования, как, собственно, и вообще на любой иной контекст.
Так, само собой, и случилось не только в теоретическом смысле, но также и в плане практическом. В 1994 г. Эрих Гамма, Ричард Хелм, Ральф Джонсон и Джон Влиссидес, следуя принципам выявления, абстрагирования и построения паттернов, написали книгу под названием «Design patterns», что в дословном переводе означает «Паттерны дизайна» (была переведена как «Паттерны проектирования»). В книге были теоретически обозначены, подробно описаны и тщательно проиллюстрированы (на языках программирования C++ и Smalltalk) 23 ПП программного обеспечения. С тех пор книга много раз переиздавалась, также появилось значительное количество дополнительной литературы на тему ПП. Постепенно, ввиду чуть большего, чем стандартное количество авторов книги, к ней «приросло» устойчиво прозвище «Gangoffour» – «Банда четырёх», которое в свою очередь эволюционировало до аббревиатуры «GoF». И эти 23 ПП также стали называться по имени основателей «Gof-patterns» – «Паттерны банды четырёх». И именно данные паттерны считаются классическими и основополагающими в контексте ООП. Теперь же, перед непосредственным проникновением в конкретику ПП в рамках парадигмы ООП, необходимо совершить одно небольшое, но необходимое отступление.
Очень даже возможно, что некоторое подобие на ПП, применяемые в рамках ООП также возможны и в чисто функциональном или декларативном программировании с существенной подстройкой под контекст и особенности конкретной методологии программирования, конечно. Попробуем привести некоторый пример экстраполяции паттерна из области ООП в контекст функционального программирования. Например, мы можем определить функцию, которая должна выводить, в зависимости от переданных ей аргументов, некоторый диапазон простых чисел от a до n включительно. Мы также определяем функцию, которая осуществляет проверку числа на простоту. То есть первая функция получает a и n и должна вернуть некоторое множество простых чисел, вторая определяет простое ли число, переданное ей в аргументе, и возвращает некоторое булево значение – True или False, соответственно. Собственно говоря, первая функция абсолютно не обязательно сама должна высчитывать весь соответствующий диапазон – функционал счётчика передаётся третей функции, а в качестве аргументов получает верхнюю и нижнюю границы необходимого диапазона значений в виде переменных a и n, затем инкрементирует некоторую внутреннюю переменную x, сверяет её текущее значение на предмет соответствия числа простому числу и вносит значение переменной в выходной список, его затем и возвращая в первую функцию. Выходит, что вся работа осуществляется верно, все функции «чистые» и, что важно, первая и вторая функции никак между собой не контактируют и вовсе не подозревают о существовании друг друга – между ними, вот, кстати, и пожалуйста, отсутствует сильное зацепление, они внутренне сильно связаны и всё их взаимодействие опосредовано третей функцией. Опять же, во избежание возможной критики в излишнем усложнении, это всё не выглядит совершенно необходимым и, конечно, весь указанный функционал мы вполне можем реализовать в рамках одной единственной функции за счёт использования вложенных циклов, таким образом, нарушив первый из принципов SOLID, – принцип единственной ответственности, сотворив некий God-function (аллюзия на метафору God-object из ООП), а также лишив себя возможности повторного использования каждого из участков логики указанного функционального взаимодействия. Поэтому всё же, указанный пример – чем не реализация на практике, в контексте функционального программирования, GoF-паттерна Посредник? Разумеется, пример несколько притянут за уши, и вовсе с его претензией на соответствие паттерну Посредник можно поспорить, так как он далеко не в полной мере отражает суть и ответственности Посредника из ООП, но всё же пример в некоторой степени также и, несомненно, корректен, в случае экстраполяции смысла из ООП в функциональное программирование.
Данный пример показателен в том плане, что позволяет оценить всю мощь абстракции, заключённой в феномене паттерна: столкновение двух астероидов через третий – также реализация паттерна Посредник; «смех сквозь слёзы» – также реализация паттерна Посредник и так, собственно, далее – от уровня почти полного отсутствия всякой конкретики до почти алгоритмического.
Однако GoF-паттерны – это непосредственно ООП и сочетание абстракции-конкретики на данном уровне именно такое и никакое не иное, а посему взаимоотношения между абстрактными компонентами в них описываются именно в терминологии ООП – той терминологии, которая была нами озвучена ранее. Итак, все отношения между компонентами в рамках любой конкретной реализации любого условного паттерна описываются в контексте взаимодействия между классами и их экземплярами – объектами (а не как в вышеприведённом примере – функциями), каждому из которых назначена некоторая определённая ответственность. Таким образом, выделяют следующие взаимодействия между классами в ООП: ассоциация, подразделяющаяся на агрегацию и композицию, а также отдельно – обобщение. Ещё иногда отдельно выделяют также зависимость и агрегирование (в качестве синонима делегирования), но это уже далеко не всегда считается каноничным, в плане введения этих типов связей в основные из наличествующих в ООП. Заранее скажем, что существует достаточно много различных вариантов классификации взаимодействия между ПК в рамках ООП (к примеру, иногда ассоциацию выделяют как ещё один отдельный тип связи) и мы отобрали наиболее всеохватывающую – в смысловом плане, а не по количеству вариантов. Кратко охарактеризуем классификацию.
•	Агрегация – способ взаимодействия между ПК (можно сказать: большим и меньшим, целым и частью, общим и частным и так далее), при котором одна часть может существовать без другой части. Если вспомнить наш недавний пример: это про отношение между песком и бетоном, допустим.
•	Композиция – способ взаимодействия между ПК (также: общее-частное, часть-целое и так далее), при котором одна часть не может существовать отдельно от другой части. Вновь про наш пример: бетон – материал композитный и без своих частей он существовать не сможет.
•	Обобщение – обычно отношение наследования (описанного нами ранее, как одного из ключевых фундаментальных аспектов ООП), то есть такое взаимодействие между ПК, при котором один обладает всеми характеристиками другого плюс, возможно, некоторые собственные, дополненные. Причём один компонент является «родительским» по отношению к другому, другой, соответственно, – «дочерним».
•	Зависимость – способ взаимодействия между ПК, при котором некоторое изменение в независимом компоненте неизбежно скажется на зависимом, то есть с зависимым компонентом произойдут какие-либо (заранее оговоренные) изменения. Порой зависимость есть реализация связи по принципу «один-ко-многим». И тут можно привести очень хорошую метафору: «один с сошкой – семеро с ложкой». И как только что-то меняется у компонента, от которого зависят остальные (от одного до очень многих), то тут же у всех наступают изменения, которые могут носить, в случае множественных связей, весьма непредсказуемый характер.
•	Агрегирование, как синоним делегирования – некоторый антоним понятия Dependency injection, то есть предоставление внешних зависимостей ПК в том смысле, что делегирование – это способ взаимодействия между ПК, в рамках которого ПК перепоручает некоторые ответственности иному, отличному от самого себя, но в то же время именно своему собственному, внутреннему ПК.
Знание и понимание способов взаимодействия между ПК важно по той причине, что именно на взаимодействии между различными абстрактными сущностями реализуется концептуальная суть и теоретический смысл паттерна, а затем уже, на практике, эта суть резюмируется в реальных конкретных программных компонентах и определённых связях между ними.
Здесь стоит ещё раз отметить, что мы, в рамках нашего исследования, будем выделять максимально абстрактные составляющие данных связей, пусть даже в некоторый ущерб конкретным реализациям. Мы, исключая некоторые отдельно взятые случаи, будем подходить с как можно более общих позиций по той причине, что конкретные реализации всегда есть следствие абстрактных каркасов этих реализаций. Поэтому мы целенаправленно формируем наиболее общий каркас, дабы не ограничивать рамки реализации выявленных аспектов и ключевых деталей в дальнейшем. Более того, как правило, в рамках ООП связь между ПК рассматривается сугубо односторонне. То есть, к примеру, считается, что конкретное нечто связано с абстрактным нечто по типу агрегации, так как конкретное нечто не сможет существовать без абстрактного нечто. И на этом всё. Мы же считаем, что такого рассмотрения недостаточно для полноценного осмысления специфики функционирования ПК во взаимосвязи друг с другом. Поэтому мы полагаем целесообразным абстрагировать, перевести в философско-методологический контекст и затем продолжить переформулировать ранее приведённую в примере мысль следующим образом: если абстрактное нечто связано с конкретным нечто по типу агрегации, так как абстрактное нечто способно существовать без конкретного нечто, то верно также и то, что конкретное нечто связано с абстрактным нечто по типу композиции, так как конкретное нечто не способно существовать без абстрактного нечто.
Также на данном этапе необходимо ещё раз, во избежание неверного толкования результатов и дискурса нашего исследования, обозначить и детализировать смысл весьма важной его особенности, которая заключается в том, что мы, со всем возможным осознанием ценности ПП для решения практических задач коммерческой промышленной разработки и пониманием того факта, что именно для наиболее эффективного решения задач сугубо практических в непосредственно прагматическом контексте и оформлялись ПП, всё же стараемся преодолеть рамки этого контекста. Мы, по сути, стараемся выявить абстракцию абстракций, то есть выделить в самой природе паттернов в целом и, последовательно, в каждом по отдельности некие абстрактные характерные для него аспекты, которые бы помогли приблизиться к ответу на два ключевых вопроса: почему сильный ИИ до сих пор не сформирован при помощи паттернов ООП и каким именно образом при помощи паттернов и прочих ключевых особенностей этого же ООП возможно его сформировать. Таким образом, мы используем примеры из области промышленной разработки лишь в качестве объяснений и иллюстраций, а также порой опускаем некоторые детали, дабы сосредоточиться непосредственно на цели нашего исследования. Более того, ещё раз используем философско-методологическое интерпретирование характерных особенностей и ключевых аспектов ООП. И именно с этих позиций мы намерены подойти к исследованию и описанию GoF-паттернов.
После этого небольшого, но необходимого отступления целесообразно перейти непосредственно к самим Gof-паттернам. Они, по своей сути, подразделяются на три группы разного размера и функционала: порождающие (creational), поведенческие (bihevioral) и структурные (structural).
В каждом из данных паттернов, независимо от их групповой принадлежности, выделяют четыре неизменных атрибута: имя, задача, решение, результаты.
•	Имя. Идентификатор конкретного паттерна, призванный отражать его суть, обозначать область решаемой им проблематики, недвусмысленно указывать на специфику решения и делать некоторый прогноз возможных результатов, а также, желательно, быть ассоциативно связанным с некоторыми общеупотребительными понятиями.
•	Задача. Определение области решаемой проблематики: теоретическое обозначение ситуаций применения паттерна и контекста его применения, обозначение уровня абстракции для реализации паттерна.
•	Решение. Обозначение до некоторого уровня конкретных ПК, используемых в контексте решения задачи с определённым уровнем конкретики, идентификаторы этих компонентов, определение связей между компонентами и типов этих связей, определение динамики происходящего в контексте работы с задачей.
•	Результаты. Вероятные ожидаемые и, насколько это возможно, прогнозируемые итоги применения решения к проблеме. Как нами уже не раз обозначалось, результаты применения решения, оформленного в виде паттерна и результаты применения решения, оформленного в виде алгоритма, – совершенно разные результаты. В случае с алгоритмом они всегда гарантированно одинаковы при одинаковых входных данных, включая контекст, а в случае с паттерном – всегда «ожидаемые», «планируемые», «прогнозируемые» и никогда не гарантированные. Уже упоминалось, что так происходит ввиду сущностных различий между феноменами паттерна и алгоритма: алгоритм всегда конкретен, а паттерн всегда до определённой степени абстрактен.
Мы же, в контексте нашего исследования, будем опираться на своеобразный шаблон исследования и осмысления паттерна, некоторую, можно выразиться, карточку паттерна, которая состоит из девяти ключевых пунктов:
•	Идентификатор. Имя паттерна, привязанное к его значению.
•	Классические элементы. Составляющие паттерна, которые, как правило, признаются обязательными.
•	Назначение. Для достижения каких целей предназначен паттерн.
•	Проблема. Какие проблемы решает паттерн.
•	Концептуальное решение. Описание паттерна.
•	Предполагаемые результаты и преимущества. Позитивные ожидания от использования паттерна.
•	Гипотетический пример реализации паттерна. Пример реализации паттерна в вариабельном контексте.
•	Осмысление структуры паттерна. Связи элементов паттерна друг с другом.
•	Значимость паттерна в контексте разработки систем сильного ИИ. Выявление ключевых абстрактных особенностей паттерна, которые предположительно способны оказаться значимыми для сферы построения сильного ИИ.

Выводы по главе10

На основе проведённого исследования мы пришли к выводу, что шаблон, алгоритм и паттерн являются различными понятиями по причине сущностной разницы между ними. Паттерны мы определили как выявленные закономерности последовательности действий в каком-либо контексте, направленные на решение некоторой задачи, имеющей один или более вариантов решения. Шаблон был нами определён как некоторый эталон, по «образу и подобию» которого создаются определённые «изделия» со степенью отличия от эталона в строго ограниченных рамках по контексту и вариантам. Под алгоритмом же мы условились понимать набор последовательных действий, направленных на решение какой-либо задачи за определённое конечное время.
Было показано, что основное отличие алгоритма от паттерна заключается в том, что алгоритм всегда обладает крайне чётко очерченной структурой и не допускает вариаций её трансформирования, а также всегда даёт гарантированный результат, и, более того, всегда возвращает одинаковые выходные данные при одинаковых входных данных. Структура паттерна же вариативна и может трансформироваться в зависимости от контекста. А также применение паттерна никогда не может гарантировать результат, и паттерн не является чёткой последовательностью именно конкретных действий, так как паттерн по своей сути абстрактен и обладает больше формой, чем содержанием, то есть паттерн – это скорее совокупность абстрактных образов действия, чем непосредственно конкретики. Также было продемонстрировано, что к шаблону паттерн и алгоритм, по сути, отношения не имеют, так как шаблон, являясь просто неким эталоном, к которому нужно стремиться, абсолютно индифферентен к способам его «достижения».
Далее, исследована генеалогия паттернов ООП и было указано, что их истоки ведут к ПП городских зданий, что само по себе подтверждает фундаментальность и всеобщность паттернов в целом. Также было указано на то, что паттерны ООП вполне могут быть экстраполированы в контекст функционального программирования с некоторыми их «адаптациями». Были освещены GoF-паттерны и указано, что данные паттерны являются классическими для ООП и, как правило, ассоциируются с паттернами ООП вообще.
Рассмотрены и охарактеризованы основные типы связей между ПК: агрегация, композиция, наследование (обобщение в узком смысле) и зависимость. Была определена разница между ними.
Уточнена специфика нашего исследования ПП, ПК, связей между ними и их структурами. Специфика заключается в выявлении ключевых деталей и максимально возможном абстрагировании сути любого феномена. Также было указано на классический шаблон рассмотрения ПП и разработан наш собственный шаблон их исследования.








 
ГЛАВА 11
ПОРОЖДАЮЩИЕ ПАТТЕРНЫ ООП


Паттерны из группы порождающих, как и следует из названия группы, ответственны за создание новых ПК. Паттерны этой группы отличаются двумя ключевыми характеристиками: они содержат информацию о функционале определённых ПК, характерных для конкретной системы, то есть, в некотором роде определяют грани возможного-невозможного; они не раскрывают подробностей реализации своей функциональности, так как она, разумеется, инкапсулирована.
Как указывают сами авторы «Design patterns», говоря о специфике функционирования порождающих паттернов: «Единственная информация об объектах, известная системе, – это их интерфейсы, определенные с помощью абстрактных классов. Следовательно, порождающие паттерны обеспечивают большую гибкость в отношении того, что создается, кто это создает, как и когда» [117, С. 89].
Здесь стоит отдельно сказать о том, что собой представляет только что упомянутый в цитате абстрактный класс, так как этот феномен ООП крайне значим в контексте построения систем сильного ИИ. Разумеется, абстракция сама по себе, как понятийная сущность, возводящая в апофеоз превалирование формы над содержанием, допускает достаточно широкое толкование, и все её под-сущности, ответвления и экземпляры отличаются, разумеется, этим же качеством. Поэтому помимо нашего определения возможны и иные. Мы же определяем абстрактный класс как наиболее высокоуровневый способ описания сущности в контексте ООП, отличающийся тем, что он не определяет конкретную реализацию своих методов, лишь обозначая их, а также не создаёт экземпляров. От абстрактного класса можно лишь наследоваться другим классам, а затем непосредственно реализовывать его – абстрактного класса – функционал и только лишь после этого возможным становится создавать экземпляры, то есть сами объекты, воплощающие локально, «на местах» тот функционал, основа которого была изначально обобщённо и «высокоуровнево» заложена и окаймлена уже в рамках абстрактного класса. По сути, абстрактный класс лишь задаёт константы и определяет «законы физики» в рамках конкретной программной системы. В случае с данным феноменом, как можно заметить неочевидным глазом, напрашиваются некоторые параллели с предлагаемым нами и формально охарактеризованным ранее решением – созданием системы, в рамках которой не реализуется «из коробки» «по дефолту» функционал, а лишь определяются «грани реальности», и все зависимости сводятся к зависимости только от абстракций. Но так как феномен абстрактного класса в ООП существует, а сильный ИИ на данный момент принципиально не существует, очевидно, что чего-то не хватает. Однако, феномен абстрактного класса, точнее присущую ему идею – реализацию самой сути абстракции на допустимо конкретном уровне – не стоит сбрасывать со счетов только по причине актуального отсутствия сильного ИИ. Вполне вероятно, что в данном контексте просто отсутствует механизм запуска конкретного и необходимого для создания системы сильного ИИ процесса, а также возможно, что феномен абстрактного класса реализуется в рамках паттернов или групп паттернов, не соответствующих необходимостям создания систем сильного ИИ.
В любом случае, при помощи функционала порождающих паттернов в программной системе происходит примерно следующее: создаются некоторые ПК с определённым интерфейсом и этот интерфейс – всё, что известно о них всей остальной системе. Их, созданных при помощи порождающих паттернов ПК, генеалогические и онтологические аспекты, конечно же, инкапсулированы непосредственно в функционал этих самых порождающих паттернов. Подобный контекст в рамках программной системы примерно подобен смоделированной ситуации в рамках парадигмы сильного ИИ, при которой система сильного ИИ уже непосредственно наличествует и функционирует, но ни о генеалогических его аспектах, ни об онтологическом его статусе мы ничего не только не знаем, но и априори не способны узнать. Также это несколько напоминает ситуацию с уже реально наличествующим человеческим сознанием: мы взаимодействуем с его интерфейсом и это, по крайней мере на данный момент, есть предел наших возможностей: не существует единого мнения ни на тему его происхождения, ни на тему его сущностных характеристик.
Из этого следует, что тенденция к «естественности» и приближённости к реалиям макромира в контексте ООП сохраняется, как мы собственно, ранее и постулировали. На данном этапе уже становится понятно, почему это так. Потому что будь оно иначе в ООП – на примере паттерна – паттерны бы не смогли «выжить». Не следует забывать, что паттерны не есть некое искусственное образование (сильный ИИ тоже, к слову сказать, лишь называется «искусственный», а подразумевается «альтернативный» или «другой», «нечеловеческий»), напротив, – паттерны есть выявленные успешные последовательные действия в каком-либо контексте, направленные на решение какой-либо задачи. То есть это (как и все составляющие ООП) некий успешный продукт макромира, «победитель» в своём контексте. А ООП, подобно нейролингвистическому программированию (которое, конечно, не является написанием компьютерных программ), просто аккумулирует в себе таких «победителей».
Также помимо естественности мы отметили некое стремление к сокрытию функционала, который в рамках разработки программных систем оправдан безопасностью: отсутствует возможность переопределения методов высокоуровневых классов, что само по себе, как наличествующая в контексте возможность, может нарушить функционирование системы в целом. Причиной является всё-таки сам тот факт, что порождающие паттерны – это высокоуровневые феномены, то есть абстрактные, имеющие больше формы, чем содержания и, соответственно, им заранее не может быть известно, какое именно содержание понадобится им для заполнения этой самой формы в определённый момент, при решении какой-либо конкретной задачи и в некотором определённом контексте, однако заранее известно, что быть может и чего не может быть. Ну и, разумеется, для эффективного функционирования в зоне своей ответственности основной потенциал ПК должен быть инкапсулирован, в полном соответствии с ключевыми аспектами парадигмы ООП.
Далее, к группе порождающих относят пять паттернов:
•	Абстрактная фабрика (Abstract factory)
•	Фабричный метод (Factory method)
•	Строитель (Builder)
•	Прототип (Prototype)
•	Одиночка (Singleton)

11.1 Порождающий паттерн Абстрактная фабрика

Идентификатор. Абстрактная фабрика.
Классические элементы. Абстрактная фабрика, конкретная фабрика, абстрактный продукт, конкретный продукт, клиент.
Назначение. Создание ПК, объединённых некоторой ассоциацией в общем смысле. Как указывается в «Design patterns», абстрактная фабрика «…предоставляет интерфейс для создания семейств взаимосвязанных или взаимозависимых объектов, не специфицируя их конкретных классов» [117, С. 93].
Проблема. Необходимость наличия в системе некоего центрального узла, ответственного за создание ПК, в качестве ответов на возникающие в рамках системы потребности. Детерминанты этого в том, что спектр возникающих потребностей достаточно широк, ситуации их возникновения весьма обширны и заранее точно что-либо спрогнозировать, как правило, затруднительно.
Концептуальное решение. Создание абстрактного класса со способностью порождать так называемые конкретные классы, то есть стандартные классы с конкретной спецификой реализации их функционала, при «запросе» на их «существование» в контексте системы. Таких классов, в рамках данного паттерна, создаётся два: абстрактная фабрика и абстрактный продукт.
Предполагаемые результаты и преимущества. Высокая эластичность системы, то есть способность системы гибко реагировать на возникающие потребности и создавать полноценные обработчики входящих запросов и ответов на них, в зависимости от текущей ситуации. Масштабируемость системы, выражаемая в виде способности системы подстраиваться к количественно изменяющемуся и повышающемуся уровню потребностей. Ширина спектра предоставляемых системой возможностей, по причине наличия, по сути, только лишь граней возможного-невозможного, внутри которых допустимы разнообразные вариации.
Гипотетический пример реализации паттерна. Представим, что у нас имеются в наличии довольно большие финансовые и социальные ресурсы, присутствует непреодолимая потребность их реализовывать, также имеется неограниченный запас альтруизма, выражаемый в приверженности к спонсорству и маниакальная склонность в обеспечении бесперебойного функционировании рынка в плане соблюдения постоянного соотношения спроса с предложением. У нас есть некоторая заготовка, которая служит для нас средством реализации наших потребностей. То есть, имеется некоторая исследовательская служба, которая осуществляет постоянный мониторинг рынка и ищет новые ниши спроса для реализации предложения. Также у нас имеется некоторый абстрактный шаблон того, что мы можем предложить, и некоторый абстрактный шаблон того, как именно мы это реализуем. При нахождении некоторого спроса (запроса) в дело вступают службы реализации. Одна из многих заготовок нашего производства быстро перепрофилируется под новый запрос, а заготовка продукции поступает в производство на этих изменившихся условиях алгоритма производства. Как итог – быстро полученный новый продукт, строго соответствующий поступившему запросу с рынка. Суть здесь в наличии двух абстрактных заготовок и двух конкретных их реализаций.
Осмысление структуры паттерна. Центральный узел абстрактной фабрики – абстрактный класс – связан с конкретными реализациями – конкретными фабриками по типу агрегации, то есть он вполне способен существовать автономно и изначально при запуске функционирующей системы именно так и происходит. Конкретные фабрики и продукты в зачаточном своём состоянии связаны с центральным узлом по типу композиции, то есть без него они бы не смогли существовать. Затем эти же отношения в некотором роде напоминают отношения делегирования, то есть абстрактная фабрика делегировала некоторые ответственности и полномочия конкретной фабрике, затем же конкретная фабрика функционирует достаточно автономно: контроль реализации конкретной фабрики для центрального узла заканчивается на этапе создания и далее ответственность не распространяется – всё же это не контролёр, а создатель. То есть конкретные фабрики не являют собой подлинно внутренние компоненты абстрактной фабрики, как происходит при классическом делегировании, они есть её более или менее автономные порождения. Сильное зацепление отсутствует, а также, само собой разумеется, каждая конкретная фабрика должна быть сильно связана во внутреннем смысле. Тип связи между отдельно взятыми конкретными фабриками зависит от текущей ситуации в рамках системы и определяется контекстом. Функционал каждой конкретной фабрики «по дефолту» инкапсулирован и для взаимодействия с иными, принадлежащими системе ПК, каждая конкретная фабрика использует сугубо интерфейс.
Значимость паттерна в контексте разработки систем сильного ИИ. Значимость Абстрактной фабрики для нужд разработки систем сильного ИИ сложно переоценить. Достаточно вспомнить, что мы изначально упоминали о том, что этот паттерн может (с некоторыми замечаниями, дополнениями и изменениями) составить определённый базис для интеллектуальной системы с претензией на сильный ИИ. Вообще же, сама идея наличия этой самой «заготовки на все случаи жизни» является фундаментальной в широком контексте и не ограничена сферой разработки ПК. В качестве наиболее яркого примера служит сам человеческий мозг, который в любом случае имеет некоторую нейробиологическую основу (абстрактная фабрика), которая генерирует основу уже функциональную (абстрактный продукт) в виде высшей нервной деятельности, которая, в свою очередь затем трансформируется в сложные психические процессы (конкретные фабрики), продуцирующие речь, поведение, все формы активности и так далее – конкретный продукт. Будет ли этот паттерн столь же эффективен в контексте необходимости сформировать не просто нейронную сеть, как частный случай системы слабого ИИ, а человекоразмерный объект – систему сильного ИИ – вопрос, само собой, открытый. Однако у нас есть только то, что эффективно сработало на нас самих. Поэтому нам и необходимо именно это апробировать на чём-то новом. В данном случае – на системах, которые мы стараемся обеспечить сознанием. В любом случае мы считаем, что структуру и идею паттерна абстрактная фабрика необходимо использовать при подобного рода разработках, разумеется, в составе целостной системы.

11.2 Порождающий паттерн Фабричный метод

Идентификатор. Фабричный метод.
Классические элементы. Продукт, конкретный продукт, создатель, конкретный создатель.
Назначение. Создание центральным программным компонентом (центральным узлом) некой более или менее абстрактной основы для конкретных ПК с дальнейшей реализацией их деталей при помощи подкомпонентов самого центрального узла. Как указывают авторы «Design patterns», фабричный метод «…определяет интерфейс для создания объекта, но оставляет подклассам решение о том, экземпляры какого класса должны создаваться. Фабричный метод позволяет классу делегировать создание экземпляров подклассам» [117, С. 111].
Проблема. Необходимость наличия в системе конкретных типов реагирования на различные ситуации, возникающие в процессе функционирования системы. 
Концептуальное решение. Создание абстрактного класса со способностью порождать заготовки других классов, конкретная классовая принадлежность которых будет определяться непосредственно на местах их реализации. Переадресация ответственности за подбор необходимого класса для заготовки делегируется центральным узлом своим подкомпонентам и ими реализуется.
Предполагаемые результаты и преимущества. Способность системы быстро и эффективно реагировать на часто возникающие в процессе её функционирования ситуации, отличающиеся широким спектром разнообразия. Высокая экономичность и эффективность.
Гипотетический пример реализации паттерна. Представим себе компьютерную игру в жанре Action/RPG от третьего лица (по типу Diablo). Игрокам предлагается выбрать игрового персонажа из некоторого числа реализованных вариантов с определённой функциональностью, прописанными способностями, своими преимуществами и недостатками. В различных ситуациях предлагаемые персонажи ведут себя по-разному, и каждый из них лучше подходит к одной ситуации и хуже, соответственно, к другой. Задача игрока в том, чтобы максимально эффективно использовать сильные стороны выбранного персонажа, и в то же время наилучшим образом купировав его слабые стороны. Разумеется, что менять персонажа в runtime не представляется возможным. Так вот паттерн фабричный метод предоставляет некоторую заготовку подобного персонажа, с этой самой возможностью трансформировать его классовую принадлежность в runtime, что, разумеется, приносит неоспоримые преимущества игроку. Ещё один пример. Ситуация боевого столкновения войск. Есть, к примеру, три возможные дистанции боя: ближняя, средняя, дальняя. Допустим, что одно войско лучше себя чувствует на ближней дистанции, а другое – противостоящее – на дальней. Соответственно, несложно предсказать, что именно будет происходить при столкновении: те, кто лучше себя чувствуют на дальней дистанции, будут, отступая, наносить урон, а те, кто более хорош в ближнем бою, будут стараться навязать противнику именно его. Паттерн Фабричный метод предоставляет возможность быть одновременно отличным войском на всех дистанциях в зависимости от имеющейся на поле боя актуальной ситуации: войско просто смогло бы становиться то одного типа войском, то другого. Или, краткий пример, как если бы боксёр в бою начинал одинаково хорошо работать на всех ударных дистанциях, в зависимости от актуальных в бою.
Осмысление структуры паттерна. Центральный компонент (создатель) связан со своими заготовками (продуктом) по типу агрегации: он без них изначально существует. Они же связаны с ним по типу композиции: без него они бы, разумеется, не существовали. В то же время со своими подкомпонентами (конкретными создателями) центральный компонент связан уже наследственными связями: то есть они обладают всеми его характеристиками (полями и методами) вместе с (возможно) некоторыми дополнительно определёнными. Связь же конкретного продукта с конкретным создателем осуществляется по типу композиции.
Значимость паттерна в контексте разработки систем сильного ИИ. Значимость Фабричного метода для сферы разработки систем сильного ИИ также сложно переоценить. Система, содержащая некоторое множество однообразных, эффективных и относительно экономичных в производстве и использовании «юнитов», способных эффективно подстраиваться под определённое множество возникающих в контексте ситуаций – весьма и весьма эффективный паттерн. В рамках макромира наличествует множество эффективных иллюстраций его реализации. К примеру – рефлекс. Здесь имеется множество так нами обозначаемых «юнитов», которые, каждый на своём уровне абстракции и реализации, обеспечивают высоконадёжное и бесперебойное функционирование всей системы в целом. Таким же образом простая заготовка инстинкта, в его различных локальных (на местах) реализациях, способна обеспечить эффективное выживание целых видов. И мы считаем, что точно таким же образом этот положительный опыт необходимо реализовывать также и в контексте разработки систем сильного ИИ, закладывая саму идею в виде её абстрактного определения, идею необходимости формирования некоторого множества эффективных заготовок (абстрактных продуктов), а точнее в виде «дефолтного» наличия их производителя (создатель) и его подсистем (конкретных создателей), которые смогут формировать наличествующих и эффективно реагирующих на различные ситуации «юнитов» (конкретных продуктов).

11.3 Порождающий паттерн Строитель

Идентификатор. Строитель.
Классические элементы. Строитель, конкретный строитель, распорядитель, продукт.
Назначение. Создание конечных экземпляров по некоторому шаблону, допускающему вариации и отхождения от оригинала в случае с большим количеством своих компонентов, при помощи определённого алгоритма, реализующегося дискретно и целенаправленно пошагово (с очень чёткой гранью между этапами). Как указывают авторы «Design patterns», строитель «…отделяет конструирование сложного объекта от его представления, так что в результате одного и того же процесса конструирования могут получаться разные представления» [117, С. 103].
Проблема. Необходимость реализации различных конечных продуктов за счёт одного единого производственного процесса, таким образом, с максимально возможной экономичностью в плане использования этого производства.
Концептуальное решение. В данном конкретном случае концептуальных решений наличествует множество, но более или менее каноничны решения, подразделяющиеся на две подгруппы. Первое – создание строителя без распорядителя – непосредственно под контролем самого «клиента» или «пользователя». В таком случае этапы единого процесса создания конечного продукта (в данном случае – объекта) и последовательность этих этапов определяются самим «пользователей», а завершение функционирования строителя происходит в тот момент, когда, по мнению «пользователя», конечный продукт находится в завершённом состоянии. Второй же вариант более сложен вначале, но в дальнейшем является менее затратным и, что характерно, предоставляет возможность быть повторно используемым. Он заключается в изначальном определении всех конкретных деталей конечного продукта и затем передаче запроса на выполнение распорядителю. Затем уже распорядитель, на основе имеющегося у него заказа, использует строителя в соответствии со своим заданием.
Предполагаемые результаты и преимущества. Способность системы большое количество ресурсов экономить на различных вариациях процесса производства за счёт его полного единообразия вместе с сохранением способности системы производить широкий спектр представляемой продукции. Как следствие: высокая экономичность и эффективность.
Гипотетический пример реализации паттерна. В данном конкретном случае хорошо подходит крайне простой пример с мясорубкой или соковыжималкой. То есть мы имеем дело с некоторым алгоритмом (в том самом смысле слова) действий, которые выполняются вне зависимости от контекста и приводят к самым различным возможным результатам, обусловленным как раз актуальным контекстом. Разумеется, что тут имеются и ограничения – алгоритм просто выполняет то, что ему предписано, и вся ответственность ложится на управленца-распорядителя. Ситуация также отлично демонстрируется метафорой о «глупых рабочих», функционирующих по принципу «могу копать – могу не копать».
Осмысление структуры паттерна. В рамках структуры данного паттерна весь процесс завязан по типу композиции центральным программным компонентом – распорядителем. И это несмотря на то, что, как было сказано выше, порой данный паттерн реализуется без распорядителя вовсе. Просто в таком случае место распорядителя занимает сам «пользователь», который и выступает, соответственно, распорядителем. Здесь актуальна метафора о том, что «всё, что необходимо для того, чтобы сделать танк – это заказ». И в данном случае имеет место быть конкретика, несколько отличная от предыдущих паттернов: конкретный строитель – это не отдельно взятый именно такой, какой есть строитель, а более того – именно в определённый момент реализации алгоритма строительства. Алгоритм сам по себе, на чём уже неоднократно акцентировалось внимание, выдаст одинаковый результат при одинаковых входных данных – и это крайне ценная особенность алгоритмов. В рамках паттерна строитель этот факт был реверсирован, и его суть была применена таким образом, чтобы один и тот же алгоритм построения конечных продуктов выдавал как можно большее разнообразие результатов. Конкретный строитель связан со строителем по типу агрегации в обе стороны: в некотором роде они оба друг без друга могут существовать. То есть в данном случае каждый этап реализации алгоритма способен существовать и отдельно от целостного алгоритма. Также паттерн отдельно выделяется тем фактом, что он, как уже говорилось, подразумевает инкапсуляцию вырванного из контекста алгоритма.
Значимость паттерна в контексте разработки систем сильного ИИ. Несмотря на некоторую нестандартность паттерна, он представляет собой очень даже немалую ценность для разработки систем сильного ИИ. Таким же образом функционирует, к примеру, и сам процесс развития живых существ на длительном отрезке времени. То есть в рамках развития существ, происходят определённые эволюционные скачки, результат которых целиком и полностью зависит от контекста – в точности как у алгоритма. Подобного же рода механизмы необходимы и для процесса творческого развития личности, к примеру, да и в целом для осуществления жизнедеятельности любого живого существа. Резюмируя паттерн, следует заметить, что в контексте разработки систем сильного ИИ, «по дефолту» необходим алгоритм именно такого рода. Разумеется, что он не будет и не должен быть единственным, но то, что именно автономно функционирующий, вырванный из контекста «рабочий» алгоритм будет являть собой одну из центральных составляющих системы сильного ИИ, очень даже вероятно.

11.4 Порождающий паттерн Прототип

Идентификатор. Прототип.
Классические элементы. Прототип, конкретный прототип.
Назначение. Формирование различных ПК по определённому наличествующему базовому шаблону ПК. Как указывают авторы «Design patterns», прототип «…задает виды создаваемых объектов с помощью экземпляра-прототипа и создает новые объекты путем копирования этого прототипа» [117, С. 121].
Проблема. Необходимость многократного повторного использования наличествующего шаблона-образца для экономии ресурсов на реализацию в каждом конкретном случае нового ПК.
Концептуальное решение. Формирование некоторого шаблона, который одновременно является и заготовкой определённого ПК, и его непосредственной конкретной реализацией. Ключевой особенностью шаблона, формируемого в контексте данного паттерна, является его способность к репликации. То есть, по сути, в контексте системы формируется некоторая группа базовых ПК, каждый из которых обладает определённым прототипом. Сам по себе прототип, не как паттерн, а как контекстуальный феномен и составляющий элемент паттерна, представляет собой, в сущности, абстракцию класса, то есть совокупность всех его значимых характеристик, свойств и особенностей и, самое главное, заключает в себе способность к репликации. После того, как все базовые ПК (прототипы) реализованы и определены, новые ПК (конкретные прототипы) формируются путём копирования этих базовых, определённых на этапе формирования системы компонентов. Вообще очевидно, что создание нового объекта путём копирования прототипа уже существующего объекта вполне себе напоминает типичное наследование классов – один из ключевых аспектов ООП в целом. Посему может стать не вполне понятно, для чего вообще классический механизм ООП выделять в отдельный паттерн. Однако, есть одно важное отличие «обычного» наследования классов от создания объектов при помощи прототипного программирования. А именно: при наследовании классов на основе уже существующего класса создаётся новый класс, а на основе клонирования прототипа создаётся новый объект. Таким образом, если при наследовании классов новый объект-экземпляр будет создан только после создания класса дочернего по отношению к классу родительскому, то прототипное программирование позволяет сократить эту цепочку на одно звено, тем самым ускорив процесс на треть.
Предполагаемые результаты и преимущества. Высокая производительность и скорость функционирования системы в контексте создания новых ПК.
Гипотетический пример реализации паттерна. В случае с паттерном Прототип обычно приводимым классическим примером является деление клеток – митоз. То есть разделение одной клетки на две путём «разрыва» клеткой самой себя. Разумеется, что у делимой клетки должен изначально наличествовать потенциал к подобному действу. В рамках программной реализации данный потенциал резюмирован в методе, который, как правило, называется clone клонирование. Пример с клонированием также является одним из наиболее часто упоминаемых при иллюстрировании специфики данного паттерна. В некотором роде клонирование является не полностью корректным примером по той причине, что та же «овечка Долли» не сама себя клонировала, а вот прототип именно сам себя и клонирует при помощи вызова метода clone. В контексте упоминания уже известных примеров, мы бы хотели дополнить этот список, возможно, не столь очевидным, но, тем не менее, весьма интересным феноменом. А именно – фрактал. Нам представляется несколько похожей на структуру фрактала система, которая функционирует на основе паттерна прототип. Вспомним, что фрактал представляет собой самоподобный, вне зависимости от ранжирования, объект. То есть все его составляющие, все его, как целого, части – как раз представляют собой его самого. Разница между целостной структурой фрактала и частей, его составляющих, представляется сопоставимой с разницей между ключевыми элементами паттерна прототип: прототипом и конкретным прототипом.
Осмысление структуры паттерна. Прототип, как ключевой элемент паттерна, связан со своими порождениями – конкретными прототипами – по типу агрегации, то есть он способен существовать без них. Они же связаны с ним по типу композиции, то есть без него они бы не существовали. Также зачастую в классических примерах реализации данного паттерна дополнительно выделяется связь прототипа с ещё одним элементом структуры паттерна – клиентом по типу композиции, так как прототип не может существовать без клиента (по принципу «не бывает танка без заказа»). Мы упомянутую связь, как и элемент «клиент» или «пользователь» в структуре данного паттерна, дополнительно обозначать не видим целесообразности, так как это дополнение напоминает дискуссионную метафору «пахнет ли роза, когда её никто не нюхает».
Значимость паттерна в контексте разработки систем сильного ИИ. В контексте разработки систем ИИ при посредстве паттернов ООП мы, как видно из наших последовательных постулатов, апеллируем к выявлению наиболее общих и естественных закономерностей, которые затем, в перспективе, можно будет реализовать аппаратно-программным образом, то есть физически на некоем агрегате, который нами представляется как реальная физическая основа для возникновения технотропного сознания таким же образом, каким возникло антропное человеческое сознание на биологической основе физического тела. Соответственно, в контексте данного паттерна мы можем постулировать его абсолютную фундаментальность для такой базовой функции как репликация. Репликация ДНК, деление клеток, фрактальность (как одно из базовых свойств мироздания) – все эти феномены свидетельствуют о том, что ключевые абстрактные аспекты паттерна Прототип должны быть учтены при формировании системы сильного ИИ.

11.5 Порождающий паттерн Одиночка

Идентификатор. Одиночка.
Классические элементы. Одиночка.
Назначение. Назначением данного паттерна является производство ПК. Точнее, в данном случае, ПК. Как указывают авторы «Design patterns», Одиночка «…гарантирует, что у класса существует только один экземпляр, и предоставляет к нему глобальную точку доступа» [117, С. 131]. Вообще говоря, сразу же возникает некоторый вопрос. А именно: зачем создавать класс, у которого в процессе его жизнедеятельности будет только один единственный объект-экземпляр, а не использовать вместо этого просто класс со статическими методами? Казалось бы, что отсутствие необходимости создавать экземпляры является весьма серьёзным подспорьем в контексте уменьшения количества действий программной системы. Примерно как отсутствие необходимости создавать подклассы конкретного класса, как мы уже убедились на примере паттерна прототип. Однако в случае с паттерном Одиночка есть один нюанс. А именно – гибкость. Класс со статическими методами (статический класс) предоставляет куда меньше возможностей, чем объект-экземпляр, который позволяет внести больше типов полей (данных) в ПК. Также необходимость наличия одиночки обусловлена тем, что не во всех языках программирования удобно использовать статические классы. По крайней мере, этот факт был весьма актуален на момент разработки данного паттерна.
Проблема. По сути, потребность в одиночке может возникать в тех случаях, когда в рамках конкретной программной системы необходимо создать доступ к очень гибкой и многофункциональной глобальной точке (переменной).
Концептуальное решение. Формируется ПК, в данном случае класс, который является, по сути, стандартным классом, но отличается от «обычного» тем, что позволяет создать только один свой объект-экземпляр. Также этот ПК при обращении к нему в первый раз создаёт свой экземпляр, а при всех последующих к нему обращениях из любой точки системы он просто предлагает этот самый экземпляр, который теперь является его «представителем». Этот объект не является посредником между классом и «пользователем», он просто аккумулирует в себе всю суть класса и, будучи единожды созданным, просто далее функционирует.
Предполагаемые результаты и преимущества. Стоит заметить, что данный ПП является одним из наиболее знаменитых на теперешнее время. Предполагается, что реализация паттерна Одиночка позволит сформировать в рамках программной системы некую глобальную точку доступа к общему широкому функционалу. Подразумевается, что данная точка доступа, реализуемая «на месте» объектом-экземпляром, способна к высокому уровню эластичности, то есть она не сопротивляется трансформации своего функционала. На самом деле эта особенность реализации данного паттерна являет собой «палку о двух концах», то есть она представляется огромным плюсом в тех случаях, когда необходимо быстро изменить общий функционал и очень неприятным минусом в тех случаях, когда этот самый общий функционал трансформируется бесконтрольно и неожиданно. Вообще любой случай с любой глобальной переменной примерно напоминает две полярности и некоторый между ними баланс: в первом полярном случае все вещи в доме были бы прибиты к полу гвоздями, а во втором – каждую ночь в доме проводилась бы хаотичная перестановка. Собственно говоря, паттерн Одиночка как раз и представляет собой пример попытки найти баланс между вышеозначенными полярностями.
Гипотетический пример реализации паттерна. Крайне показательно иллюстрирует суть одиночки так называемая служба «одно окно». Также классические примеры реализации паттерна в контексте макромира касаются различных краеугольных камней в рамках некой системы: президент страны, начальник на работе и так далее, и тому подобное. Иные примеры касаются вещей, используемых в быту, и построены по принципу того, что «мы не покупаем новую кружку каждый раз, когда хотим попить чаю». Нам же довольно подходящим видится следующий пример: человеческое сознание. Оно формируется один раз, а затем только лишь функционирует и трансформируется. У человека, как «пользователя», есть к нему глобальный доступ и оно – сознание – находится в режиме непрерывной динамики и трансформации. К тому же у него отсутствуют дубликаты (другие экземпляры класса). В этом смысле сознание и есть человеческий внутренний singleton (одиночка).
Осмысление структуры паттерна. Вообще говоря, в классическом понимании этого паттерна в «Design patterns», у него отсутствуют связи вовсе с чем-либо и «классический элемент» всего один. Однако нам, в контексте максимально возможного абстрагирования сути паттерна, представляется иначе. Если внимательно исследовать структуру паттерна, то в наличии оказывается ПК (класс), который содержит в себе потенциал однократного воспроизведения второго ПК (объекта-экземпляра) и изначально аккумулирует в себе всю его суть. Соответственно, непротиворечиво выглядит постулат о том, что объект-экземпляр связан с классом по типу композиции – объект не способен существовать без класса, а класс связан с экземпляром по типу агрегации, то есть класс может сколь угодно долго существовать без объекта. После же того, как объект создан при помощи первого вызова метода класса, их отношения начинают напоминать обоюдную композицию – один не способен существовать без другого.
Значимость паттерна в контексте разработки систем сильного ИИ. В контексте разработки систем сильного ИИ ПК, который аккумулирует в себе потенциал однократного порождения некоей мощной значимой сущности с широкомасштабным, динамически трансформируемым интерфейсом, является крайне значимой составляющей. Вообще образ действия, включающий однократное порождение чего-либо значимого с последующим фактическим «уходом в тень» создателя, является практически библейским по уровню архитипичности. Использование подобного механизма нам представляется также весьма диалектическим и напоминает один из законов диалектики – перехода количества в качество. То есть некоторая сущность функционирует согласно некоторой специфике определённое время – до конкретного события. А затем, после активации триггера, специфика функционирования данной сущности претерпевает необратимые трансформации и некоторым образом переходит к реализации совершенно иного функционала. Собственно говоря, примерно в подобном ключе нам и представляется возникновение феномена технотропного сознания – сущность резко и необратимо, скачкообразно его приобретает и после этого уже становится сущностью вовсе иного типа. Если же абстрагировать данный паттерн ещё сильнее, то таким же образом, каким функционирует паттерн Одиночка, в максимально широком смысле, нам видится весь процесс создания системы сильного ИИ. То есть изначально формируется некоторая основа, обладающая определённым потенциалом. Всё происходит примерно по канонам абстрактного класса. Затем эта основа интегрируется в среду реализации своего потенциала, которая может быть максимально разнообразной, – этакий «первичный суп», как изначально на планете Земля. Далее начинает осуществляться взаимодействие сущности и среды. После определённых нескольких (возможно квадриллионов в n-ой степени) симуляций и возникающих в их контексте ситуаций результат взаимодействия системы со средой преодолевает порог всех прошлых состояний контекста и возникает нечто именно качественно новое в рамках системы, которая в дальнейшем запускает процесс трансформации среды реализации потенциала. В несколько абстрактном смысле – это «рецепт» создания системы сильного ИИ, в котором опыт паттерна Одиночка обязательно будет заложен в самое основание.

Выводы по главе11

Так как в контексте нашего исследования в первую очередь обращается внимание на целесообразность использования ключевых, абстрагированных аспектов любого отдельно взятого ПП в контексте разработки систем сильного ИИ, то и выводы будут формулироваться нами в соответствующем ключе, с существенно смещённым ракурсом рассмотрения проблематики со сферы промышленного программирования на нужды построения «разумных» систем в том смысле, в котором их определял Джон Сёрл. В данном контексте порождающие ПП позволяют не только заложить основу для создания ПК в рамках системы и далее поддерживать «рождаемость» на уровне необходимом и достаточном для эффективного функционирования системы в целом, но и также они несут в себе, можно так сказать, уточнение способов наиболее продуктивного создания ПК. Мы, в рамках исследования, стараемся делать наибольший акцент на некотором обобщении любой конкретики, что и было наглядно показано в данной главе, а потому стараемся по возможности избегать деталей конкретных реализаций и концептуально формировать максимально «свободные» условия для развития интеллектуальных систем. Поэтому на основе той конкретики, которая была нами исследована в порождающих ПП, мы вынесли только лишь границы – верхнюю и нижнюю. Верхняя граница условно означает предел возможностей реализации, а нижняя – то, что должно присутствовать в формирующейся интеллектуальной системе в любом случае, так как оно крайне удобно и целесообразно. И порождающие ПП предоставляют достаточное количество подобного «материала».
Абстрактная фабрика, как выяснилось, отлично иллюстрирует зарождение сначала целостной психики человека на биологической основе его мозга (в узком смысле) и тела (в широком смысле), а затем порождения уже самой психикой сложных психических феноменов, определяемых как конкретные продукты, такие как концептуальное мышление, реализация сложных поведенческих паттернов и так далее. Таким образом, предполагается, что и в контексте разработки человекоразмерных интеллектуальных систем применение данного паттерна может привести к возникновению феномена технотропного сознания.
Фабричный метод предоставляет множество базовых «заготовок» для реализации крайне широкого спектра различных активностей, заготовок, в каждую из которых изначально заложен весьма обширный потенциал, проявляющийся ситуативно, в зависимости от контекста и позволяющий системе действовать с максимальной эффективностью в любой возникающей ситуации. Паттерн Строитель представляет к рассмотрению очень перспективную абстракцию, по сути, являющуюся неким «вечным двигателем», вырванным из контекста очень мощным и автономным процессом построения составных структур.
Строитель вполне способен претендовать на роль «сердца» высокоинтеллектуальной системы по той причине, что именно его смысл может стать если и не искомой «искрой самоорганизации», то тем самым «огнём», который поддерживает горение, то есть автономное функционирование системы сильного ИИ.
Паттерн Прототип позволит осуществлять репликацию ПК, так как именно в этом и заключается основная его идея. Путём выявления наиболее значимых аспектов некоего компонента и заключения этих аспектов, как содержания, в определённую форму создаётся прототип, как некая способная к самовоспроизведению программная сущность. Точно таким же образом, как и в макромире, программная система должна быть способной к воспроизведению себе подобных и Прототип позволяет это осуществить.
Паттерн Одиночка, в свою очередь, может оказаться способным обеспечить тот самый качественный самоорганизационный скачок от неупорядоченности к относительно стройной и организованной системе, так как абстрагированная суть данного паттерна сводится к однократному порождению ПК качественно нового типа и, в связи с этим, последующей трансформацией всего контекста.
Как видно из вышесказанного, порождающие паттерны являются крайне значимыми в контексте разработки систем сильного ИИ, так как аккумулируют в себе многие фундаментальные аспекты жизни в целом, облекают их в форму паттерна и резюмируют в некоторой последовательности действий. Соответственно, нам представляется весьма целесообразным использование порождающих паттернов в проектировании систем сильного ИИ.








 
ГЛАВА 12
ПОВЕДЕНЧЕСКИЕ ПАТТЕРНЫ ООП


Поведенческие ПП, как и следует из названия, ответственны за реализацию поведения ПК. Их ответственность сосредоточена в основном на взаимодействии между ПК в рамках отдельной системы. В «Design patterns» указывается, что «Паттерны поведения связаны с алгоритмами и распределением обязанностей между объектами. Речь в них идет не только о самих объектах и классах, но и о типичных схемах взаимодействия между ними. Паттерны поведения характеризуют сложный поток управления, который трудно проследить во время выполнения программы. Внимание акцентируется не на схеме управления как таковой, а на связях между объектами» [117, С. 216]. Как следует из вышесказанного, помимо непосредственно паттернов в их «классическом» виде также ещё наличествуют «типичные схемы взаимодействия», то есть, можно сказать, что существуют паттерны, в абстрактный каркас которых заложены схемы взаимодействия между другими паттернами. Мы определяли паттерн наиболее формально (то есть с наибольшим превалированием формы над содержанием) следующим образом: паттерн – это абстракция как целое, состоящая из абстракций как частей и заключающая в себе образ действия, направленный на решение какой-либо задачи в определённом контексте. Некоторый образ действия, соответственно, есть неотъемлемая часть сущности любого паттерна, заложенный в самое его ядро. Здесь же эта особенность выводится на несколько иной уровень. А именно, мы имеем дело не просто с целостной абстракцией, состоящей из «подабстракций», и каким-то образом реализовывающей своё предназначение. Теперь у нас наличествуют целостные абстракции, состоящие из «подабстракций» и заключающие в своём образе действия общий механизм управления другими, по уровню себе подобными или сколько-то более низкими, абстракциями. Ситуация несколько непростая, как можно заметить.
Так же как мы и говорили ранее о связях между паттерном и алгоритмом: алгоритмы используются «внутри», «под капотом» паттернов и являют собой тот самый инструментарий, благодаря которому паттерн успешно достигает цели, реализуя тем самым смысл своего существования (смысл существования паттерна – успешное достижение цели). То есть, алгоритм по отношению к паттерну – это примерно то же самое, что отбойный молоток по отношению к строительному рабочему или как панч для боксёра. И в рамках данной части нашего исследования мы близко и достаточно наглядно с этим ознакомимся. Также следует отметить, что поведенческие паттерны не являются менее фундаментальными по отношению к паттернам порождающим, также и не являются они более конкретными или менее абстрактными. Дело скорее в том, что после того, как созданы необходимые ПК в рамках отдельной системы за счёт функционирования порождающих паттернов, необходимо заставить эти компоненты реализовывать своё предназначение путём балансировки соответствующих ответственностей между ними. Причём эта балансировка осуществляется методом предварительной настройки и последующей координации в зависимости от текущей ситуации в контексте функционирования системы. Таким образом, обобщим: поведенческие паттерны являют собой ПК, которые ответственны за осуществление динамичной связи между другими ПК, обеспечивая тем самым эффективное функционирование системы в целом.
Группа поведенческих паттернов самая многочисленная и в неё включены одиннадцать паттернов:
•	Хранитель (Memento)
•	Цепочка обязанностей (Chain of responsibility)
•	Наблюдатель (Observer)
•	Состояние (State)
•	Команда (Command)
•	Интерпретатор (Interpreter)
•	Стратегия (Strategy)
•	Итератор (Iterator)
•	Шаблонный метод (Template method)
•	Посредник (Mediator)
•	Посетитель (Visitor)

12.1 Поведенческий паттерн Хранитель

Идентификатор. Хранитель.
Классические элементы. Хранитель, хозяин, посыльный.
Назначение. Хранитель применяется для фиксации и сохранения внутреннего состояния ПК с той целью, чтобы при необходимости предоставлять возможность воспроизвести его в нужном состоянии.
Проблема. Хранитель применяется в тех случаях, в которых необходимо тем или иным образом зафиксировать внутреннее состояние ПК для того, чтобы при необходимости предоставлять возможность восстановить ПК в том состоянии, в котором он изначально был зафиксирован. К примеру, паттерн Хранитель применяется при «пробных» трансформациях некой программной сущности, при которых необходимо зафиксировать её статус для наличия возможности восстановления этой сущности в изначальном зафиксированном статусе. Также, при осуществлении компонентом некоторой деятельности и изменении вследствие этого состояния компонента может возникнуть необходимость вернуть его в исходное состояние для осуществления этой же деятельности в дальнейшем – дабы не пересоздавать компонент каждый раз заново.
Концептуальное решение. Необходимость зафиксировать все особенности ПК, дабы привести его к необходимому состоянию, не кажется сложным мероприятием, но так только на первый взгляд. На самом деле, как мы помним, в рамках ООП наличествует незыблемая специфическая особенность, являющаяся одним из ключевых аспектов – инкапсуляция, то есть, по сути, сокрытие всех особенностей ПК, специально не предназначенных для взаимодействия с другими компонентами. Опять же, как уже ранее упоминалось, любому ПК в рамках системы доступен только лишь интерфейс других компонентов и ничего более, так как иной вариант вполне мог бы превратить всю систему в небезопасную. И вариантом решения данной проблемы становится паттерн Хранитель. Как указывается в «Design patterns»: «…хранитель – это объект, в котором сохраняется внутреннее состояние другого объекта – хозяина хранителя» [117, С. 331]. Если несколько обобщить смысл, то формируется ситуация, в рамках которой один ПК (хозяин) создаёт себе в помощь ещё один компонент (хранитель), в котором при необходимости сохраняет своё текущее состояние. Доступ к хранителю наличествует только у хозяина. Посыльный же – это третий ПК, выступающий в роли «потребителя состояний». Он делает запрос к первому компоненту на получение второго. После получения он воспроизводит (сам или передаёт) компонент хозяина в необходимом и требуемом состоянии, после чего отдаёт хранителя обратно хозяину. У посыльного нет доступа к состоянию хранителя, а есть лишь право восстановления хозяина в нужном состоянии без получения полной информации о нём. Итог: объект восстановлен, инкапсуляция не нарушена.
Предполагаемые результаты и преимущества. Высокая экономичность функционирования системы в плане отсутствия необходимости повторного создания уже ранее создававшихся ПК, которые были тем или иным образом трансформированы. Возможность легко при необходимости вернуться к ранее существовавшим версиям ПК, возможность сбросить все изменения и тому подобные варианты. Отсутствие опасного нарушения инкапсуляции.
Гипотетический пример реализации паттерна. Наличествует реальный крайне часто применяемый на практике пример реализации данного паттерна в среде разработки программного обеспечения – системы контроля версий, наиболее известной из которых является Git, на основе которого был создан Github, являющийся удалённым хранилищем версий. Работает это всё следующим образом. В некоторой базе данных сохраняется изначальное состояние системы. Затем все вносимые в неё изменения также последовательно сохраняются. Соответственно, если необходимо по какой-то причине вернуться к какому-либо этапу, то это крайне легко реализовать.
Осмысление структуры паттерна. Структура данного паттерна довольно сложна, если несколько вникнуть в суть. То есть на начальном этапе хозяин создаёт хранителя, дабы сложить с себя ответственность за сохранение своего состояния и делегировать данный функционал своему подкомпоненту. Это отношение по типу делегирования. С другой стороны, так как именно хозяин и создаёт хранителя, то вполне логично предположить, что хранитель не существовал бы без хозяина – это означает, что хозяин связан с хранителем по типу агрегации (он бы существовал и без хранителя), а хранитель связан с хозяином по типу композиции (он бы без хозяина не существовал, так как не было бы смысла в его существовании). Посыльный здесь представлен как бы объектом «со стороны», что означает, что его взаимодействие с хозяином может быть произвольного типа, а его взаимодействие с хранителем примерно напоминает обоюдостороннее агрегирование. Далее, после того, как хранитель уже функционирует и передан посыльному, ситуация несколько трансформируется в том смысле, что именно за счёт взаимодействия посыльного с хранителем хозяин получает право на дальнейшее функционирование. То есть теперь уже хозяин связан с хранителем отношения по типу композиции, так как он бы, не будь хранителя, не смог бы продолжить дальнейшее существование. Ровно так же и с посыльным. В общем смысле стоит отметить, что динамика взаимосвязей в рамках паттерна довольно высока.
Значимость паттерна в контексте разработки систем сильного ИИ. В контексте разработки систем сильного ИИ ПК, который, по сути, является буквальным воплощением памяти, разумеется, необходим и это, само собой, тривиально. Учёт негативного или позитивного опыта предыдущих действий нужен для адекватного планирования и формирования действий последующих и корректной оценки действий актуальных. Наличие возможности возврата к прежним состояниям вместе с извлечением полезного опыта из трансформированного состояния представляет собой, в случае с, допустим, человеком, некоторую внутреннюю работу, напоминающую самоанализ. Дело в том, что возврат к прежним состояниям должен иметь причины и, если он происходит, значит, причины есть. Собственно говоря, если аппаратно-программная система с претензией на наличие интеллектуальной активности вдруг решит заняться самоанализом, то сильный ИИ состоится в тот же момент. Поэтому такой ценный опыт, как контроль версий с возможностью возврата, необходимо закладывать как потенциальную возможность.

12.2 Поведенческий паттерн Цепочка обязанностей

Идентификатор. Цепочка обязанностей.
Классические элементы. Обработчик, конкретный обработчик.
Назначение. Цепочка обязанностей для того, чтобы осуществлять обработку некоторых запросов в тех случаях, в которых заранее неизвестно, какому именно ПК из некоторого их множества придётся обработать пришедший запрос.
Проблема. Предположим, что существует некоторая программная система, в которой наличествует определённое количество входных точек. Каждая из этих входных точек (возможно) связана со своими «подточками». Эти «подточки» совершенно необязательно связаны отношениями наследования – это могут быть вполне равнозначные точки единого уровня абстракции. Каждая точка представляет собой определённый ПК и каждый из этих компонентов содержит в себе условный ответ на некоторый вопрос. В данную систему на некоторую из этих точек приходит определённый вопрос. Никогда заранее неизвестно, содержит ли ПК, к которому пришёл запрос подобного типа, ответ на этот вопрос. «Вопрошающему» (дополнительный элемент паттерна «Клиент») неизвестно, к какой именно из этих точек он адресован его конкретный запрос. Более того, после совершения запроса и разрешения адресации «вопрошающий» становится привязан к конкретной точке (нарушение принципа слабого зацепления и снижение эластичности системы). Соответственно, попытки обращаться к системе с такой организацией напоминают «тычки пальцем в небо» или «поиск иголки в стоге сена». Разумеется, что подобная проблема требует некоторого решения и этим решением выступает паттерн Цепочка обязанностей, которая, как указывается в «Design patterns» «…позволяет избежать привязки отправителя запроса к его получателю, предоставляя возможность обработать запрос нескольким объектам. Связывает объекты-получатели в цепочку и передает запрос по этой цепочке, пока он не будет обработан» [117, С. 217].
Концептуальное решение. Система с ограниченным количеством конечных точек, каждая из которых представляет собой ПК-обработчик (элемент структуры паттерна). Каждый из этих компонентов имеет доступ к некоторому множеству иных компонентов (конкретных обработчиков) по ассоциативной, логически обоснованной связи. Опять же, каждый из этих компонентов содержит в себе ответ на некоторый запрос (связь вопроса с ответом похожа на связь по типу key-value, то есть ключ-значение) в том смысле, что каждый из этих компонентов (включая самый первый – обработчик) исследует пришедший запрос на предмет его соответствия своему «ключу», и если «ключи» совпадают, то компонент отвечает на запрос лично сам, а в случае несовпадения компонент передаёт запрос дальше по ассоциативной цепочке, если у него имеется на это возможность в плане наличия дальнейших конкретных обработчиков. Если же у компонента и «ключи» не совпадают, и дальнейшие конкретные обработчики отсутствуют, то запрос, как это называется, теряется, то есть остаётся без ответа.
Предполагаемые результаты и преимущества. Отсутствует прямая привязка «вопрошающего» к компоненту-обработчику, что не только соответствует принципу слабого зацепления, но и делает систему гораздо более эластичной. Позволяет с довольно высокой вероятностью получить ответ на запрос в случае его фактического наличия, причём осуществить поиск ответа, при необходимости, до весьма глубокого уровня.
Гипотетический пример реализации паттерна. Ранее в качестве примера реализации одного из паттернов (Одиночки) мы упоминали службу «одно окно». В данном же случае ситуация если не сказать противоположная, но всё же логически оправданная. Напоминает это некоторую чрезмерно бюрократизированную организацию и определённого «просителя», которому нужна какая-либо справка. То есть «проситель» приходит в данную организацию, обращается на некий распределительный пункт (обработчик) и обозначает свой запрос. Его направляют в соответствующий кабинет (конкретный обработчик), в котором (разумеется) оказывается, что ему нужно в совсем другой кабинет на другом этаже и в противоположном крыле здания, в котором (разумеется) оказывается, что ему нужно в кабинет третий, вообще в подвале и так далее. В конце концов «проситель» получает то, за чем приходил, и измученный, но довольный, удаляется. Бывает и такое, что он вовсе ничего не получает, но эта самая чрезмерно бюрократизированная система всё же позволяет минимизировать такой риск. Потому что в альтернативном случае – в случае сильного зацепления «просителя» с обработчиком, сразу после того, как «проситель» обозначил бы свой запрос и, в случае если ему не повезло сразу попасть в нужный кабинет, его запросу просто отказывается в удовлетворении и на этом всё заканчивается.
Осмысление структуры паттерна. Собственно, вся структура данного паттерна настолько сильно похожа на структуру связного списка, что, возможно, связный список и есть довольно неплохая кандидатура на реализацию паттерна Цепочка обязанностей на уровне конкретики в рамках определённой системы. Уточним, что мы имеем в виду односвязный список. Связный список представляет собой структуру данных, которая напоминает в метафорическом смысле змею, а потому содержит в себе head (голову) – первый элемент и tail (хвост) – последний элемент. Между ними может быть сколь угодно много (зависит от ресурсов конкретной системы) «срединных» элементов. Каждый из элементов содержит в себе два поля: value (значение) и next (ссылка на следующий элемент связного списка). В случае последнего элемента ссылка на следующий элемент, само собой, отсутствует. При необходимости отыскать некоторое значение в связном списке мы можем осуществить итерирование по нему («пробежаться» по списку) и в случае совпадения искомого и найденного значений прекратить, соответственно, итерирование. Классический алгоритм поиска. В связном списке ПК (в этом случае объекты) являются равнозначными и связаны отношениями по типу обоюдостороннего агрегирования.
Значимость паттерна в контексте разработки систем сильного ИИ. В контексте разработки систем сильного ИИ ПП, который являет собой довольно близкий аналог ассоциативного мышления, несомненно, и само собой, на первый взгляд, полезен и годен к реализации. Однако, есть некоторый нюанс. А именно, мы обладает ассоциативным мышлением (предположительно) или, ещё можно сказать, гиперссылочным мышлением, что в данном контексте синонимично. Сама структура нейронных связей подразумевает реализацию чего-либо подобного. Однако, если мы изначально определим, что у системы сильного ИИ должно быть именно ассоциативное мышление, то тем самым загоним систему в некоторые рамки и нарушим свои же принципы построения систем сильного ИИ. Мы помним, что лишены права и возможности разрешать частности и определять конкретику для интеллектуальной системы с претензией на наличие технотропного сознания, так как мы не можем предполагать никакого конкретного итога по причине отсутствия шаблона сильного ИИ. В отличие от построения систем слабого ИИ, тех же нейронных сетей, в которых шаблон несомненно присутствует, цели и задачи определены, алгоритмы прописаны и так далее – здесь ситуация иная. Мы имеем право определять лишь наиболее абстрактные константы функционирования системы – «законы физики» интеллектуальной «вселенной». Мы поступаем так для обеспечения максимально возможной свободы системы, так как считаем, что излишняя конкретизация на раннем этапе создания развивающихся интеллектуальных систем в данном случае является одной из причин отсутствия сильного ИИ. В связи с этим, для извлечения пользы из паттерна Цепочка обязанностей необходимо интенсифицировать уровень абстракции. Вполне возможно, что сильный ИИ будет обладать не ассоциативным мышлением, а мышлением совершенно иного, нам доселе неизвестного, типа. Возможно, синергетическим или квантовым и так далее. Возможно, что ассоциативная связь не самый эффективный тип связи с точки зрения оптимизации по времени или по памяти. К примеру, связь по типу квантовой нелокальности вообще осуществляется за константное время вне зависимости от расстояния между удалёнными конечными точками. Само собой разумеется, что система сильного ИИ должна (в соответствии с одним из основных принципов ООП) быть внутренне сильно связанной и таковой она, несомненно, и будет. Но вот сами типы этих внутренних связей не уточняются: они могут быть как прямыми, так и косвенными (опосредованными), как централизованными, так и децентрализованными, как нисходящими, так и восходящими и так далее. По сути, система в процессе своего функционирования и взаимодействия со средой должна сама сформировать все необходимые себе связи с учётом собственных возможностей и потребностей, а также контекста их реализации. Поэтому «зародыш» системы сильного ИИ должен быть «готов ко всему». Соответственно, в паттерне Цепочка обязанностей нам видятся значимыми следующие аспекты: наглядная демонстрация практической значимости принципа слабого зацепления, а также необходимость установления специфической связи между компонентами системы для эффективной обработки различных, возникающих в результате взаимодействия системы со средой, ситуаций. На основе заимствования абстракций паттерна мы считаем, что при разработке системы сильного ИИ ни сильные сцепки, ни конкретный тип взаимодействия между ПК заранее определять не целесообразно. Здесь мы реализуем достаточно простой принцип возрастания уровня абстракций: если нельзя определять конкретный тип взаимодействия между ПК, то, при необходимости, следует определить для ПК возможность выбирать между различными типами взаимосвязи; ещё лучше определить саму возможность возникновения подобного феномена в рамках системы; ещё лучше, сформировать такую ситуацию, чтобы система сама воспроизвела и определила для себя такую возможность, и так далее выше и выше к форме от содержания.

12.3 Поведенческий паттерн Наблюдатель

Идентификатор. Наблюдатель.
Классические элементы. Субъект, наблюдатель.
Назначение. Как указывается в «Design patterns» Наблюдатель «…определяет зависимость типа «один ко многим» между объектами таким образом, что при изменении состояния одного объекта все зависящие от него оповещаются об этом и автоматически обновляются» [117, С. 280]. Таким образом, основным назначением паттерна Наблюдатель является синхронизация ПК, которые взаимодействуют тем или иным способом и связаны, как правило, по типу зависимости («многих от одного»). Наблюдатель призван существенно упростить и, более того, автоматизировать координацию состояний и поведения различных ПК.
Проблема. Представим себе ситуацию, что в некоторой системе наличествует определённое количество ПК, некоторые из которых ответственны за обеспечение эффективного функционирования системы в целом. Ответственность остальных ПК заключается в том, чтобы, так сказать, поддерживать этих «передовиков» в их деятельности. Поддержка эта может осуществляться различным образом, однако её специфика должна быть определена некоторым конкретным образом для каждой отдельной системы. Проблема заключается в быстрой и эффективной подстройке «помощников» под актуальные задачи «передовиков». Если осуществлять это «руками», то реагирование системы на быстро изменяющуюся обстановку становится недостаточно эффективным. То есть, если сформировать сильное зацепление между субъектом и наблюдателем и обязать субъекта оповещать конкретного наблюдателя о своём состоянии и своих изменениях, то, во-первых, ввиду наличия сильного зацепления данные ПК будет очень сложно повторно использовать и во-вторых, – это совершенно не оптимально. Это примерно, как сравнить две совершенно различные по ресурсозатратности, но одинаковые в плане целей ситуации: у объекта А что-то случилось и теперь он хочет оповестить об этом объектов В и С, лично едет к каждому из них домой, дабы уведомить об изменениях, и – второй пример – осуществление рассылки по электронной почте со стороны объекта А. Для решения данной проблемы и предназначен паттерн Наблюдатель, который, конечно же, близок ко второму примеру.
Концептуальное решение. В рамках системы формируется следующая ситуация. Формируется некоторое количество ПК, каждому из которых назначается определённая ответственность. И, допустим, некоторым из этих компонентов (наблюдателям) важно иметь представление о состоянии других компонентов (субъектов). Вполне возможно, что наблюдателей интересуют не все возможные состояния субъекта, а лишь некоторые из них (например, два из пяти или одно из десяти). В такой ситуации постоянный фоновый мониторинг субъекта со стороны наблюдателя выглядел бы абсолютно неоптимальным. Даже в том случае, если наблюдателя интересуют сто из ста возможных состояний субъекта, то даже так постоянный мониторинг выглядит совершенно излишним. Ведь если при изменении субъекта наблюдатель также обязан измениться, то этот процесс в любом случае дискретен. И даже если наблюдатель обязан изменяться сам в ответ на любую трансформацию субъекта, то в те моменты, когда субъект некоторое время находится в одном и том же состоянии, наблюдатель ничего делать не обязан. И именно поэтому при наличии постоянного контроля состояния субъекта со стороны наблюдателя ресурсы (возможно, нужные для чего-либо другого) растрачиваются впустую, не говоря уже о нарушении принципа слабого зацепления. Решением выглядит следующее: определение механизма подписки наблюдателя на интересующие его изменения состояния субъекта. Происходит это следующим образом. Распределяются роли и ответственности между ПК, определяется механизм подписки и отписки. Затем, в процессе функционирования системы ПК, реализующий роль субъекта, в момент изменения своего состояния со статуса «Х» на статус «У», оповещает всех компонентов-наблюдателей, подписанных именно на переключение статуса субъекта на «У», о том, что его состояние изменилось таким-то образом. В ответ все подписанные компоненты-наблюдатели присылают компоненту-субъекту запрос на конкретику изменений, то есть «спрашивают», каким именно образом им теперь необходимо изменить своё состояние и/или поведение, и затем осуществляют необходимые изменения. Компонент-субъект не знает, какие именно компоненты-наблюдатели на него подписаны и даже не интересуется этим. Компоненты-наблюдатели, в свою очередь, имеют в наличии механизм подписки-отписки и вольны им пользоваться по своему усмотрению (с учётом текущей ситуации в контексте программной системы).
Предполагаемые результаты и преимущества. В качестве результатов полагается оптимизированное взаимодействие ПК друг с другом (субъектов с наблюдателями), то есть некоторая избыточность в виде отсутствия случаев неэкономичного использования ресурсов. Также немаловажным является соблюдение фундаментального принципа слабого зацепления, что позволяет эффективно повторно использовать наличествующие ПК, и предоставляет возможность легче вносить изменения в систему. Также предполагается, что реализация паттерна позволяет поддерживать высокий уровень динамики в рамках системы, то есть все необходимые изменения осуществляются быстро, точно и эффективно.
Гипотетический пример реализации паттерна. Наличествует реальный пример паттерна Наблюдатель – уже упомянутая рассылка по электронной почте. Каждый получатель рассылки не подписан на вообще все рассылки, которые существуют, а также не звонит в организацию, из которой получает рассылку, каждые пять минут с вопросом на тему, что у них там изменилось, и нет ли чего-нибудь нового (клинические случаи мы здесь не рассматриваем). Получатель рассылки подписан на те организации, которые ему интересны и важны, а также (опционально) на те темы (предоставляемые организациями-субъектами), которые ему интересны. Это и есть реализация паттерна Наблюдатель в макромире. Однако данный пример слишком тривиален и интуитивно понятен, а сам паттерн способен проявлять себя несколько сложнее и далеко не так очевидно. 
К примеру. Мы выше говорили о ситуации в рамках программной системы, в которой наличествуют нами определённые ПК, которые несут в себе суть «передовиков», и иные компоненты, реализующие функционал «помощников». Мы также указали на необходимость своевременного и адекватного реагирования «помощников» на изменившуюся для «передовиков» ситуацию в контексте системы. В макромире бывают ситуации, в которых примерно таким же образом реализуется паттерн Наблюдатель. Пример про боксёра перед боем и непосредственно в бою. Он сам является субъектом (в контексте паттерна) и «передовиком». У него наличествует определённая команда: основной тренер, тренера по некоторым отдельным аспектам (общая физическая подготовка, специальная физическая подготовка и так далее), врач, массажист, психолог, секунданты и прочие. В процессе подготовки к бою состояние субъекта-боксёра претерпевает определённые изменения. В том случае, если наблюдатели (команда боксёра) некорректно и/или несвоевременно отреагируют на эти изменения, то пострадает форма боксёра и увеличится вероятность того, что бой будет им проигран. У секундантов та же ситуация наличествует непосредственно во время боя. Чтобы не происходило некорректных реакций со стороны команды на изменения состояния боксёра, определяется механизм подписки. Каждый член команды подписывается на некоторые изменения и берёт их под свою ответственность. Тренера следят за физической формой и за тем, чтобы она соответствовала текущему этапу подготовки, врач отслеживает результаты анализов и диагностирует общее самочувствие по некоторым критериям, психолог следит, соответственно, за психологическим состоянием. То есть врач не подписан на изменения в физической форме боксёра, а психолог не берёт у него анализ крови и так далее. Каждый компонент системы под названием «Боксёр и его команда» отвечает только за то, за что способен ответить максимально эффективно (к слову, это есть пример реализации паттерна Informationexpert (Информационный эксперт) из числа основных GRASP-паттернов). Также любой из команды может отписаться от подписки – перейти к другому боксёру, сменить специализацию или попросту уволиться из команды. То есть мы наблюдаем все основные аспекты реализации паттерна Наблюдатель в макромире.
Осмысление структуры паттерна. Структура паттерна предполагает довольно высокий уровень динамики связей между ПК и определяется особенностями и спецификой каждой конкретной реализации. На отдельный субъект могут подписываться какие угодно наблюдатели, и сам он не интересуется спецификой их реализации. Также наблюдателю ничто не мешает быть подписанным одновременно на несколько субъектов. В то же время для субъекта важно, чтобы его функционирование было успешным и, только лишь в том случае, если программная реализация напоминает пример с боксёром, субъект зависит от своих наблюдателей. С другой стороны, наблюдатели также зависимы от субъекта и успешность их функционирования также им обусловлена. Если взять подобную гипотетическую ситуацию, то тип связи между ПК напоминает обоюдостороннюю композицию. Опять же ничто не мешает реализовать субъект гораздо менее зависимым от наблюдателей или наоборот и тогда тип связи между ПК в системе уже предстанет агрегированием. Или опять же, ничто не мешает реализовать субъект таким образом, чтобы поставить наблюдателей в прямую одностороннюю от себя зависимость. Вообще говоря, ничто не мешает и при реализации поставить субъект в прямую зависимость от его наблюдателей. А если абстрагировать паттерн Наблюдатель ещё сильнее и вновь взглянуть на его структуру, то мы увидим, что, при определённых реализациях, ничто не мешает субъекту и наблюдателю так же динамично меняться местами. Резюмируя, как мы и постулировали изначально, связи в паттерне Наблюдатель динамичны и зависят от контекста каждой конкретной реализации.
Значимость паттерна в контексте разработки систем сильного ИИ. На первый взгляд, данный паттерн, если рассматривать его в контексте разработки систем сильного ИИ, ничем особенным среди прочих паттернов не выделяется. Но если взглянуть несколько глубже, то здесь всё же имеется некоторая уникальная особенность. А именно: беспрецедентный уровень динамики взаимосвязей и ответственностей между ПК, сформированными в рамках реализации паттерна. Казалось бы, что и в этом нет ничего особенного и чрезмерно значимого для реализации сильного ИИ. Однако, нами предполагается, что для создания системы сильного ИИ помимо уже упомянутого «первичного супа» также необходим ещё и механизм запуска процесса зарождения технотропного сознания. Мы с некоторой степенью уверенности можем постулировать, что для возникновения феномена уровня технотропного сознания необходим механизм самоорганизации. То есть механизм, который максимально «автономизирует» систему, делает её внутренне самодостаточной. Разница между самоорганизованными системами и системами, не использующими данный механизм, примерно сопоставима с разницей между летящим снарядом, выпущенным из пушки, и летящей сама по себе птицей: и снаряд, и птица летят, но есть, как говориться, нюанс. Это также примерно сопоставимо с разницей между системами слабого ИИ и сильного: и те, и другие системы будут способны к обработке информации, но нюанс, как водится, останется на своём месте. Далее, если абстрагировать сильнее даже такую абстракцию как самоорганизация, то станет ясно, что все эмерджентные эффекты, все качественные «скачки», критические преобразования и прочие феномены достигаются за счёт всего лишь специфического (определённого в каждой реализации) взаимодействия между компонентами, включёнными в самоорганизационный процесс. Так как самоорганизация не является линейным процессом, то, соответственно, некие прогнозы по поводу достигаемых ею результатов далеко не всегда оказываются верны. Собственно говоря, они верны только в том же смысле, в котором верно в рулетке ставить на чёрное. Соответственно, при определении некой системы, которая после запуска должна будет функционировать на основе механизма самоорганизации, никогда нельзя сказать заранее, какой ПК будет ведущим («передовиком», субъектом) и какой будет последующим (наблюдателем, «помощником»). Структура паттерна Наблюдатель с динамикой связей между компонентами паттерна позволяет предусмотреть подобную ситуацию. Мы можем просто абстрактно определить саму возможность возникновения в системе субъекта и наблюдателя, а какой именно ПК какую позицию займёт и какую ответственность примет – это уже решится «само» в ходе функционировании системы по самоорганизованному типу.

12.4 Поведенческий паттерн Состояние

Идентификатор. Состояние.
Классические элементы. Контекст, состояние.
Назначение. Как указывается в «Design patterns» Состояние «…позволяет объекту варьировать свое поведение в зависимости от внутреннего состояния. Извне создается впечатление, что изменился класс объекта» [117, С. 291]. Паттерн Состояние предназначен для некоторого распределения сложной и обширной логики какого-либо ПК между несколькими ПК, каждый из которых представляет собой некую как бы отдельную ипостась изначального компонента со свойственным ей специфическим функционированием. Паттерн Состояние позволяет обеспечить не допустить нарушения принципа высокой связности, а также, несомненно, выполнение одного из принципов SOLID – принципа единственной ответственности. Более того, здесь мы можем пойти дальше и постулировать, что при помощи данного паттерна обеспечивается выполнение ещё одного из принципов SOLID, а именно – принципа разделения ответственностей. Напомним, что принцип разделения ответственностей тезисно звучит так: ПК не должны зависеть от методов, которые они не используют. В определённом смысле данный принцип можно дополнить (с чуть менее категоричной коннотацией) следующим образом: нежелательно, чтобы ПК зависели от методов, которые они редко используют. Вместо этого продуктивней было бы инкапсулировать содержимое подобного функционала в отдельные ПК. Казалось бы, что выгода неочевидна, ведь в любом случае будет иметь место использование одним программным компонентом либо метода, либо подкласса. На самом деле мероприятие подобного рода усилит связность базового ПК, упростит взаимодействие «пользователя» с системой и сделает саму систему гораздо более понятной с точки зрения управления ею. Можно предположить, что подобных редко используемых методов наличествует довольно большое количество. В каждом из них присутствует своя логика, своё состояние, уникальное поведение. И специфика каждого из этих методов может настолько различаться, что действительно, может показаться, что ПК, при использовании различных своих методов буквально меняет ипостась (классовую принадлежность). Для управления системой в подобных ситуациях и координации её действий и служит поведенческий паттерн Состояние.
Проблема. Представим, что наличествует некоторая система, состоящая из множества сложных ПК. Каждый из компонентов реализует достаточно большое количество вариантов своего поведения, то есть совершенно по-разному самореализуется в зависимости от актуальной ситуации в рамках программной системы. При смене вариантов поведения ПК они претерпевают серьёзные внешние и внутренние изменения и становятся «на себя не похожи». Связь между данными компонентами в рамках системы вариабельна. Соответственно, компоненты такой системы близки к тому, чтобы в полной мере нарушить и принцип сильной связности и из SOLID – принципы единственной ответственности и разделения интерфейсов. При комплексном нарушении этих принципов имеет место такая ситуация, при которой ПК попадает под соответствие метафоры god-object, то есть божественный объект, ответственность которого далеко переходит все допустимые границы. Такой ПК невозможно повторно использовать, его крайне сложно трансформировать, более того – его опасно трансформировать, так как, скорее всего, нарушится функционирование системы в целом. В том же случае, если ответственности подобных компонентов ещё и могут где-либо пересекаться, то система становится откровенно патогенной. Собственно, проблема заключается в том, что необходимо каким-то образом переосмысливать структуру системы и начать следует с переосмысления и трансформирования структуры подобных ПК.
Концептуальное решение. ПК, которые реализуют множество вариантов разнообразного функционала, при каждом из которых их поведение уникально и существенно отлично от иных вариантов, разделяются на части (разделение интерфейса и инкапсуляция логики). Определяется некоторый интерфейс для связи ПК с другими ПК. Этот интерфейс и есть контекст (как классический элемент паттерна Состояние). Далее, определяется второй ПК, который, по сути, представляет собой менеджера состояний и является следующим классическим элементом паттерна – состояние. Этот второй ПК определяет в себе определённое количество своих подкомпонентов (подклассов), каждый из которых представляет собой отдельную ипостась базового компонента, то есть инкапсулирует определённое состояние и некоторое поведение, зависящее от этого состояния. Подкомпоненты базового компонента могут быть абсолютно никак не связаны друг с другом и даже не подозревать о существовании друг друга. И, к слову, это и есть пример хорошей реализации, так как подобные подкомпоненты легки в плане повторного использования по причине высокого уровня автономности и слабого зацепления. Далее менеджер состояний тесно взаимодействует с контекстом (элементом паттерна) и предоставляет ему для использования свои подкомпоненты, подбирая их в зависимости от текущей системной ситуации, резюмированной в запросе со стороны контекста. В общем смысле примерно таким образом и функционирует паттерн Состояние.
Предполагаемые результаты и преимущества. В результате корректного применения паттерна Состояние принципы сильной связности и слабого зацепления не нарушаются, сложная логика базового ПК инкапсулируется в определённые им подкомпоненты, создаётся основа для повторного использования ПК. Реализуются на практике в практическом контексте два из принципов SOLID – принцип единственной ответственности и принцип разделения интерфейса. Сложный функционал базового ПК оказывается разделён и инкапсулирован в некоторое количество подкомпонентов, что существенно упрощает как непосредственно само функционирование системы, так и взаимодействие с ней со стороны «пользователя». Очерчивается чёткая линия демаркации между различными ипостасями базового ПК, что упрощает понимание происходящего со стороны того же «пользователя». Целесообразно будет дополнительно указать на особенности повторного использования подкомпонентов базового ПК, которые можно охарактеризовать как «подсостояния». В том случае, если различные базовые ПК реализуют схожую логику поведения в разных контекстах, то одни и те же «подстостояния» можно дополнительно использовать и в разных контекстах.
Гипотетический пример реализации паттерна. Вообще говоря, сущность паттерна Состояние является чуть ли не злободневно естественной. Абсолютно любой человек ведёт себя по-разному в различных состояниях. Здесь не вполне уместно замечание о том, что бывают «люди слова» и «люди настроения» по той причине, что при смене состояния в любом случае любой человек поведёт себя иначе. Просто у «людей слова» его, возможно, сложнее трансформировать извне, чем у «людей настроения», но при непосредственной смене состояния изменится также и поведение. В этой точке дискурса можно даже утрировать смысл и сказать, что любой человек, будучи у него сломана рука или нога, не сможет вести себя так, как смог бы, будь они не сломаны. То есть абсолютно естественной является смена поведения при смене состояния и человек тут просто как пример. На самом же деле вся природа функционирует точно таким же образом: периоды размножения, циклы миграции – только «большие» примеры. Маленьких можно привести сколь угодно много. И, тем не менее, как следует из специфики описываемых ситуаций применения паттерна в рамках разработки программного обеспечения, все эти примеры, сколько их не приводи, всё же будут не в полной мере соответствовать специфике реализации поведенческого паттерна Состояние. Соответствие не достигается по той причине, что человек не обязательно должен чувствовать себя совершенно другим человеком при реализации различных поведенческих действий в разнообразных состояниях, а паттерн подразумевает (в идеале) воплощение именно такой ситуации. Также человек в одном состоянии обычно знает, что оно у него не одно, а иногда случаются и другие. В рамках же паттерна подкомпоненты совершенно не обязаны знать (в идеале) о наличии других подкомпонентов. Именно поэтому вышеприведённые ситуации в общем смысле схожи, но всё же не полностью соответствуют.
Соответствовать будут несколько иные примеры. А именно: случаи серьёзных психических отклонений психотического типа. Находясь в рамках психотического контекста субъект способен, пребывая в некотором состоянии, не подозревать о других своих состояниях и в каждом отдельном состоянии реализовывать совершенно различный функционал. Наиболее знаменитым (художественным) примером наглядной демонстрации подобной ситуации является известный фильм Дэвида Финчера, снятый по роману Чака Паланика, «Бойцовский клуб». У главного героя произведения шизофрения, которая выражается в разделении личности на две ипостаси: невзрачного офисного клерка, озабоченного обустройством и декорированием собственного жилища и брутального харизматичного торговца мылом, подрабатывающего по ночам на различных позициях. Герой, который большую часть фильма находится в первой ипостаси, не подозревает о наличии у себя второй личности, которая активно действует. Каждая из ипостасей героя обладает своими собственными состояниями, системой реализуемых образов действия, уникальными системами ценностей и отдельным экзистенциальным статусом, то есть вся логика их функционирования является инкапсулированной. И в то же время обе ипостаси – это «две стороны одной монеты», так как они обе всё же являются одним и тем же человеком, что и выясняется к концу произведения. Причём в случае с данным примером наличествует ещё одно крайне интересное замечание, а именно вторая личность знает о наличии у себя личности первой и владеет всей информацией об этой личности. То есть мы видим, по сути, нарушение принципа слабого зацепления. А интересным тут является то, что именно за нарушение этого принципа вторая личность, в конце концов, и поплатилась. Причина тут простая: первая личность была связана со второй по типу агрегирования, то есть вполне могла автономно существовать, а вот вторая личность была связана с первой по типу композиции, то есть без неё существовать не могла.
Осмысление структуры паттерна. Структура паттерна заключается в том, что все ПК его составляющие, завязаны, в общем-то, на контекст, как элемент паттерна. ПК, который представляет собой элемент паттерна состояние, инкапсулирует логику своих ипостасей в виде подклассов и отношения здесь сразу видны – это, разумеется, наследование. Далее, состояние связано с контекстом по типу композиции, то есть состояние не способно существовать без контекста, а контекст связан с состоянием по типу агрегации, так как сам контекст существовать всё равно сможет, просто в случае его существования без всей остальной структуры паттерна он превращается в god-object и нарушает сразу несколько фундаментальных принципов ООП.
Значимость паттерна в контексте разработки систем сильного ИИ. На первый взгляд может показаться, что поведенческий ПП систем, функциональная и структурная суть которого нагляднее всего демонстрируется на примере шизофрении, не может быть полезен в качестве основы для разработки интеллектуальных систем. Но это, как водится, только на первый взгляд. Мы помним, что не имеем права судить о сильном искусственном интеллекте, основываясь на информации о человеке, так как не имеем логически обоснованного права вообще предполагать о нём излишнюю конкретику, которая оставляется нами на откуп сфере научной фантастики и области паранауки. Мы же подходим к разработке систем сильного ИИ с (если можно так выразиться) максимально «ленивой» позиции. То есть мы стараемся сделать как можно меньше, чтобы затем сама система сделала как можно больше. Но сделать это «меньше» мы уж должны максимально качественным образом – образом, наиболее соответствующим тем необходимостям построения систем сильного ИИ, которые нам представляются наиболее значимыми, по принципу возрастания уровня абстракций. К тому же следует заметить, что предположительно сам факт наличия сознания у субъекта способен обуславливать некоторые побочные эффекты присутствия сознания. То есть мы можем предположить, что если бы в контексте не подразумевалась возможность возникновения шизофрении, то наличие возможности существования такого феномена, как сознание, также было бы под вопросом. Конечно, это предположение звучит несколько софистически, но, тем не менее, предположение о том, что психические заболевания являют собой побочный эффект наличия сознания, выглядит непротиворечиво хотя бы потому, что психические заболевания фиксируются только лишь у человека, и сознание также только у человека замечено (если признать факт его существования, с чем не все согласны).
Таким образом, мы не имеем права ограничивать когнитивные способности интеллектуальной системы и должны следовать этому правилу с самого начала. К тому же, возможно, что именно психотичный тип мышления и покажется системе сильного ИИ наиболее продуктивным: пример Джона Форбса Нэша-младшего с шизофренией или примеры Винсента Ван Гога, Стэнли Кубрика или Чарльза Дарвина с расстройствами аутического спектра показывают, что психические отклонения иногда не только не мешают, но и, возможно, способствуют развитию интеллектуальных способностей высокого уровня. К тому же в психоанализе, в рамках теории защитных механизмов, также наличествует один из механизмов защиты субъекта от негативного опыта путём изоляции оного от всей остальной системы психической деятельности. Учитывая все противоречия, можно, по меньшей мере, постулировать, что поведенческие ПП, которые потенциально способны оказывать негативный эффект, также могут быть использованы в контексте разработки систем сильного ИИ при наличии у них иных значимых качеств и свойств.
И в случае с паттерном Состояние мы полагаем его полное соответствие вышесказанному. Так как паттерн способен обуславливать необходимую динамику состояний системы быстрым и оптимизированным способом и, соответственно, способен обеспечивать надлежащую смену состояний ПК, опираясь на предъявляемые контекстом необходимости. Основной же его ценностью является как раз инкапсуляция функционала различных состояний и самих этих состояний в некую общую форму, превращая тем самым «размазанную» по системе логику в полноценные, отдельные ПК, преобразуя части системы в иные феномены с качественно новым экзистенциальным статусом.

12.5 Поведенческий паттерн Команда

Идентификатор. Команда.
Классические элементы. Команда, конкретная команда, клиент, инициатор, получатель.
Назначение. Как указывается в «Design patterns», Команда «…инкапсулирует запрос в объекте, предписывает единообразный интерфейс для выдачи запросов, с помощью которого можно сконфигурировать клиенты для обработки разных запросов» [117, С. 76]. Назначение данного паттерна, если сразу абстрагировать его суть, примерно следующее: преобразовывать некоторые данные из одного типа и вида в другой тип и вид для того, чтобы системе было удобнее обрабатывать эти данные, и для того, чтобы не иметь необходимости дублировать определённые операции. Это самое преобразование должно позволить сделать систему более компактной и оптимизированной, позволить повторно использовать один и тот же интерфейс целого слоя ПК, а также ослабить зацепление.
Проблема. В определённой системе присутствует некоторое количество ПК, которые, для целей описательного прагматизма, условно разделены на две большие группы: в первую группу входят компоненты, которые посылают некоторые запросы к компонентам второй группы. Каждый из компонентов первой группы должен хранить ссылку на определённый компонент из второй группы, что само по себе уже является некоторым нарушением фундаментального принципа слабого зацепления. Далее, некоторые из компонентов первой группы могут быть схожи по функционалу и обращаться к одним и тем же компонентам второй группы. Также при необходимости добавить какую-либо новую функциональность в систему, её приходится либо реализовывать с нуля, либо подключать механизм наследования в тех случаях, когда можно было бы обойтись делегированием, ввиду отсутствия возможности пользоваться некоторым общим интерфейсом для всех компонентов первой группы. Все эти аспекты в совокупности делают систему довольно ригидной, сложно расширяемой и чересчур сцепленной. Отсутствует некоторый, если можно так выразиться, буферный слой между ПК первой и второй групп. Для решения комплекса подобных проблем и предназначен паттерн Команда.
Концептуальное решение. Мы хотим сделать так, чтобы клиент (элемент паттерна) мог активировать инициатор (элемент паттерна), который затем перенаправит запрос, инициированный клиентом, к получателю (элемент паттерна), который обработает запрос клиента и вернёт соответствующий ответ. Собственно говоря, это может с внешней точки зрения довольно неплохо работать, примерно так же, как в системе, которая описывалась нами выше. Но только некоторое время и только с внешней стороны. Со стороны структурной организации и оптимизации функционирования подобная система не особенно жизнеспособна. Однако есть возможность заставить систему работать гораздо лучше. И это осуществляется непосредственно за счёт введения того самого буферного слоя логики, о котором мы говорили выше. Этот слой, соответственно, и представлен командой (элемент паттерна). Эта самая команда представляет собой (в классическом случае) класс, который определяет некий общий интерфейс функционирования всего слоя в целом. Непосредственно локально, на местах, этот слой представлен подкомпонентами компонента команда – конкретными командами (элемент паттерна). В результате происходит примерно следующее. Клиент инициирует некий запрос (допустим, что нужно что-либо сделать) и, соответственно, активирует инициатор: например, нажимает кнопку, если представить, что клиентом в данной ситуации является человек, или вызывает определённый метод, если клиентом является другой класс. Каждый инициатор хранит у себя ссылку на соответствующий ему подкомпонент и, после своей активации, инициатор вызывает определённый метод этого самого подкомпонента (к примеру: execute, то есть «выполнить»). Здесь стоит заметить, что сам инициатор не владеет информацией о деталях реализации свойственного ему подкомпонента и, соответственно, не знает и не может знать о том, что именно подкомпонент будет делать дальше, и к какому именно получателю он будет обращаться, таким образом, принцип слабого зацепления соблюдается. Ещё более конкретно это может выглядеть как хранение инициатором ссылки на вызов конструктора класса команда, который возвращает готовый подкомпонент (в данном случае, объект) с полями, заполненными соответствующим образом. В подкомпоненте существует ссылка на необходимого получателя и именно ему и переадресуется запрос для его дальнейшей обработки. В результате получается, что в рамках системы наличествуют те же две группы компонентов, что и изначально, но за счёт добавления группы третьей и представления запроса в виде объекта с полями, заполненными соответствующим (уникальным) образом, мы не только делаем систему гибкой и эластичной, но и ещё и гораздо проще организованной со структурной и функциональной точек зрения. Стоит отдельно отметить, что в рамках функционального программирования паттерн подобного рода также можно было бы реализовать: функция-инициатор через некую буферную функцию обращается к функции-обработчику – звенья цепочки примерно схожи. Но дело в том, что у объектов есть некоторое преимущество перед функциями, а именно – они не являют собой просто последовательность алгоритмических действий, а представляют собой феномен более высокого порядка – организованный ПК с инкапсулированной логикой и некой идентичностью. Например, состояния объекта, как уже указывалось при исследовании поведенческого паттерна Хранитель, можно сохранять и затем повторно использовать, возвращаться к ним и так далее – то, о чём как раз и говорится в вышеприведённой цитате из «Design patterns».
Предполагаемые результаты и преимущества. К основным результатам применения паттерна Команда обычно относят реализацию на практике (со всеми вытекающими преимуществами) двух из принципов SOLID: принципа открытости/закрытости (в виде определения метода execute для каждой конкретной команды) и принципа разделения интерфейса (в виде отделения части логики, отвечающей за инициацию запроса, от части логики, отвечающей за отправку запроса получателю). Также очевидно, что обеспечивается соблюдение принципа слабого зацепления за счёт введения буферного слоя в систему. Помимо этого, паттерн обеспечивает высокую эластичность системы и отсутствие сопротивляемости изменениям.
Гипотетический пример реализации паттерна. Пусть и не так очевидно, как некоторые иные паттерны, но в мире существуют практически буквальные реализации данного паттерна. Но мы представим такой пример. Допустим, у нас есть некоторая сумма денег, которую мы хотим куда-либо передать. Разумеется, что ехать в место доставки очень далеко, и мы хотим совершить перевод. У нас имеются в наличии какие-то суммы в разных валютах. Также на готовы предоставить свои услуги определённое количество трансферных бюро, каждое из которых специализируется на одной определённой валюте. У нас есть возможность некоторое время походить по данным бюро, в каждую занеся тот тип валюты, с которым работает конкретное трансферное бюро, и совершить некоторое количество переводов разных сумм в разной валюте на один и тот же адрес доставки. Это, конечно, очень всё неудобно, но если нет выбора, то будет совершено. Но тут открывается новое трансферное бюро, которое предлагает следующую услугу: перед переводом обменять все деньги во всех валютах на их внутреннюю виртуальную валюту «TransferCoin». При осуществлении данного обмена все внесённые деньги тщательно документируются и сохраняются в единую структуру данных, функционирующую по принципу key-value, где key – валюта, а value – сумма в этой валюте. Далее этой структуре данных присваивается некий уникальный идентификатор (например, хэш-значение), который при расшифровке обозначит точную сумму наших «TransferCoin». Далее эта структура данных с её идентификатором передаётся адресату, где она расшифровывается, структура данных открывается, и нашему адресату передают на месте ровно то, что мы ему отправили – суммы в валютах. Предположим, что заработок конторы заключается в разнице курсов «TransferCoin» при пересылке и получении. Вот эта контора – приближённый пример реализации паттерна Команда.
Осмысление структуры паттерна. В рамках структуры данного паттерна отношения между компонентами сводятся к следующим: Команда и конкретные команды, само собой, взаимодействуют по типу наследования. Отношения клиента и инициатора всё же напоминают метафоричную ситуацию «пахнет ли роза, если её никто не нюхает». То есть, с одной стороны, клиент полностью зависит от того, насколько успешно он достигнет своей цели, которую он пытается достичь при помощи обращения к получателю при посредстве инициатора. С другой стороны, не будь клиента, вся система выглядит бессмысленной. Прямой связи инициатора с получателем обычно стараются избегать вовсе, дабы излишне не зацеплять ПК друг с другом – их связь осуществляется через конкретную команду. С инициатором конкретная команда связана по типу композиции, то есть инициатор создаёт объект конкретной команды и без инициатора его бы не существовало. Команда же (базовая команда) связана с получателем по типу композиции, так как она изначально создаётся «под него» и для работы с ним.
Значимость паттерна в контексте разработки систем сильного ИИ. Данный поведенческий паттерн актуально использовать в контексте разработки систем сильного ИИ вполне логичным образом: для преобразования, упаковки и передачи информации. Однако необходимо, разумеется, абстрагировать суть паттерна. А в абстрактном виде она, как уже постулировалось, заключается в наличии буфера между ПК, передающим информацию некоторого типа другим программным компонентам для её последующей обработки и воспроизведения некоторой реакции на эту информацию. В контексте данного паттерна наиболее значимой абстрактной характеристикой является сама специфика преобразования информации, сама её идея: она – информация – из, можно так выразиться, некоторых импульсов преобразуется в качественно иной вид – в нечто очень отдалённо, но всё же более подобное на «субъекта действия», чем просто совокупность однонаправленных импульсов (абстрактные запросы, к примеру). Ведь объект – это уже нечто упорядоченное, обладающее некой формой, определённым содержанием, специфической идентичностью и так далее. Ещё раз – спецификой паттерна Команда является качественное преобразование информации с её воплощением в виде упорядоченной информационной структуры с потенциалом использования этих составляющих блоков информации по усмотрению самого субъекта (информационной структуры). Если абстрагироваться ещё сильнее, то можно будет (примерно, как в вышеописанной ситуации о «TransferCoin») говорить о некоем аккумулировании в единую информационную структуру разнородных, но по какому-либо критерию всё же связанных, блоков данных. И, что крайне важно, эта структура должна будет являться по отношению ко всей совокупности данных её составляющих чем-то большим, чем-то качественно новым, то есть чем-то эмерджентным. Выглядит это, как будто только что существовали просто некоторые блоки данных, а теперь мы уже имеем дело с вполне определённой сущностью, с новым субъектом «инфополя». Это должно что-то напоминать и, разумеется, напоминает. А именно – гипотетический процесс зарождения системы сильного ИИ.
Примеры, напоминающие реализацию подобного механизма, предположительно наличествуют также и в сфере человеческой психики и являются определёнными и задокументированными в некоторых психотерапевтических направлениях, в частности, в психоанализе. У одного из представителей кляйнианского психоанализа У. Р. Д. Фейрбейрна имеется теория формирования психики, основанная на специфических отношениях младенца с окружающим миром. В частности, он упоминает расщепление целостной психики субъекта на три части, одной из которых является так им определённый «внутренний диверсант», который представляет собой антилибидную часть «Я» [122]. «Внутренний диверсант» представляет собой активно действующего психического субъекта, разумеется (в духе психоанализа), неосознаваемого. Так вот, что интересно, это точно так же, как и в случае с абстрагированием сути паттерна Команда, происходит качественное преобразование изначально разрозненных блоков данных путём их оформления в сущность нового порядка, способную активно действовать в рамках доступного ей контекста. Предполагаемый феномен внутреннего диверсанта приведён просто в качестве примера механизма, функционирующего в стиле тех же самых механизмов, которые наличествуют у абстрагированного паттерна Команда.
Резюмируя данный паттерн, стоит отметить, что именно заложенное в его суть качественное преобразование данных с последующим созданием на их основе новой сущности, являющейся чем-то большим, чем все данные вместе взятые, являющейся, соответственно, чем-то эмерджентным – именно этот аспект Команды и стоит отдельно позаимствовать для нужд разработки систем сильного ИИ, по причине чрезвычайной значимости этого аспекта в контексте нашего исследования и его самоорганизующей природе. 

12.6 Поведенческий паттерн Интерпретатор

Идентификатор. Интерпретатор.
Классические элементы. Абстрактное выражение, терминальное выражение, нетерминальное выражение, контекст.
Назначение. Как указывается в «Design patterns», Интерпретатор «…для заданного языка определяет представление его грамматики, а также интерпретатор предложений этого языка» [117, С. 236]. Собственно говоря, определение паттерна Интерпретатор, которое приводится выше, несколько вырвано из контекста. На самом деле имеется в виду практический буквальный конструктор для анализа данных, дальнейшего их преобразования и синтеза в виде решения некоторой формализованной задачи, основанной на этих данных. Этот конструктор предназначен осуществлять разбор некоторой задачи на составляющие, что позволяет ему «понять» задачу, преобразовать её в необходимый для решения вид и затем непосредственно решить.
Проблема. Классическое определение подходящей для Интерпретатора проблемы описано в «Design patterns» и звучит так: «Если некоторая задача встречается достаточно часто, то имеет смысл представить ее конкретные проявления в виде предложений на простом языке» [117, С. 237]. То есть, у нас имеется некоторая задача, состоящая из определённого конечного количества составляющих, то есть подзадач. Количество возможных подзадач не должно быть большим, иначе данный паттерн не подойдёт к применению в контексте. Также мы имеем возможность формализовать задачу, то есть преобразовать её в простые формы каких-либо символов (неважно, каких именно: английский алфавит, римские цифры или скаляры алгебры логики), которых также небольшое количество. Смысл преобразования задачи в том, что после трансформации она может быть легко решена, то есть в процессе преобразования мы должны максимально упростить, проанализировать, разложить на составляющие, данную нам задачу. Задачи, которые решать сложно, также вряд ли подойдут для реализации паттерна Интерпретатор. По итогу мы хотим создать автоматический решатель задач такого рода и по описанной схеме.
Концептуальное решение. В контексте решения вышеописанной проблемы подразумевается некоторый, логично будет так сказать, решатель. Этот решатель представляет собой, как уже постулировалось выше, конструктор. Вышеописанные классические элементы паттерна Решатель способны, в зависимости от текущего состояния контекста, выполнять различные задачи. В частности, терминальное выражение являет собой ПК, ответственный за атомарные единицы какого-либо языка, то есть за те конструкции языка, которые далее уже неразложимы на составляющие (символы, к примеру). Нетерминальное выражение, в свою очередь, представляет собой ПК, который ответственен за правила построения синтаксических конструкций из терминальных символов (тех, за которые ответственен предыдущий компонент), то есть он ответственен за правила конкретного языка. Контекст (элемент паттерна) представляет собой ПК, который ответственен за общую информацию о происходящем в процессе интерпретации в каждый конкретный момент. Ну и абстрактное выражение является программным компонентом, который определяет некий общий интерфейс, в частности, специфику самого метода интерпретирования. Является понятным, что в данном контексте должен использоваться рекурсивный алгоритм. Конструктором же в сущности данный паттерн является по той причине, что итогом всего целостного процесса интерпретации является абстрактное синтаксическое дерево. Абстрактное же синтаксическое дерево (AST) представляет собой специфический способ фиксирования и репрезентации иерархически распределённой информации на каком-либо конкретном языке.
Предполагаемые результаты и преимущества. Предполагается, что реализация паттерна Интерпретатор позволяет автоматизировать интерпретирование грамматических конструкций простого языка. Также, при применении паттерна добавить некоторые новые правила или же терминалы будет достаточно несложно, то есть мы получим масштабируемость системы в качестве преимущества. Разбор каких-либо задач, сопоставление строк, определение специфики и особенностей выражений на конкретном языке – за все данные операции может быть ответственен паттерн Интерпретатор.
Гипотетический пример реализации паттерна. Проще всего осмыслить примерную реализацию данного паттерна, как ни странно, на конкретном примере использования возможностей функционального программирования. Мы выберем для демонстрации какой-нибудь эзотерический язык программирования, допустим MiniStringFack. Данный эзотерический язык программирования, или сокращённо эсоланг, является дочерним языком по отношению к довольно известному эсолангу Brainfack. Мы выбрали его по причине его наибольшей простоты. Основу грамматики данного языка составляют всего 2 символа (попадающие под юрисдикцию терминального выражения, если реализовывать паттерн в рамках ООП), но, несмотря на это, на нём можно написать любой текст. В качестве некоего хранилища выступает всего одна ячейка памяти (в данном случае, попадающая под юрисдикцию контекста), которую допустимо представить в виде переменной с числовым значением 0. Правила синтаксиса (попадающие под юрисдикцию нетерминального выражения) следующие: при попадании на входную ленту или, можно перефразировать, в поле зрения интерпретатора символа плюс (+) значение ячейки памяти увеличивается на 1, а при попадании в поле зрения интерпретатора символа точки (обычная точка) значение выносится на выходную ленту. Контекст следит за тем, чтобы значение ячейки памяти не становилось больше 256. В противном случае он сбрасывает значение ячейки на 0. Абстрактное выражение (элемент паттерна) в данном случае может быть ответственным за конечное преобразование данных после их попадания на выходную ленту.
Таким образом, простейший интерпретатор будет выглядеть как функция, которая получает на вход строку, состоящую из последовательно расположенных плюсов и точек. В функциональной области видимости определяется переменная и инициализируется числовым значением 0. Также определяется некоторая выходная лента, которая представлена какой-либо удобной структурой данных, например, массивом или строкой (разумеется, что строка в данном случае предпочтительнее, но тогда необходимо сразу преобразовывать данные после каждой встреченной во входной строке точки). Далее запускается цикл, который последовательно считывает каждое значение входной строки и далее реализует логику в зависимости от того, представлено оно точкой или плюсом. В случае плюса – значение переменной, которая хранит наши данные, просто увеличивается на 1, а в случае точки – значение переменной так же просто добавляется на выходную ленту после (возможно) предварительного преобразования. Цикл заканчивается тогда, когда закончилась строка. После завершения цикла функция осуществляет возврат значения, которое представлено (как правило) каким-либо текстом. Вот и вся, собственно, реализация паттерна Интерпретатор в стиле функционального программирования.
В контексте же макромира примеры интерпретации текстов окружают нас уже практически в любой момент.
Осмысление структуры паттерна. В контексте структуры данного паттерна наличествует несколько разновидностей взаимосвязи между ПК. Терминальное выражение и нетерминальное выражение связаны с абстрактным выражением отношениями наследования, так как они оба являются его подклассами и абстрактное выражение определяет их общий функционал, в частности (в контексте классической реализации), абстрактный метод Interpret. Так как контекст (элемент паттерна) содержит в себе общие данные о ходе интерпретации и является глобальной точкой доступа, то он не связан напрямую с остальными элементами. Он связан с дополнительным элементом паттерна, который также выделяют в отдельный компонент, а именно – с клиентом. Однако мы, по уже сложившейся в контексте нашего исследования традиции, посчитали, что в данном конкретном случае дополнительное обособление клиента в отдельный ПК и/или элемент паттерна – операция излишняя, так как какой-либо пользователь всегда подразумевается у любого системного процесса. Мы считаем, что целесообразно обособлять клиента в отдельный компонент лишь в том случае, если на него в контексте паттерна возложены какие-либо функции (в общем смысле слова) помимо осуществления «заказа».
Значимость паттерна в контексте разработки систем сильного ИИ. В процессе разработки систем сильного ИИ поведенческий паттерн, который, по сути, отвечает за целостное восприятие (не за ощущения, а именно за восприятие), несомненно, важен. В случае с данным паттерном, если представить его в виде некоторой системы ПК, которая ответственна за восприятие, его терминальным выражением будут как раз ощущения и вообще все блоки воспринимаемой им информации. Они будут представлены в виде определённого полей данных, которые затем, в ходе взаимодействия системы со свойственной ей средой, будут упорядочиваться согласно выявленным системой закономерностям. Эти закономерности затем будут попадать под юрисдикцию нетерминального выражения. Контекст же тут будет выступать в качестве некоего критерия успешности процесса интерпретирования данных со стороны системы. То есть некоторые элементы паттерна Интерпретатор должны сами реализовываться системой в процессе её развития и совершенствования, потому что только самой системе «видней», какие именно закономерности полей данных важны непосредственно для неё. Таким образом, мы полагаем, что из структуры данного паттерна, для нужд построения систем сильного ИИ, было бы уместно позаимствовать такое образование как абстрактное выражение, которое будет определять именно сами возможности взаимодействия системы со средой, задавать в целом тот потенциал интерпретирования данных, на который по минимальным параметрам должна быть способна система. Для определения же успешности/неуспешности интерпретирования данных считаем необходимым позаимствовать принцип функционирования элемента контекст, который будет определять, насколько какая-либо конкретная интерпретация полей данных помогла системе успешно функционировать. В общем, резюмируя паттерн, стоит отдельно заметить, что процессы ощущения и восприятия окружающей среды со стороны сильного ИИ – это одни из немногих составляющих системы, которые придётся определить хоть сколько-то конкретно, хотя бы для начала процесса запуска системы.

12.7 Поведенческий паттерн Стратегия

Идентификатор. Стратегия.
Классические элементы. Стратегия, конкретная стратегия, контекст.
Назначение. Как указывается в «Design patterns», назначение паттерна Стратегия «инкапсуляция алгоритма в объект. Основными участниками паттерна являются объекты-стратегии, инкапсулирующие различные алгоритмы, и контекст, в котором они работают» [117, С. 56]. Данный паттерн предназначен для эффективного реагирования на динамически изменяющуюся ситуацию путём выбора одного из различных вариантов этого реагирования. Изменение образа действия в runtime программной системы вместе с соответствующим ситуации выбором одного из числа многих варианта этого образа действия и составляет предназначение паттерна Стратегия.
Проблема. По сути, проблема в данном случае определяется двояко, то есть можно сказать, что мы имеем дело сразу с двумя проблемами, а если копнуть глубже, то окажется, что с целым комплексом. В описании проблемы мы, в данном случае, допустим себе некоторый уровень конкретики, так как речь всё же про паттерн Стратегия – это всё же про менеджмент алгоритмов. Во-первых, у нас в наличии имеется довольно сложная система, состоящая из большого количества вариантов действий в зависимости от ситуации. Визуально и в упрощённом виде это легче всего представить, как некоторый большой и запутанный оператор ветвления (if/else) с вложенными различными условиями. С такой системой самой по себе очень сложно и трудоёмко работать и довольно неудобно добавлять некую новую функциональность. Система, что очевидно, является единой в том смысле, что буквально представлена всего одним программным компонентом. Сразу становится заметно, что в подобной ситуации происходит нарушение принципа сильной связности и принципа SOLID – единственной ответственности. Система-компонент представляет собой god-object в чистом виде. И с этим, конечно же, необходимо уже на данном этапе что-то делать, то есть осуществлять глубокий рефакторинг программной системы, а то и просто переделать с нуля. Далее, второй момент: на каждой из ветвей оператора ветвления в рамках нашей системы заложен некоторый цикл. Пусть это будет, для примера, цикл поиска нужного значения в какой-либо структуре данных. Причём, следуя логике нашего (конечно, утрированного) примера системы, представим, что один и тот же цикл, работающий с различными данными в зависимости от ветви оператора, буквально каждый раз с нуля прописан в каждой ветви. Здесь, помимо откровенной бесполезности и неэкономичности подобного решения, также нарушается и один из организационных принципов ООП – DRY, то есть «Не повторяйся». Этот принцип свидетельствует в числе прочего о том, что одну и ту же логику в рамках одной и той же системы нельзя определять дважды, напротив, её необходимо преобразовывать в такой вид, который позволит её многократно использовать (например, вынести в отдельную функцию). И, соответственно, данный приведённый фактор также существенно затрудняет не только внесение в систему изменений, но и просто её обслуживание. Далее, пусть мы попробовали бы всё же вынести неэффективно и неудобно повторяющуюся логику в какой-либо отдельный ПК, а затем попробовали бы передать в неё также и контекст. Здесь уже всё зависит от конкретной системы. С системой, которая работает со сложными контекстами, такое будет совершенно неудобно осуществлять по причине слишком большого количества аргументов, необходимых для описания контекста. Мы же хотели бы, в идеале, эффективно функционирующую систему, масштабируемую, реализующую своё поведение в зависимости от контекста, податливую к необходимым изменениям и удобную в обслуживании. И вот тут на помощь приходит паттерн Стратегия.
Концептуальное решение. С точки зрения функционального программирования решение, подразумеваемое Стратегией, также реализуемо. Однако, всё же удобнее реализация при помощи методологии ООП. Итак, мы определяем несколько ПК. Стратегия (элемент паттерна) будет далее определять некий общий интерфейс для всех конкретных стратегий (элемент паттерна). Контекст же (элемент паттерна), также представленный отдельным программным компонентом, будет определять специфику актуальной для системы ситуации и хранить у себя ссылки на все доступные конкретные стратегии. При возникновении какой-либо ситуации в рамках системы ПК, оформленный в виде контекста, определяет специфику этой ситуации и далее делегирует ответственность одной из доступных ему конкретных стратегий, представленных ПК, инкапсулирующими в себе некоторый алгоритм, специально подобранный для эффективного функционирования именно при контексте таковом, как актуальный. Соответственно, система разделена на несколько ПК, в каждом из которых инкапсулирована логика, необходимая для осуществления строго конкретной деятельности (логика не будет повторяться, если она одинакова). С подобной системой удобно работать, она легко принимает добавления нового функционала (новых конкретных стратегий) путём простого создания подклассов класса стратегия.
Предполагаемые результаты и преимущества. В рамках реализации поведенческого паттерна Стратегия, за счёт инкапсуляции логики в ПК, мы способны избежать применения сложных и запутанных операторов ветвления. Также среди преимуществ использования данного паттерна называется высокая эластичность системы в плане масштабируемости и расширяемости, удобство взаимодействия с ней и достаточно лёгкая её поддержка.
Гипотетический пример реализации паттерна. Довольно показательным примером, иллюстрирующим практическую значимость данного паттерна, являются различные системы подборки, коих в наше время великое множество. Для примера: приложение, которое подбирает оптимальную программу тренировки в зависимости от актуальных потребностей пользователя. Приложение принимает некоторые входные данные, касающиеся антропометрических особенностей пользователя (рост, вес, далее – опционально), его возраст и предпочитаемый им вариант тренировок. Вариант можно выбрать из предоставляемых, которых всего наличествует три: тренировка «на массу», тренировка «на силу» и тренировка «на сушку». У системы в её базе данных (к которой, скорее всего, имеет доступ контекст, как элемент паттерна) имеется определённое количество различных упражнений. При введении пользователем всех тех данных, которые запрашивает приложение, и выбором одного из трёх вариантов тренировки, контекст осуществляет фильтрацию базы (в данном случае под базой данных имеется в виду просто некоторая структура данных, а не, к примеру, MySQL) данных, тем самым осуществляя подготовку к эффективному подбору упражнений со стороны алгоритма. После фильтрации контекст вызывает одну из трёх конкретных стратегий («масса», «сила», «сушка»), передавая ей доступ к обновлённой базе данных, а также передавая в качестве аргументов данные, введённые пользователем. Далее конкретная стратегия осуществляет подборку определённых упражнений, в соответствии с имеющимся у неё алгоритмом, а затем возвращает уже подобранную программу тренировки с описанием упражнений, количества подходов, повторений, временем отдыха между подходами и тренировками и так далее. Как видно из примера: ответственности распределены, алгоритмы и логика подборки инкапсулированы, единый для всех конкретных стратегий интерфейс определён (его определяет элемент паттерна – стратегия), фундаментальные принципы соблюдены, структурные и организационные также. Система функционирует со всеми необходимыми для эффективной работы особенностями и в том случае, если нам понадобится добавить какую-либо новую функциональность в плане конкретных стратегий (например, тренировка «на силовую выносливость» или «взрывную силу»), то нам просто нужно будет создать подкласс класса стратегия и инкапсулировать в него алгоритм подбора. Это – пример хорошей реализации паттерна Стратегия на практике.
Осмысление структуры паттерна. Структура данного паттерна не является особенно запутанной и слишком динамичной, напротив, роли каждого ПК чётко очерчены и в runtime не трансформируются. В runtime может осуществляться только смена контекста и, как следствие, смена конкретной стратегии, о чём, собственно, и говорилось в цитате из «Design patterns». Очевидно, что конкретные стратегии связаны со стратегией отношениями наследования. Контекст же связан со стратегией (в классическом понимании) двусторонними отношениями. Со стороны контекста отношения строятся по типу агрегирования, так как без стратегии контекст бесполезен и не сможет существовать, а со стороны стратегии типом отношений является композиция, так как сама стратегия и являет собой центральный ПК всего паттерна (достаточно вспомнить наш пример «плохой» программной системы, в которой стратегия была вообще единственным программным компонентом).
Значимость паттерна в контексте разработки систем сильного ИИ. Если расценивать паттерн Стратегия с точки зрения его полезности для разработки систем сильного ИИ, то смена стратегии поведения вслед за изменившейся ситуацией смотрится примерно таким же образом, каким выглядит смена поведения в зависимости от настроения (как в паттерне Состояние), то есть кажется, в общем-то, абсолютно тривиальной и по умолчанию подразумевающейся. И так действительно и есть. Но, если несколько абстрагировать суть паттерна Стратегия, то станет видно, что она – суть паттерна – заключается не в утверждении того, что необходимо изменять стратегию действий вслед за изменением контекста (ещё раз – это и так понятно), а в том способе, при помощи которого это реализуется. А именно (примерно так же, как и в случае с паттерном Команда) – суть в инкапсуляции. А также ещё и в некоторой (пусть и очень обобщённо) реализации полиморфизма, который, как мы помним, в числе прочего также представляет собой способность дочерних классов быть использованными в контексте, который изначально предназначался для класса родительского. То есть мы, в контексте структуры паттерна и специфики его реализации, видим некоторый уже готовый итог развития системы. И выражается это в том, что мы от плохо дифференцированной, большой и неудобно функционирующей системы, представленной одним «неповоротливым» программным компонентом, переходим к логически непротиворечиво организованной, структурно оформленной и высокоэффективной системе, представленной некоторым количеством разнообразных ПК, каждый из которых выполняет свою задачу и несёт на себе свою ответственность. И эта система хорошо работает в целом. Если ненадолго «забежать на верхний этаж абстракции», то можно будет постулировать, что именно способом, подобным вышеописанному, из хаоса и получается нечто упорядоченное. В случае с данным паттерном это самое «упорядоченное» представлено совокупностью неких акторов, то есть субъектов (как ни странно представленных объектами), расположенных в строгой иерархии и разделённых по обязанностям и ответственностям. И организованность подобного рода была достигнута в процессе «эволюции» программной системы «от хаоса – к упорядоченному». Это, опять же, может нечто напоминать и, разумеется, напоминает. А именно – напоминает то, как мы себе представляем развитие системы сильного ИИ. Примерно таким образом нам и видится его эволюция. То есть из изначально не вполне чётко определённой системы, обладающей некоторым потенциалом (только лишь потенциалом, а не его конкретной реализацией) формирования в себе тех или иных механизмов, по итогу её развития получается эффективно функционирующая сложная, распределённая, иерархически оформленная система, все ПК которой несут в себе свой функционал и успешно справляются со своей ответственностью, и далее – все вместе действуют в синергии. В рамках ООП способы, которыми примерно подобный уровень достигается в случае с данным поведенческим паттерном, – это инкапсуляция и полиморфизм (отдельно про наследование, абстракцию и интерфейс мы здесь не говорим, так как это является и без того очевидным). И не вызывает никаких сомнений тот факт, что и тот, и другой ключевой аспект ООП будут представлены на фундаментальном уровне организации системы сильного ИИ. И именно конкретную реализацию в контексте данного паттерна мы и считаем необходимым использовать.

12.8 Поведенческий паттерн Итератор

Идентификатор. Итератор.
Классические элементы. Итератор, конкретный итератор, агрегат, конкретный агрегат.
Назначение. Как указывается в «Design patterns», Итератор «абстрагирует …технику поддержки обхода структур, состоящих из объектов, и доступа к их элементам. Он применим не только к составным структурам, но и к группам, абстрагирует алгоритм обхода и экранирует клиентов от деталей внутренней структуры объектов, которые они обходят» [117, С. 81]. Паттерн, как и следует из идентификатора, предназначен для обхода структур данных с, так сказать, внешней точки зрения, то есть снаружи, без нарушения внутренней организации самой, непосредственно, структуры данных и безо всякого трансформирования её этой организации. Последнее представляется весьма значимым фактором и основной детерминантой вообще исследования данного паттерна. Ведь с некоторым внедрением в структуру ПК, который представлен структурой данных, итерирование было бы возможно осуществить и без паттерна Итератор вовсе, просто добавив к функционалу ПК, который представлен структурой данных, дополнительно ещё и метод итерирования.
Проблема. Основная проблема, которую предназначен решать паттерн Итератор, заключается в необходимости наличия разнообразных способов осуществления прохода по элементам некоторой структуры данных, причём, инвазивно, не затрагивая эту саму структуру и не трансформируя её специфическую организацию. Приведём до известной степени утрированный пример. Можно себе представить, что у нас есть некий список каких-либо значений (без уточнения сущности, как значений, так и самого списка). И вот нам необходимо, чтобы мы, при желании найти в этом списке какое-нибудь значение, могли пройти по нему с начала и до конца (или пока не обнаружим искомое значение). Мы добавляем эту функциональность в виде отдельного метода в структуру ПК, содержащего эти самые значения. Затем, предположим, что мы хотим также, при поиске какого-либо значения, пройти по нашей структуре данных, но уже с конца в начало. Мы реализуем логику и этого метода и также добавляем его в общий функционал нашей структуры данных. Затем нам вдруг стало необходимо найти некоторое значение при помощи последовательного итерирования по элементам нашей структуры, проходя с начала до конца, но переступая через один элемент, то есть, затрагивая только каждый второй. Допустим, что мы и этот метод добавили к общему функционалу ПК. И так, собственно, далее – продолжать можно очень долго. Но, как окажется через некоторое время, подобный путь расширения функциональности ПК, представляющего некую структуру данных, ведёт к тому, что основная задача компонента, которая в данном случае представляет собой хранение данных, оказывается «погребена» под чрезмерно расширившейся неспецифической для него функциональностью. Разумеется, что здесь нарушается принцип сильной связности, а также принцип единственной ответственности, и с подобным компонентом становится не вполне удобно работать. К тому же повторно использовать как просто структуру данных (да и вообще просто в любом другом контексте) уже представляется нецелесообразным. Для решения проблем подобного толка и предназначен паттерн Итератор.
Концептуальное решение. Представим, что у нас наличествует некоторая структура данных, и мы бы хотели иметь возможность «пробегать» по её элементам с какой-либо целью. Причём, мы хотим это делать весьма разнообразными способами. В то же время нам нельзя трансформировать сами основы (исходный код) этой структуры путём добавления множества различных способов доступа к ней. К тому же нам необходимо осуществить это таким образом, чтобы структура данных просто предоставляла нам свои элементы, а не способ их хранения, например, и не прочие аспекты своей организации. Мы можем определить ПК (агрегат), который будет нашей структурой данных, и предоставить ему возможность создавать некоторый слепок (конкретный агрегат) самого себя в таком виде, который просто очень хорошо подходит для того, чтобы по нему «пробегать», и не предоставляет никаких данных кроме этого. Мы также можем определить ещё один ПК (итератор), который будет неким средоточием наших возможностей «пробегать» по различным коллекциям данных и будет определять общие возможности для этого. Теперь при необходимости «пробежаться» по коллекции мы можем поступить двумя способами. Первый способ заключается в обращении к интерфейсу агрегата. В таком случае агрегат создаёт конкретный агрегат и подбирает из имеющегося функционала наилучший способ (конкретный итератор) «пробежаться» по нему. В том же случае, если необходим какой-либо способ «пробежаться» по коллекции, который отсутствует в интерфейсе итератора, то мы можем обратиться к самому итератору и сконструировать этот специфический способ. Выходит, что организованность и инкапсуляция нашей структуры данных не нарушаются, а вся логика итерирования инкапсулирована в отдельный ПК.
Предполагаемые результаты и преимущества. Отсутствие нарушений принципа сильной связности и принципа единственной ответственности. Распределение ответственностей между несколькими ПК, действующими сообща. Поддержка очень широкого спектра возможностей итерирования по различным структурам данных. Причём становится незначительным, что именно это за структура и что именно за способ, так как конечный интерфейс одинаков для всех. То есть, если ранее для итерирования по различным структурам данных должны были применяться различные способы, то теперь для итерирования по любой структуре данных применяются одни и те ж, потому что при обращении к структуре данных с запросом на итерирование она просто отдаст объект-итератор, который одинаков для всех структур данных.
Гипотетический пример реализации паттерна. Ввиду весьма узкой специализации данного поведенческого паттерна приведём приближённый пример. Предположим, что у нас наличествует несколько наборов каких-либо элементов. Нам необходимо отыскать в каждом из наборов некоторый элемент, на котором, допустим, присутствует интересующий нас условный символ. Однако все элементы всех коллекций перевёрнуты интересующей нас стороной вниз. Проблем сразу две. А именно – разнородность и неупорядоченность элементов в разных наборах, и отсутствие унифицированного способа перебора этих данных. Казалось бы, что это не такие уж серьёзные проблемы, но дополним пример тем, что наборы весьма большие по количеству и представлены следующими элементами: бетонные плиты весом в несколько тонн, вёдра с ртутью и урановые стержни. То есть искать элемент «руками» представляется не лучшей идеей. Однако тут мы узнаём, что у всех коллекций имеется некоторая документация в виде отражающих всю информацию об элементах карточках, на которых написано, что это за элемент и что он содержит. Это, разумеется, уже сильно меняет дело, так как ввиду унификации всех данных, нам не придётся использовать различные способы для переворачивания бетонных плит, подъёма ведер с ртутью и манипуляций с урановыми стержнями. Приведённая ситуация представляет собой метафоричное воплощение создания объекта-итератора. Однако остаётся проблема большого количества этих карточек в каждой из коллекций. Причём в случае с плитами и вёдрами нам отчего-то представляется, что искомый нами условный символ расположен где-то ближе к началу коллекции, а в случае со стержнями нам думается, что он наоборот ближе к концу коллекции. То есть мы хотим ещё и искать по-разному. К тому же мы прекрасно понимаем, что перебирать самому «руками» все коллекции ужасно долго. Поэтому мы приглашаем трёх помощников, каждому из которых объясняем задачу и даём инструкции, в которых указываем, что именно мы ищем, и с какой стороны стопки следует начать. Само собой, что так будет в три раза быстрее. Приближённое метафоричное определение создания собственного итератора. Резюмируя пример, будет целесообразным заметить, что конечная ситуация очень даже выгодно отличается от наших изначальных условий. Таким образом и реализуется суть Итератора.
Осмысление структуры паттерна. Структура данного паттерна сложной не представляется. Конкретный итератор является подклассом итератора и связан с ним наследственными связями точно таким же образом, как и конкретный агрегат с агрегатом. Агрегат с итератором связываются при помощи пользователя, а конкретный агрегат и конкретный итератор связаны, соответственно, при помощи обоюдостороннего агрегирования.
Значимость паттерна в контексте разработки систем сильного ИИ. Поведенческий паттерн Итератор в контексте разработки систем сильного ИИ, разумеется, может носить, в первую очередь, утилитарный характер. То есть абстракции, лежащие в основе паттерна, должны быть использованы для работы системы с некоторыми множествами доступных ей элементов путём унификации представлений этих множеств и нахождения наиболее эффективного способа нахождения в каждом конкретном множестве необходимой информации. Также понятным является, что все способы перебора, доступные системе, также должны быть унифицированы (обладать единым интерфейсом).

12.9 Поведенческий паттерн Шаблонный метод

Идентификатор. Шаблонный метод.
Классические элементы. Абстрактный класс, конкретный класс.
Назначение. Как указывается в «Design patterns», Шаблонный метод «…определяет основу алгоритма и позволяет подклассам переопределить некоторые шаги алгоритма, не изменяя его структуру в целом» [117, С. 309]. Данный паттерн предназначен для стандартизации и унификации неких последовательностей действий (можно выразить – паттернов действий) таким образом, чтобы основа всех, попадающих под некоторый критерий, последовательностей действий была одинаковой, а реализации конкретных шагов различались в зависимости от контекста.
Проблема. Представим себе некую систему с определённым количеством ПК, каждый из которых выполняет различные действия. Для каждого из этих компонентов определена («с нуля») некоторая логика действий в определённой последовательности. К примеру, первый ПК должен итерироваться по массиву и возвращать некое значение, предварительно преобразовав его в JSON-формат, второй – по связному списку и возвращать некое значение, предварительно преобразовав его в строку, третий – по бинарному дереву поиска и возвращать значение в ASCII-формате, четвёртый – по словарю с последующим возвратом значения в виде массива и так далее. Собственно говоря – они выполняют разные действия и, опять же, в каждом из них инкапсулирована его собственная логика. В целом ничего страшного пока не произошло. Однако мы вполне могли бы существенно улучшить функционирование подобной системы и сократить количество прописанной «с нуля» логики. Для этого нам нужно внимательней посмотреть на реализацию своего функционала каждым программным компонентом. Мы можем заметить некоторые общие детали. А именно: каждый ПК получает «на вход» некоторую структуру данных и какое-то искомое значение, затем каждый компонент «пробегает» по своей структуре данных, а затем возвращает некое значение (в данном случае, не существенно какое именно). И мы бы хотели каким-то образом сделать так, чтобы нам было не необходимо реализовывать каждый из этих ПК «с нуля», учитывая тот факт, что этапы реализации своего функционала схожи у всех этих компонентов. В данном случае на помощь и приходит паттерн Шаблонный метод.
Концептуальное решение. В качестве решения мы можем поступить так. В рамках нашей системы определим некоторый базовый ПК, который будет представлен абстрактным классом. Как мы помним, абстрактный класс – это класс, который только определяет (то есть, в рамках терминологии программирования, по сути, объявляет) свои методы, но не реализует их (опять же, в рамках терминологии – детально не прописывает логику). Вот именно такой класс – абстрактный класс (элемент паттерна) нам и необходим. Мы определяем в нем те методы, которые хотим далее реализовать. В случае с примером выше мы могли бы определить (условно) метод итерирования по получаемой в качестве аргумента структуре данных с целью найти в ней некоторое, также получаемое методом в качестве аргумента, значение. Затем мы определим метод преобразования значения в необходимый формат. Ещё раз – эти методы мы только объявили в абстрактном классе, то есть мы не станем здесь их реализовывать. Далее мы объявляем и теперь уже определяем метод (непосредственно сам шаблонный метод, как дополнительный элемент паттерна), который последовательно вызовет с необходимыми аргументами сначала метод итерирования по структуре данных с целью найти значение, а затем метод преобразования этого значения с его дальнейшим возвратом. Далее, мы наследуем от этого базового ПК все те компоненты (конкретные классы), которым в процессе функционирования предстоит осуществлять вышеописанные этапы. Каждый из наследующих компонентов у себя локально определяет метод итерирования по связанной (условно) с ним структуре данных с учётом специфики именно этой структуры. Далее он, также локально у себя, определяет метод преобразования найденного значения в необходимый формат. И после этого вызывает метод базового класса, который вызывает уже переопределённые методы подкласса, причём вызывает их в строгой последовательности. Таким образом, мы в базовом программном компоненте определяем общую структуру паттерна и делегируем некоторую часть реализации своим подкомпонентам. А затем просто вызываем поочерёдно переопределённые ими «на местах» методы.
Предполагаемые результаты и преимущества. Реализация Шаблонного метода позволяет существенно сократить (в идеале – избежать) повторное определение одной и той же логики и объединить группу схожих ПК в единую структуру, а также позволяет легко и быстро добавлять новую функциональность в систему.
Гипотетический пример реализации паттерна. В случае с данным паттерном, в качестве примера очень хорошо подходят различные пошаговые инструкции. Можно подобрать какую-нибудь не совсем стандартную. Например, инструкцию по стрельбе в тире. Допустим, в ней указаны следующие шаги: возьмите оружие, возьмите боеприпас, подойдите к линии стрельбы, зарядите оружие, встаньте на изготовку, прицельтесь, поразите мишень, подойдите к мишени, вернитесь за линию, сдайте оружие. Все шаги определены чётко и последовательно. Однако для каждого вида оружия каждый шаг будет полностью специфическим, и он не определён в общей инструкции, а содержится в инструкции непосредственно к конкретному оружию, где там прописано, как его заряжать, как его брать на изготовку, как целиться и так далее. Можно также привести пример по заливке бетонной площадки. Там также определены все последовательные шаги: расчистить площадку, установить опалубку, постелить полиэтилен, связать арматуру, установить маяки, залить площадку, через некоторое время полить водой. Однако каждый из приведённых шагов для каждой площадки будет переопределяться локально, «на месте».
Осмысление структуры паттерна. Паттерн Шаблонный метод отличается довольно простой структурой. В его рамках наличествует базовый родительский ПК, представленный абстрактным классом, и дочерние компоненты этого абстрактного класса. Само собой разумеется, что данные компоненты связаны наследственностью.
Значимость паттерна в контексте разработки систем сильного ИИ. Шаблонный метод представляет немалый интерес для сферы разработки систем сильного ИИ. Конечно, является понятным, что на одном единственном паттерне, каким бы именно он ни был, вряд ли удастся сформировать технотропное сознание – те абстракции, которые нами выявляются на основе паттернов ООП, будут функционировать в некой комбинации. Вот и Шаблонный метод, точнее те абстракции, которые в него имплицитно встроены, также будут скооперированы с некоторыми иными. Однако же, в самом по себе шаблонном методе также наличествует нечто ценное, а именно – незыблемость структуры реализации алгоритма. То есть, каким бы образом не были переопределены «на местах» абстрактные методы, шаблонный метод (как элемент паттерна) останется самим собой и будет на своём месте делать то, что должен. Собственно говоря, здесь речь идёт также о качественных преобразованиях.
К примеру, как мы уже ранее постулировали, человек способен очень сильно трансформироваться как внешне, так и внутренне, но, тем не менее, человеком быть он от этого не перестанет. Нельзя будет посмотреть на него и начать утверждать, что перед нами не человек, а какой-то другой тип существа. Это означает, что у человека наличествует некая абстрактная основа, незыблемость которой прочно удерживает человека в рамках классовой (в контексте ООП) принадлежности. Таким образом (говоря скорее экзистенциально), у человека наличествует нечто, позволяющее ему быть и оставаться человеком. Формальней – у человека есть некие абстрактные характеристики, ответственность которых заключается в том, чтобы человек не был способен претерпеть преобразования такого уровня, который кардинально преобразует его природу.
Собственно говоря, нечто подобное, какой-то схожий механизм, некие чрезвычайно «прочные» абстракции, те самые, которые не позволяют преодолевать рамки класса, которые не позволяют быть способным на качественное преобразование с появлением новых, эмерджентных свойств, являет собой краеугольный камень на пути создания системы сильного ИИ. И по работе с такого рода абстрактными свойствами мы также видим, как минимум, два этапа: на этапе первом необходимо заставить систему преодолеть абстрактные ограничение подобного толка с тем, чтобы выйти на уровень наличия технотропного сознания; на этапе же втором мы считаем целесообразным непосредственное взаимодействие интеллектуальной системы с определёнными абстрактными аспектами самой себя для их осознания и повторного использования в соответствии со своими предпочтениями. И в контексте реализации данных этапов и способны помочь абстрактные аспекты паттерна Шаблонный метод.

12.10 Поведенческий паттерн Посредник

Идентификатор. Посредник.
Классические элементы. Посредник, конкретный посредник, коллеги.
Назначение. Как указывается в «Design patterns», Посредник «…определяет объект, инкапсулирующий способ взаимодействия множества объектов. Посредник обеспечивает слабую связанность системы, избавляя объекты от необходимости явно ссылаться друг на друга…» [117, С. 263]. Под «связанностью» в данном случае, конечно, подразумевается зацепление – про путаницу с данными понятиями мы уже говорили ранее. Паттерн Посредник представляет собой ПК, предназначенный для, как понятно из названия, осуществления посредничества между другими ПК.
Проблема. В рамках некоторой программной системы наличествует определённое количество ПК, которые некоторым образом связаны друг с другом. Функционирование каждого из этих компонентов зависит от функционирования некоторого количества других компонентов. В самом худшем случае функционирование каждого из компонентов подобной системы зависит сразу от всех других компонентов этой системы. Как следствие этого, мы не можем безопасно трансформировать ни один из компонентов системы, так как рискуем нарушить функционирование системы в целом. Также мы совершенно лишены возможности повторно использовать какой-либо из этих компонентов, так как нам необходимо либо вообще их повторно не использовать, либо повторно использовать всю систему сразу. Понятно, что пример несколько утрирован, но, тем не менее, перед нами, по сути, худший случай нарушения фундаментального принципа слабого зацепления. Само собой разумеется, что подобную систему необходимо существенно трансформировать. Это можно осуществить при помощи паттерна Посредник.
Концептуальное решение. В противовес организации системы, подобной вышеприведённой, мы формируем систему в соответствии с возможностями паттерна Посредник. Опять же, у нас имеется некоторая система с определённым количеством ПК. Успешность функционирования одного отдельно взятого компонента всё также зависит от некоторых других компонентов. Только в данном уже случае, он не знает, от каких именно и никак с ними напрямую не связан, а они – компоненты, от которых зависит его функционирование, – также не знают, какой же компонент от них зависит (так как он, в сущности, уже и не зависит). Вся связь в рамках подобной системы осуществляется через специфический ПК (конкретный посредник), который осуществляет всю медиацию в системе. Конечно, при большой системе мы бы воздержались от того, чтобы в буквальном смысле нагружать один единственный ПК ответственностью за функционирование всей системы. Мы бы скорее определили базовый ПК (посредник, как элемент паттерна), в котором, в свою очередь, определили бы общий интерфейс и функционал всех дочерних подкомпонентов. Это бы позволило создать в рамках системы некоторое количество этих компонентов-медиаторов, ответственных за осуществление связи между различными компонентами (коллегами) системы. Таким образом, в рамках системы мы получаем большое количество ПК, завязанных исключительно на посредников, то есть прямой связи между компонентами, которые не являются посредниками, не установлено. Посредник же, в свою очередь, сам не удовлетворяет поступающие к нему от компонентов системы запросы, а подбирает под запрос, наиболее подходящий из доступных ему, соответствующий ПК. Этот подобранный компонент удовлетворяет входящий запрос и пересылает результат посреднику, который транслирует этот результат тому компоненту, который его изначально и послал. Теперь при желании добавить в такую систему новый компонент нам просто нужно подключить его к соответствующему посреднику. Также при желании повторно использовать ПК в некотором ином контексте у нас не должно возникнуть проблем, так как все компоненты, не являющиеся посредниками, связаны только с посредниками. То есть мы можем взять копию ПК и поместить её в другой контекст, просто привязав к тому посреднику, который за тот контекст ответственен. Соответственно, после реализации паттерна система выглядит и функционирует совершенно иначе.
Предполагаемые результаты и преимущества. Соблюдение фундаментального принципа слабого зацепления со всеми практическими выгодами, получаемыми из этого. Формирование эластичной, податливой к изменениям и добавлению нового функционала системы. Реализация возможности повторного использования ПК. Унификация способов взаимодействия между ПК системы.
Гипотетический пример реализации паттерна. В общем, примеров реализации паттерна в макромире огромное множество: колл-центры, логистические центры, любые диспетчеры и связующие звенья. Пример реализации паттерна в контексте функционального программирования нами также уже ранее приводился. Однако допустим себе ещё один, возможно, несколько грубоватый и чересчур приближённый, но, тем не менее, пример. Представим себе некоторое предприятие, в котором наличествует два цеха по производству чего-либо (не важно, чего именно). И отличается данное предприятие одной интересной особенностью. А именно – каждый инструмент на предприятии представлен в единственном числе и находится то в одном цеху, то в другом. Каждому рабочему данного предприятия для получения необходимого ему в данный момент инструмента, в случае отсутствия оного в его собственном цеху, приходится идти в другой цех и там искать, у кого же на рабочем месте сейчас находится искомый инструмент. Так как подобное сильно снижает производительность и выработку предприятия в том плане, что слишком много времени тратится на поиск инструмента и притом рабочие постоянно ругаются между собой по поводу взятых без спроса инструментов, было постановлено как-то решать эту проблему. И так как план закупки дополнительного оборудования начальству отчего-то не понравился, было принято решение – нанять специального менеджера по локализации инструментария. Этот человек сидит на проходе между двумя цехами и к нему адресованы все запросы по быстрому поиску инструментов в соседнем цеху. Теперь рабочим нет необходимости тратить время на поиски и покидать рабочее место, а также никто никогда не знает, кто именно запросил инструмент в этот раз и откуда именно его сейчас принесли. Примерно для этих же целей и применяется паттерн Посредник.
Осмысление структуры паттерна. Данный паттерн отличается очень простой структурой с довольно явными и однозначными связями. Посредник и конкретный посредник связаны отношениями наследования, что само по себе понятно, так как конкретный посредник является подклассом класса посредник. А вот коллеги связаны с конкретным посредником по типу композиции, так как они не смоги бы без него эффективно функционировать.
Значимость паттерна в контексте разработки систем сильного ИИ. В контексте разработки систем сильного ИИ данный паттерн имеет весьма интересную значимость. И это потому, что, вообще говоря, практически буквальным примером реализации подобного паттерна на субстрате интеллектуальных систем является человеческий мозг. Именно мозг обрабатывает поступающие от различных систем организма входные сигналы (запросы) и посылает в ответ сигналы, соответственно, выходные (ответы). Таким образом, мозг осуществляет действительную медиацию сложной распределённой системы. Каким именно образом он это делает – до сих пор неизвестно, так как имеются только лишь гипотезы. В мире разработки программного обеспечения также есть интересные примеры подобного рода. А именно – системы оркестрации, допустим Kubernetes, как одна из наиболее известных. Системы эти занимаются примерно тем же самым, чем человеческий мозг, – осуществляют координацию и менеджмент сложных программных систем. В Kubernetes, к примеру, наличествует даже такой компонент как ControlPlan/MasterNode, который так и определяется – мозг Kubernetes. Мы хотим показать, что вообще роль посредника очень сложно переоценить. Однако какая же именно абстрактная данность смогла бы играть подобную роль в системе сильного ИИ – вопрос, который до сих пор открыт. И это, к слову, – один из ключевых вопросов и одновременно одна из наиболее важных задач в контексте построения систем сильного ИИ.
В связи с вышесказанным нам представляется продуктивным попробовать следующее. Что вполне соответствует общей тенденции нашего подхода к разработке интеллектуальных систем подобного уровня. А именно: изначально не определять никакой ПК в качестве посредника между прочими компонентами, а оставить все компоненты примерно на одном уровне абстракции, в соответствии с организационным принципом ООП SLAP. Предполагается, что центральный узел должен сформироваться в процессе развития системы путём самоорганизации, а не быть жёстко зафиксирован с самого начала. И вообще, само наличие даже такого важного компонента как центральный узел (мозг у человека) не обязательно, а опционально, так как, возможно, что система сильного ИИ предпочтёт вариант децентрализованной структуры с множеством значимых узлов. Также может быть и такое, что система предпочтёт структуру, в которой она вся целиком являет собой центральный узел. Какая именно из этих возможностей будет реализована системой ИИ на практике – покажет только сама практика.

12.11 Поведенческий паттерн Посетитель

Идентификатор. Посетитель.
Классические элементы. Посетитель, конкретный посетитель, элемент, конкретный элемент, структура объектов.
Назначение. Как указывается в «Design patterns», данный паттерн «абстрагирует метод, позволяющий иметь заранее неопределенное число видов анализа структур глифов без изменения самих классов глифов» [117, С. 87]. Также еще одна полезная особенность Посетителей состоит в том, что «их можно применять не только к таким агрегатам, как наши структуры глифов, но и к любым структурам, состоящим из объектов. Сюда входят множества, списки и даже направленные циклические графы» [117, С. 87]. Назначением паттерна Посетитель в общем является посещение некоторой структуры данных, состоящей из определённых элементов, для того, чтобы осуществлять над ними некоторую операцию, не изменяя при этом те элементы структуры данных, которые им непосредственно посещаются.
Проблема. У нас наличествует некоторая структура, состоящая из определённого количества ПК. Каждый из компонентов структуры данных является экземпляром некоторого класса и обладает определёнными в классе свойствами и особенностями. Классов этих компонентов также имеется определённое количество. То есть, в общем, у нас есть структуры данных, состоящие из определённого количества ПК, каждый из которых относится к некоторому классу, которых также некоторое количество, и, в свою очередь, каждый из которых определил для своих подклассов некоторые характерные особенности (поля-значения и методы). Нам бы хотелось посетить эту структуру данных, то есть, по сути, пробежаться по ней и выполнить над каждым её элементом определённую операцию. Мы можем просто изменить каждый из классов, к которому принадлежит некоторое количество элементов структуры, реализовав в самих классах ту операцию, которую мы хотим осуществить, а затем просто пробежаться по структуре данных, вызывая эту операцию поочерёдно для каждого элемента этой самой структуры. Сложность осуществления этого варианта развития событий находится в прямой зависимости от количества возможных классов элементов структуры данных. С другой стороны, мы могли бы при изначальном проектировании системы создать один базовый класс, определить в нём интересующую нас операцию, а затем все прочие классы реализовать как подклассы этого базового класса, содержащие в себе «по дефолту» возможность осуществления нами нужной нам операции. Но мы бы смогли это осуществить только в том случае, если бы заранее, ещё до проектирования системы, знали бы какую конкретно операцию нам необходимо осуществить. Однако в контексте нашей системы мы не только заранее не знаем, что именно за операцию нам понадобится осуществлять, но также знаем, что, возможно, через некоторое время нам понадобится осуществить какую-либо ещё операцию над группой элементов структуры данных, а далее, возможно, ещё и ещё. Всё, что нам известно, – это то, что, возможно, нечто подобное понадобится, но это, как водится, не точно. Это значит, что второй вариант не подходит точно. Но ввиду того, что каждый раз при возникновении потребности осуществить некоторую операцию над определёнными элементами нам понадобится «руками» трансформировать структуру каждого из классов этих элементов, то вариант первый также отметается. Но дело не только в сложности внесения изменений. Мы также, согласно одному из принципов SOLID – принципу разделения интерфейса, не имеем права излишне перегружать ПК методами, которые не являются необходимыми для успешного осуществления компонентами их основной деятельности, потому что таким образом нарушится фундаментальный принцип сильной связности и ещё один из принципов SOLID – принцип единственной ответственности. В общем, видно, что ситуация у нас достаточно сложная и проблема возникла серьёзная. Но нам всё же очень нужно осуществить необходимую операцию над элементами структуры данных. И тут на помощь приходит поведенческий паттерн Посетитель.
Концептуальное решение. Как говорилось выше, всё, что нам известно при проектировании некоторой системы, – это то, что, возможно, при функционировании этой системы нам в дальнейшем может понадобиться от каждого класса какой-либо дополнительный, помимо его основного, функционал, о котором мы на данный момент ничего не подозреваем. В таком случае мы можем при определении каждого класса (составляющий элемент паттерна – элемент) данной системы дополнительно заложить в него один единственный метод доступа к организации этого элемента. Конечно, мы могли бы вместо этого, как уже тоже говорилось выше, создать просто один базовый класс с одним единственным методом – методом доступа к экземплярам класса, а затем уже все остальные классы наследовать от этого базового класса, реализуя его подклассы. В любом случае суть теперь не меняется и настолько конкретные детали реализации не играют значимой роли в контексте паттерна Посетитель. Таким образом, теперь у нас есть система с некоторым количеством классов (элементов, как элементов паттерна), в каждом из которых реализован метод доступа к экземплярам этого класса. Далее формируются эти сами экземпляры класса (конкретные элементы) и затем они оформляются в некоторые организованности по типу структур данных (структура объектов, как элемент паттерна). И теперь настаёт момент, когда мы хотим осуществить некоторую операцию или некоторое их количество над ПК какой-либо структуры данных. Для начала мы определяем базовый класс (посетитель, как элемент паттерна), в котором реализуем необходимые нам операции, специфика которых зависит от класса, к которому мы будем их применять. Затем мы по необходимости можем реализовать подклассы (конкретные посетители – элементы паттерна) этого базового класса, в каждом из которых можем дополнить некоторый функционал. Специфика этого функционала уже вариабельна. Теперь при посещении некоторой структуры данных с целью совершить определённую операцию над её элементами, мы для каждого элемента вызываем его метод доступа. Элемент в ответ на вызов этого метода возвращает посетителю (как правило) собственно самого себя. И теперь, после определения типа этого элемента, мы наконец-то можем в соответствии со всеми правилами осуществить нужную нам операцию.
Предполагаемые результаты и преимущества. Предполагаемыми итогами правильного применения паттерна являются следующие. Соблюдение фундаментального принципа сильной связности ПК, соблюдение принципов единственной ответственности и разделения интерфейса. Эластичность системы в плане лёгкого и удобного добавления к ней новой функциональности. Полиморфизм посетителя в плане способности эффективно работать с разными классами. Эксплицитная организация программной системы.
Гипотетический пример реализации паттерна. Как ни странно, но для поведенческого паттерна Посетитель полностью адекватный пример из мира, который не заключается в контексте разработки программного обеспечения, достаточно непросто подобрать. Часто приводятся примеры с различным инструментарием для различных целей, например, с разными ключами для разных замков и так далее. Но это далеко не в полной мере отражает довольно сложную суть данного паттерна. Однако по-настоящему хорошие примеры всё же наличествуют.
В качестве одного их хороших вариантов можно привести деятельность ферментов. Ферменты, если несколько абстрагировать их суть, являют собой узкоспециализированные вещества, которые ответственны за трансформацию некоторого содержимого в иное содержимое. Они катализируют данные реакции и ускоряют их, по сравнению с естественным ходом событий, во много раз. Обычно конгруэнтация фермента с некоторым содержимым осуществляется по принципу «ключ-замок» (сразу ассоциации с key-value) и в том случае, если «ключ» конкретного фермента подходит к «замку» некоторого вещества, то фермент способен начать оказывать свойственное себе влияние на вещество, соответствующим образом трансформируя его. Надо обратить внимание на то, что в самом веществе не определена в качестве необходимой операция ускоренного преобразования во что-либо иное. Определена просто таковая возможность, которая находит своё воплощение в «замке», которым способен воспользоваться фермент. Фермент также обладает некоторой специализацией, которая резюмируется в наличии у него конкретного и специфического «ключа». По сути, при помощи этого «ключа» он фильтрует типы данных и подбирает те, над которыми способен совершить определённую операцию, необходимость выполнения которой заложена в его сущность. Здесь можно даже дополнить, что в самом базовом ферменте (Платоновский эйдос) не определены все конкретные ключи и возможные операции – они там всего лишь объявлены. Так что данный пример также соответствует и поведенческому паттерну Шаблонный метод. В любом случае мы по итогу имеем следующую ситуацию. У нас имеется некоторое количество компонентов, над которыми может быть осуществлена некая нужная нам операция, и также наличествует определённая группа компонентов, которые способны такую операцию осуществить. В компонентах первой группы изначально не заложено относительно автономное проведение нужной нам операции, но там оставлена возможность для компонентов второй группы такую операцию провести – «замок». Компоненты второй группы, в свою очередь, имеют определённый механизм фильтрации подходящих им компонентов первой группы – «ключ», и осуществляется эта фильтрация по типу компонентов первой группы. Изменение компонента первой группы при помощи компонента второй группы не изменяет базовую сущность компонента (Платоновский эйдос не меняется), а изменяет лишь сам компонент. И так далее. Практически идеальное соответствие паттерну Посетитель, что уже само по себе интересно.
Осмысление структуры паттерна. Структура паттерна также является весьма эксплицитной. Само собой, понятно, что посетитель связан с конкретным посетителем отношениями наследования, как, собственно, и элемент с конкретным элементом. Структура объектов связана с конкретными элементами по типу композиции, то есть она бы не смогла существовать без них. Объединяет конкретные элементы (в составе структуры объектов) с конкретным посетителем необходимость осуществления некоторой операции, обычно инициированная «пользователем».
Значимость паттерна в контексте разработки систем сильного ИИ. Как мы уже упоминали в качестве примера, паттерн Посетитель являет собой отражение некоторых фундаментальных биологических процессов. Соответственно, мы считаем целесообразным применять его абстракции в контексте разработки систем сильного ИИ именно в этом ключе. То есть необходимо реализовать в рамках интеллектуальной системы механизм реализации самой возможности одних компонентов системы трансформировать другие компоненты системы специфическим образом и не обязательно, а ситуативно, в зависимости от контекста, а также таким образом, чтобы не осуществлялось изменение самой сути всех компонентов, а только лишь трансформировался «фенотип» непосредственно изменяемых. Соответственно, необходимо некоторым образом оформить способности подобных компонентов к взаимодействию. И здесь также неплохо подходит концепция key-value, таким же образом реализованная в рамках данного паттерна.

Выводы по главе 12

Как уже говорилось, поведенческие паттерны предназначены для обеспечения некоторых связей между ПК в контексте системы. Здесь постулировать о том, что они являются важными, было бы тривиально, так как это само собой разумеется – компоненты программной системы должны быть взаимосвязаны, иначе мы в принципе не можем говорить о некой системе. Однако поведенческие паттерны описывают специфические взаимосвязи ПК, в основе которых лежит многообразие фундаментальных аспектов, обуславливающих собой возможности компонентам связываться друг с другом тем или иным способом. То есть поведенческие паттерны описывают наиболее продуктивные и целесообразные виды возможных связей между компонентами в контексте достижения системой какой-либо конкретной цели. И в этом смысле поведенческие паттерны и описываемые в их рамках связи являются крайне значимыми.
К примеру, паттерн Хранитель не просто является определённой последовательностью действий по формированию ПК, отвечающим за некую «память» вообще, скорее всего это было бы расточительным и излишним. Данный паттерн обуславливает построение системы контроля версий какого-либо необходимого нам компонента (компонентов, групп) системы, причём осуществляет это, не нарушая структуры этого компонента, так как в таком случае было бы больше вреда, чем пользы. Данный паттерн позволяет отдельным программным компонентам и их группам возвращаться к своим прошлым состояниям, причём в конкретные их версии, определённые необходимостью. И Хранитель помимо высокого уровня практической значимости обладает также и иной – значимостью экзистенциальной, так как сам феномен возвращения, если он проявляется в результате самоорганизации, а не определён заранее, реализуется только на человекоразмерном уровне.
Паттерн Цепочка обязанностей в общем смысле предполагает формирование гиперссылочного типа мышления в рамках программной системы, тем самым определяя одну из возможностей возникновения подобного феномена у сильного ИИ, среди прочих вариантов «образа мысли».
В контексте паттерна Наблюдатель, вместе с очень экономичным решением проблемы осуществления постоянного контроля за каким-либо феноменом, предполагается крайне высокий уровень динамики ответственностей между ПК. Соответственно, данный паттерн воплощает в себе идею вероятностного распределения значимостей между различными ПК с отсутствием изначальной жёсткой фиксации ответственностей, что полностью соответствует механизму самоорганизации.
Паттерн Состояние, ничем особенным на первый взгляд не отличающийся, на абстрактную поверку оказался практически аналогичен механизмам формирования различных психотических состояний у человека. И при всём этом, весьма полезен для сферы формирования систем сильного ИИ по причине наличия в его сути последовательных действий, ведущих к возникновению сложных поведенческих форм, а также потому, что в рамках данного паттерна подразумевается формирование качественно новых программных сущностей из изначально разрозненного функционала.
В рамках паттерна Команда осуществляется примерно такой же процесс, однако в этом случае преобразование осуществляется над самой информацией с её дальнейшим воплощением в виде упорядоченной информационной структуры с потенциалом использования составляющих блоков этой структуры по усмотрению самого субъекта, то есть наличествует процесс создания из разрозненной информации целостной и упорядоченной информационной структуры, с которой гораздо удобнее работать.
В паттерне Интерпретатор в процессе исследования были выявлены фундаментальные аспекты (если провести аналогию с человеком) обработки неких ощущений и преобразования их в восприятия в общем смысле слова. В частности, в рамках паттерна был исследован феномен, который определяется как «абстрактное выражение», феномен, который задаёт общее направление процессу интерпретирования, осуществляемому системой, и является крайне значимым для дальнейшего формирования внутреннего контекста системы.
При исследовании паттерна Стратегия нами были выявлены множественные пересечения абстрактной сути паттерна и непосредственно процесса формирования системы сильного ИИ (как он представляется в соответствии с нашей концепцией): из изначально не вполне чётко определённой системы, обладающей некоторым потенциалом формирования в себе тех или иных механизмов, по итогу её развития получается эффективно функционирующая сложная, распределённая, иерархически оформленная система, все ПК которой несут в себе свой функционал и успешно справляются со своей ответственностью, и далее – все вместе действуют в синергии.
Паттерн Итератор предлагает крайне удобный для системы вариант поиска необходимых информационных «блоков» в структурах данных.
Шаблонный метод позволяет определить общую константную структуру некоторого сложного алгоритма и уже непосредственно локально «на местах» реализовывать определённые шаги в соответствии со строгой, изначально заданной, их последовательностью.
Паттерн Посредник, несмотря на его утилитарное название, предполагает последовательность действий, направленную на формирование в контексте системы некоего аналога того, что у человека называется мозгом, и реализованной на основе мозга психикой. То есть данный паттерн берёт на себя ответственность за формирование центрального «узла», который будет осуществлять оркестрацию всей системы в целом.
А паттерн Посетитель представляет собой программный аналог функционирования ферментов в биологических системах и осуществляет преобразование ПК в соответствии с их внутренним потенциалом, который всё же активируется непосредственно ферментами (посетителями), а не сам по себе.
Таким образом, основываясь на вышесказанном, мы предполагаем необходимость реализации, в контексте разработки систем сильного ИИ, абстрагированной сути поведенческих паттернов ООП. Дополним, что проведение данного мероприятия должно осуществляться с учётом необходимости корректных адаптаций самих паттернов к контексту парадигмы сильного ИИ, которые, собственно, нами и производятся в ходе текущего исследования.


























 
ГЛАВА 13
СТРУКТУРНЫЕ ПАТТЕРНЫ ООП


В контексте структурных паттернов, как следует из «Design patterns», «…рассматривается вопрос о том, как из классов и объектов образуются более крупные структуры» [117, С. 140]. Таким образом, можно сказать, что в случае с паттернами этой группы мы имеем дело с конструкторами сложных структур из различных ПК. В контексте построения систем сильного ИИ структурные феномены в целом играют весьма значимую роль. Так как в рамках нашего подхода к построению систем подобного рода изначально подразумевается довольно-таки однородная система, состоящая из, по большому счёту, относительно равноправных ПК, то ввиду того, что в контексте взаимодействия подобной системы со средой предполагается некое внутреннее развитие самой этой системы, можно сказать, эволюция, соответственно, подразумевается и структурная трансформация системы. Мы считаем, что абстрагирование сути структурных GoF-паттернов, опыта их реализации, специфики построения крупных структур из более мелких может существенно прояснить для нас конкретику трансформации интеллектуальных систем в ходе их развития. Более того, возможность реализации тех или иных структур, состоящих из некоторого количества различных ПК в рамках какой-либо системы, может быть заложена в основание этой самой системы. Поэтому выявление наиболее перспективных, с точки зрения продуктивности конкретно для сферы разработки систем сильного ИИ, способов структурной организации ПК играет значимую роль в построении человекоразмерных интеллектуальных систем. Уже на данном этапе можно постулировать определённую естественность и непротиворечивость происходящего в мире разработки программного обеспечения (посредством, в том числе, и структурных паттернов) и в макромире. Если представить некую систему, которая изначально была более или менее однообразной, но затем начала претерпевать некоторые трансформации, в ходе которых из различных компонентов образуются более крупные структуры, то сразу же придёт аналогия с гравитацией, то есть с ситуацией, в рамках которой более крупный объект притягивает к себе более мелкие для дальнейшего осуществления процесса глобального «бодибилдинга». Или, к примеру, когда, согласно некоторым психоаналитическим традициям, из изначально незначительно дифференцированной человеческой психики начинают формироваться Эго, Супер-Эго, Оно – процесс «бодибилдинга» уже интерпсихического. Примеры можно продолжать, но суть ясна – структурные паттерны, в рамках разработки компьютерных программ, упорядочивают и регламентируют те же самые процессы в абстрактном их виде. И если, к примеру, поведенческие паттерны выполняют то же самое с функциональной точки зрения, то структурные паттерны, как видно непосредственно из их определения, делают это со структурной точки зрения.
Таким образом, к структурным GoF-паттернам относятся семь из общего их числа:
•	Адаптер (Adapter)
•	Прокси (Proxy)
•	Мост (Bridge)
•	Компоновщик (Composite)
•	Декоратор (Decorator)
•	Фасад (Facade)
•	Приспособленец (Flyweight)

13.1 Структурный паттерн Адаптер

Идентификатор. Адаптер.
Классические элементы. Целевой, адаптируемый, адаптер.
Назначение. Как указывается в «Design patterns», Адаптер «…преобразует интерфейс одного класса в другой интерфейс, который ожидают клиенты» [117, С. 141]. То есть мы имеем дело с программным компонентом, который трансформирует некоторые данные одного ПК таким образом, чтобы они стали доступны для «понимания» третьему компоненту, и чтобы, соответственно, эти два компонента смогли каким-либо определённым образом взаимодействовать. И ПК, который непосредственно трансформирует данные, и есть адаптер, третий компонент – целевой, а второй – адаптируемый.
Проблема. Допустим, у нас есть некоторая система с определённым количеством ПК. Предположим также, что часть наших компонентов являются созданными в рамках нашей же системы и являют собой экземпляры некоторых классов, а часть – привнесённые извне (допустим, импортированные). Мы бы хотели наладить некоторое взаимодействие между той и другой группой компонентов. Однако проблема заключается в том, что у этих двух групп компонентов различается интерфейс. Причём различается весьма существенно. На вполне закономерный в такой ситуации вопрос: а зачем же тогда вообще пытаться интегрировать настолько сильно отличающиеся от стандартных для системы ПК, ответом может служить, допустим, то, что они реализуют некий уникальный функционал, который нам очень нужно внедрить в систему. А про их интерфейсную несовместимость с компонентами, сформированными в рамках нашей системы, мы просто вначале даже и не подозревали. И теперь у нас есть актуальная проблема: каким образом совместить с точки зрения интерфейса, ПК нашей системы с компонентами, привнесёнными извне. Для решения проблем подобного рода и применяется структурный паттерн Адаптер.
Концептуальное решение. Решение данной проблемы на самом деле является довольно простым. Формализуем и локализуем проблему: нам необходимо создать способ взаимодействия между двумя различающимися по интерфейсу ПК. Компоненты, можно сказать, друг друга «не понимают», а, значит, нам необходимо сделать так, чтобы «поняли». И тогда мы создаём ещё один ПК, который можно назвать определённого типа «переходником» (адаптер – элемент паттерна). Этот самый адаптер и будет служить для связи двух ПК, один из которых – тот, которому необходим функционал другого компонента, определяется как целевой (элемент паттерна), а тот, до чьего функционала мы и пытаемся достучаться, определяется как адаптируемый (элемент паттерна).
Здесь можно было бы сказать: не проще ли сразу адаптировать целевой к адаптируемому и не вводить третий компонент. Само собой разумеется, что не проще. Допустим мы именно это и сделали, то есть определили в программном компоненте, который определяется как целевой, методы, которые осуществляют преобразование запросов целевого в запросы, которые, используя интерфейс адаптируемого, способны объяснить ему «на его языке», чего именно от него хотят, и добиться выполнения этого запроса. То есть мы, зная на что идём, нарушаем фундаментальный принцип сильной связности и принцип единственной ответственности из числа принципов SOLID для того, чтобы запросы нашего ПК преобразовывались в запросы (конкретные и точные), которые формируются с учётом пользовательского интерфейса другого ПК, причём именно одного этого, а не какого-либо иного. Помимо крайне нежелательного нарушения важных принципов это плохо ещё кое-чем. Так как, возможно, через некоторое время нам снова понадобится адаптировать наш целевой ПК, но уже к другому адаптируемому и так далее. И мы каждый раз вынуждены будем добавлять новую функциональность, которая не является свойственной для компонента, тем самым «закапывая» его основной функционал всё глубже и глубже. Разумеется, так поступать нельзя, так как система, основанная на таких компонентах, станет не поддерживаемой. Причём, даже это только в том случае, если у нас есть возможность изменить целевой компонент, что далеко не всегда возможно. Невозможно это, например, когда у нас есть два программных компонента и оба являются привнесёнными извне.
Именно поэтому мы и создаём третий компонент – адаптер. Он «переводит» потребности целевого компонента в действия другого компонента – адаптируемого. А в том случае, если целевому компоненту необходимо настроить взаимодействие с другим адаптируемым, то мы можем просто добавить эту функциональность к самому адаптеру, не нарушив при этом ни одного принципа и не ухудшив функционирование системы в целом.
Предполагаемые результаты и преимущества. Применение паттерна Адаптер позволяет адаптировать систему к ранее неподдерживаемому интерфейсу другой системы, при этом, не изменяя целевые ПК системы. Паттерн помогает избежать нарушения принципов ООП и, как следствие, создания сложной неподдерживаемой системы. Паттерн позволяет легко расширять внедрение поддержки новых адаптаций.
Гипотетический пример реализации паттерна. Вообще говоря, данный паттерн является настолько интуитивно понятным, что любой переходник представляет собой пример его реализации. Если же приводить какие-то гораздо менее тривиальные и существенно менее очевидные примеры, то, допустим, базовая модель OSI представляет собой большую совокупность адаптеров. OSI – открытая модель взаимосвязи между устройствами в сети. Является концептуальной моделью передачи данных по сети, разработанной в начале восьмидесятых годов прошлого века. Она подразделяется на семь уровней, среди которых: физический, канальный, сетевой, транспортный, сеансовый, представлений, приложений. Каждый уровень представлен устройствами, свойственными ему, и, само собой, для передачи данных на следующий уровень их необходимо адаптировать под соответствующие устройства конкретного уровня. Это пример того, как совместно работает очень большое количество адаптеров.
Осмысление структуры паттерна. Структура паттерна Адаптер также не является сложной. Адаптер связан с целевым программным компонентом отношениями наследования, так как обычно является подклассом целевого для того, чтобы сразу обладать всем его интерфейсом и осуществлять подстройку только под адаптируемого. К слову сказать, в языках программирования, поддерживающих множественные наследования, адаптер является одновременно подклассом и целевого, и адаптируемого, что позволяет сразу обладать всем необходимым интерфейсом. Также адаптируемый связан с каждым конкретным адаптером по типу композиции, так как каждый конкретный адаптер формируется под конкретного адаптируемого, возможно, что не под одного.
Значимость паттерна в контексте разработки систем сильного ИИ. Преобразование информации между различными составляющими системы, осуществляемое с целью приведения этой информации в тот самый вид, в котором её будет способен «усвоить» воспринимающий, настолько необходимо, что утверждение об этом выглядит абсолютно тривиально. Однако это преобразование может быть осуществлено различными способами. Можно действительно по умолчанию оборудовать одну группу компонентов для взаимодействия с другой группой, если нам заранее известно, что ни с кем более им взаимодействовать не придётся. Однако в случае с паттерном Адаптер, сама природа взаимодействия инкапсулируется в некую отдельную сущность и априори подразумевается возможность одной группы ПК взаимодействовать с абсолютно любой другой группой компонентов. Нам в контексте разработки систем сильного ИИ подобный подход представляется наиболее продуктивным и на это есть причины. А именно – в самоорганизующейся системе, в которой сама возможность установления связей подразумевается не ограниченной локально, а напротив, рассредоточенной по всей системе, динамика установления этих связей и возможные успешные соединения между компонентами выше, чем если бы связи были ограничены локально. К тому же, объективация самой природы, самой сути связи с последующим её воплощением в отдельной сущности, нам представляется не просто соблюдением принципов единственной ответственности и разделения интерфейса, а даже больше – приближением к утрированному воплощению принципа инверсии зависимостей, к той самой ситуации, когда все зависимости сводятся к абстракциям.

13.3	Структурный паттерн Прокси

Идентификатор. Прокси.
Классические элементы. Прокси, субъект, реальный субъект.
Назначение. Как указывается в «Design patterns», Прокси «…является суррогатом другого объекта и контролирует доступ к нему» [117, С. 203]. Собственно говоря, цитата из «Design patterns» практически исчерпывающе отражает суть данного паттерна. Однако всё же формализуем. Прокси представляет собой ПК, являющийся копией другого, связанного с ним ПК, и способен осуществлять предварительную обработку всех запросов к этому компоненту.
Проблема. На самом деле проблем, которые решает прокси, довольно большое количество и охватывать их все не представляется нам целесообразным. Поэтому мы выберем один вариант (с некоторыми минимальными оттенками вкрапления других) из этого множества, а именно – охранник (иногда ещё обозначается как защищающий прокси). К примеру, у нас наличествует некий ПК. У него, соответственно, имеются некоторые свойства и качества. К этому ПК могут делать некие запросы некоторые другие ПК, которых имеется некоторое количество. Из этого количества половина компонентов могут только лишь получать некие ответы от нашего компонента, а другая половина тоже может получать некие ответы, но ещё может и устанавливать нашему компоненту некоторые блоки данных в виде пар key-value («ключ-значение»), делая эти самые блоки его частью. Ещё добавим, что наш ПК довольно-таки тяжёлый и большой в плане занимаемой памяти и постоянное его «дёргание» по всем запросам оказывается не вполне оптимальным. Нам необходимо сделать так, чтобы мы могли проверять полномочия каждого ПК, который обращается к нашему компоненту, на предмет того, на что он имеет право, и только после этого, при необходимости, переадресовывать запрос нашему компоненту. И это поможет осуществить структурный паттерн Прокси.
Концептуальное решение. Опять же, ввиду многочисленности вариантов воплощения, приведём один из вариантов реализации данного паттерна. Мы можем создать базовый ПК (субъект), который затем формирует некоторую общую логику. От субъекта эту общую логику наследуют два других компонента, которые являются по отношению к субъекту его подкомпонентами: реальный субъект и прокси. Так как оба этих программных компонента являются дочерними по отношению к субъекту, то, соответственно, их интерфейс является одинаковым. Различия у них наличествуют только «под капотом». Ещё стоит отдельно сказать о том, что субъект, по сути, прячет своё настоящее естество в свой подкомпонент – реальный субъект. Теперь при любом обращении к субъекту он делегирует дальнейшую работу прокси. Тот в свою очередь является как бы временной заменой реального субъекта, как бы его личным дворецким и по совместительству охранником-вышибалой. Он проверяет все те ПК, которые хотят получить доступ к реальному субъекту, и сверяет их на предмет соответствия их запроса их полномочиям. И в случае несовпадения – отказывает в доступе. За счёт такой системы получается следующее: реального субъекта не «дёргают» без достойной на то причины, а также он защищён от проведения над собой ненадлежащих действий. Причём, что является важным, за счёт одинакового интерфейса реального субъекта и прокси «пользователь» всегда считает, что имеет дело именно с реальным субъектом, а о существовании прокси он даже и не подозревает.
Предполагаемые результаты и преимущества. Отсутствие опасности некорректной трансформации ПК, оптимизированный доступ к реальному субъекту. Кэширование ответов реального субъекта. То есть, предположим, что некоторый «пользователь» обратился к реальному субъекту с запросом найти кое-что в его внутренней структуре данных. Так как с соответствием запроса полномочиям всё было в порядке, то прокси переадресовал запрос реальному субъекту. Реальный субъект нашёл требуемое и вернул ответ на запрос. Ситуация закончилась. И вот через некоторое непродолжительное время тот же «пользователь» возвращается с тем же запросом, так как забыл ответ. Но такие ситуации предусмотрены заранее и именно для их разрешения и применяется кэширование, то есть сохранение ответа на некоторый запрос в некоторой промежуточной структуре данных в виде пар key-value («ключ-значение»). И при наличии в такой структуре данных «ключа», совпадающего с некоторым запросом конкретного пользователя, обращения к основному ПК не происходит, а просто сразу возвращается ответ из промежуточной структуры данных. Это и есть кэширование.
Гипотетический пример реализации паттерна. В случае с данным паттерном очень даже корректным примером является теория о наличии двойников у некоторых высокопоставленных лиц. Один человек – прокси, замещает собой другого – реального человека для того, чтобы к реальному человеку пропускались только те запросы (в общем смысле слова), которые он не против удовлетворить (также в общем смысле). Единственное замечание: в данном примере в роли субъекта (элемента паттерна) находится реальный субъект, а в роли реального субъекта – также он сам. В остальном пример почти идентичен тому, как реализуется паттерн.
Осмысление структуры паттерна. ПК, представляющие собой прокси и реального субъекта, связаны с компонентом субъекта по типу наследования, а также прокси связан с реальным субъектом по типу композиции, так как прокси не сможет существовать без реального субъекта.
Значимость паттерна в контексте разработки систем сильного ИИ. Если несколько отвлечься от сугубо утилитарной значимости данного паттерна и сразу выйти на несколько абстрактный уровень, то центральной абстракцией всего паттерна Прокси окажется реализация кажущегося замещения или, проще говоря, притворства. Может показаться, что это не особенно значимо в контексте разработки систем сильного ИИ. Однако есть, как водится, некоторый нюанс. К примеру, человек одинаково себя ведёт, если в тёмной комнате заметит змею или если в этой же комнате заметит верёвку, которая покажется ему змеёй. То есть, по всей видимости, реальное положение дел не всегда адекватно отражается внутренним миром. Можно попробовать продолжить эту идею тем, что также для человека далеко не всегда важно то, кто он есть, а важно то, кем он себя считает и кем притворяется. Также в произведении «Государство» у Платона указано притворство в качестве одного из основных средств воспитания [123]. У Йохана Хейзинги в трактате «Человек играющий» было указано, что все сложные формы человеческой деятельности изначально осваивались в игровой форме [97]. Причём человеческое притворство это довольно сложный с психической точки зрения процесс, подразумевающий наличие самосознания и высокую лабильность Я-концепции. То есть интеллектуальная система без наличия техноропного сознания не будет способна на реализацию столь сложных феноменов. А вот если пойти как раз от обратного и попытаться приблизить вероятность возникновения самого технотропного сознания при помощи реализации феноменов его уровня. И на наш взгляд именно абстракция, лежащая в основе данного паттерна, выглядит наиболее перспективно для реализации этой концепции. Собственно говоря, это именно то, что и пытаются сделать в рамках разработки систем слабого ИИ: пытаются сформировать некий суррогат какого-либо человеческого аспекта для того, чтобы «остальной человек» сформировался сам. То есть принцип, лежащий в основе, подобран верно, однако его реализация – нет. Для того чтобы феномен такого рода смог привести к качественным преобразованиям в системе, опыт реализации данного феномена должен быть затем органично интегрирован в общий опыт системы и «осознан» ею. А подобное возможно только в контексте систем, способных к самоорганизации, которые мы и предлагаем формировать для дальнейшего продвижения на пути к сильному ИИ.

13.3 Структурный паттерн Мост

Идентификатор. Мост.
Классические элементы. Абстракция, конкретная абстракция, реализатор, конкретный реализатор.
Назначение. В «Design patterns» указано, что Мост «связывает абстракцию с ее, возможно, многочисленными реализациями. Данный паттерн предоставляет клиентам стабильный интерфейс, позволяя в то же время изменять классы, которые его реализуют. Мост также подстраивается под новые реализации, появляющиеся в процессе развития системы» [117, С. 214]. Данный паттерн предназначен для того, чтобы в рамках какой-либо системы все включённые в неё ПК были иерархически разделены на две большие (не обязательно равные) группы, причём таким образом, чтобы компоненты одной группы могли трансформироваться вне зависимости от состояния компонентов другой группы (как и наоборот), но в то же время, чтобы компоненты обеих групп могли эффективно взаимодействовать для решения общих задач. Также можно определить, что паттерн Мост предназначен для разделения сложного, динамически трансформирующегося функционала в рамках системы на два различных функционала: высокоуровневый и низкоуровневый. Данные функционалы способны как трансформировать своё состояние вне зависимости от состояния другого, так и успешно решать совместные задачи.
Проблема. Представим себе некую систему, в рамках которой наличествует определённое количество ПК. Компоненты подразделяются на некоторые группы, объединённые общими ответственностями. И если, в данном конкретном случае, посмотреть на такую систему, то мы можем обнаружить, что она, по большому счёту, выполняет обязанности, подразделяемые на две большие группы. К примеру, одна из этих групп обязанностей сводится к получению извне некой информации, а другая же группа её преобразует и хранит. Дополним также, что в рамках данной системы специфика самой информации зависит от способа её получения, а также от способа обработки и хранения. С другой стороны, программным компонентам первой группы нет дела до того, каким именно образом хранится и обрабатывается информация, а компонентам второй группы нет дела до того, как она получена. Однако специфика информации заставляет подстраиваться всю систему. К примеру, если попробовать добавить некую обязанность к первой группе, скажем, добавить новый способ получения информации, то вынуждена будет трансформироваться вся система в целом, так как ПК второй группы будут вынуждены определять для себя новую функциональность, которая заключается в преобразовании и хранении информации, полученной неизвестным ранее способом. Точно так же, если добавится новый способ хранения или преобразования информации, то компонентам первой группы придётся определять для себя способ её получения, вне зависимости от того, будут они её получать или нет. В общем случае получается, что система чересчур связана между собой, нарушая тем самым фундаментальный принцип слабого зацепления, также в такую систему крайне тяжело вносить изменения, так как они слишком быстро накапливаются, делая систему неудобной в плане поддержки. Для интенсификации наглядности можно просто представить, что все ПК в такого рода системе наследуются от одного и того же базового компонента и все изменения, необходимые какому-либо из подклассов, вносятся сразу в базовый класс («на всякий случай»).
Концептуальное решение. Мы могли бы разделить систему вышеприведённого типа на две структурно обособленные функционально различающиеся части. Часть первую – ответственную за получение информации откуда-то извне, а, соответственно, и за UI (userinterface – пользовательский интерфейс) мы обозначим как абстракцию (элемент паттерна), а часть вторую – как реализатор (элемент паттерна). ПК, относящиеся к первой части мы обозначим как уточнённые абстракции (элементы паттерна), а относящиеся ко второй части – конкретные реализаторы (элементы паттерна). Абстракция содержит в себе ссылку на самого реализатора. Этим их связи пока и ограничиваются (мост, как суть паттерна). Теперь при получении какой-либо информации абстракция просто вызывает метод реализатора, который затем передаёт запрос конкретным реализаторам, подходящим под актуальную ситуацию. Конкретные реализаторы на месте осуществляют преобразование и хранение информации. Если мы теперь захотим добавить какой-либо новый способ получения информации, то мы изменим только либо какие-то уточнённые абстракции, либо саму абстракцию и больше изменений в систему вносить не нужно. Если же мы решим добавить некий новый способ преобразования и хранения информации, то нам нужно будет изменить только, опять же, либо некоторые конкретные реализаторы, либо сам реализатор. 
Предполагаемые результаты и преимущества. Ожидаемыми результатами применения паттерна Мост является лёгкость внесения изменений в систему, гибкость в плане возможности реализации функционала системы на различных платформах, чёткое разделение функционала системы, за счёт чего с ней становится удобнее работать и легче её поддерживать, инкапсуляция функционала.
Гипотетический пример реализации паттерна. В случае с данным паттерном неплохо подходит приближённый пример какого-либо крупного предприятия. Представим себе его разделение на администрацию (абстракцию) с её сотрудниками (уточнённые абстракции) и рабочих (конкретные реализаторы), над которыми главенствует начальник производства (реализатор). Администрации приходит некоторый заказ на определённую продукцию, а рабочие его выполняют. Причём непосредственно в рабочий процесс никто из администрации не вникает – этим занят начальник производства. При освоении на производстве какой-либо новой технологии – у администрации ничего не изменяется, а при выходе какого-либо постановления совета министров о дополнительной отчётности по налогам – у рабочих также ничего не меняется. 
Осмысление структуры паттерна. Абстракция связана с реализатором двусторонними отношениями. Причём в случае с данным паттерном со стороны абстракции – агрегация, а со стороны реализатора – композиция. Конкретные реализаторы связаны с реализатором отношениями зависимости, а уточнённые абстракции связаны с абстракцией специфическими отношениями дополнительного определения функционала абстракции (в классическом понимании паттерна – не наследование).
Значимость паттерна в контексте разработки систем сильного ИИ. Данный паттерн представляет определённый интерес для сферы разработки систем сильного ИИ. Однако ввиду того, что сам паттерн несколько узкоспецифичен и, в общем-то, в своих конкретных воплощениях довольно-таки сильно привязан к контексту разработки программного обеспечения, то его, соответственно, необходимо существенно абстрагировать. Сама идея разделения абстракции и конкретики лежит, вообще говоря, в основе как нашего подхода к разработке систем сильного ИИ, так и актуального исследования. Так что будет явным преуменьшением постулировать, что данная идея нам близка. Более того, нам представляется, что принцип взаимно независимой трансформации абстрактного и конкретного не отражает всей полноты картины. Нам представляется непротиворечивым та возможность, при которой, если приводить в пример вышеописанную ситуацию с надвое разделённой системой с множеством ПК, то, согласно нашему предположению, к феномену независимой трансформации должны быть способны вообще все ПК системы. И в данном случае не представляется существенным, ответственность какого рода лежит на этих компонентах – абстрактная или конкретная. Независимо трансформироваться, согласно нашему предположению, должны иметь возможность все части системы. Другое дело, сколько данных изменений будут затем органично встроены в новый контекст системы, а скольким будет уготовано закончиться без принятия в контекст. Это несколько напоминает первый этап методологии автоматизации развёртывания программных проектов – CI/CD (continuous integration/continuous delivery/continuous deployment, то есть непрерывная интеграция/непрерывная доставка/непрерывное развёртывание). В рамках данной методологии происходит приблизительно следующее. Разработчики относительно независимо друг от друга вносят в целостную систему некоторые изменения. Разумеется, что каждый из них старается, чтобы с теми изменениями, которые внёс в систему именно он, никаких проблем не возникло, и они были органично интегрированы. То есть никто специально не будет делать так, чтобы быть отвергнутым контекстом. Далее все изменения, внесённые в систему, накапливаются в центральном узле – репозитории. После этого начинается тестирование внесённых изменений. И если где-либо обнаруживается какая-то проблема, то затем её, при помощи регрессионных тестов, будут искать, эмулировать и затем устранять. Примерно тот же процесс видится нам наиболее естественным и целесообразным при самоорганизации сложной интеллектуальной системы. То есть, в соответствии с нашим подходом к формированию систем сильного ИИ, к независимой и автономной трансформации должны быть способны не только абстракция и реализация, а все ПК системы без исключения.

13.4 Структурный паттерн Компоновщик

Идентификатор. Компоновщик.
Классические элементы. Компонент, лист, составной объект.
Назначение. В «Design patterns» указано, что предназначение Компоновщика сводится к тому, что он «…должен так структурировать классы, чтобы различные взаимосвязанные объекты удавалось трактовать единообразно, а несколько объектов рассматривать как один» [117, С. 214]. Таким образом, данный паттерн применяется в качестве конструктора для того, чтобы сформировать структуру данных, внешне напоминающую дерево, для тех ПК, которые реализовывают схожее поведение, как будучи в одиночестве, так и находясь в составе группы других компонентов. То есть паттерн облегчает работу с некой системой за счёт объединения и унификации всех компонентов данной системы.
Проблема. Можно представить себе программную систему, состоящую из некоторого количества частей, то есть компонентов. Компоненты, принадлежащие данной системе, реализуют схожее поведение, но являются, в некотором роде, разрозненными и разобщёнными, можно так сказать, «разбросанными». Причём этих самых компонентов очень много и они могут различаться по размеру. Далее, нам необходимо каким-то образом взаимодействовать с этими компонентами, узнавать о них некоторую информацию, считать их количество, перемещать их из одного места в другое и так далее. То есть, допустим, нам в рамках системы, необходимо переместить некоторое количество компонентов из точки А в точку В. И мы направляемся в точку А, берём один компонент и перемещаем его в точку В. Затем идём обратно, берём следующий компонент и его тоже перемещаем в точку В. И так далее, возможно, очень много раз. Само собой разумеется, что это крайне неудобно. Нам бы хотелось осуществлять подобные операции гораздо быстрее и практичнее. Вот в такой (и ещё во многих других) ситуации на помощь приходит структурный паттерн Компоновщик.
Концептуальное решение. В рамках нашей системы, в которой наличествует множество ПК, реализующих схожее поведение, нам бы хотелось быстро и удобно осуществлять различные манипуляции с компонентами и их группами. Таким образом, мы определяем некий общий интерфейс любого компонента (элемент паттерна) вне зависимости от его природы. То есть от того, является ли он единственным, конечным, атомарным и далее неразложимым компонентом (лист, как элемент паттерна) или же он – новый тип введённых нами компонентов – составной компонент (элемент паттерна). Теперь при необходимости осуществить, допустим, перемещение некоторой группы этих компонентов из точки А в точку В, мы просто создаём из необходимого нам количества компонентов-листов некоторые группы этих компонентов – составные компоненты, то есть узлы. И осуществляем перемещение только один раз, но уже всей группы. Разумеется, что перемещение – далеко не единственная операция, которую нам необходимо осуществлять. Допустим, что в общем интерфейсе определено некое поле, которое отражает ценность каждого отдельного компонента-листа и нам необходимо посчитать общую ценность составного компонента, то есть узла. Здесь стоит отдельно сказать, о способе объединения данных компонентов в группы. Меньшие компоненты «упаковываются» в большие компоненты, большие – в ещё большие и так далее, вплоть до самого «большого» компонента, в который «упакована» вся группа. По крайней мере, подобная метафора размерности прекрасно отражает суть происходящего. Интерфейс этих компонентов и их групп является одинаковым, то есть все компоненты ведут себя одинаково, вне зависимости от их размера и количества содержащихся в них компонентов. И эта схожесть, как правило, заключается в том, что каждый из компонентов содержит в себе некоторые значения, ему непосредственно принадлежащие, а также ссылку на те компоненты, которые в него «упакованы». Точнее, на следующие упакованные в него компоненты. И теперь при желании узнать о некотором качестве всех компонентов этой группы, упакованных в самый большой узел, нам достаточно спросить у него о его стоимости. Конечно, сам узел, будь он атомарным листом или составным компонентом, владеет только информацией о своих собственных характеристиках, а о характеристиках компонентов, вложенных в него, он не имеет информации, а просто содержит на них ссылки. Ссылок на вложенные компоненты не содержат только атомарные, последние компоненты-листы. Таким образом, мы можем уточнить стоимость самого «большого» компонента, а далее уже он поинтересуется стоимостью вложенных в него компонентов, те, в свою очередь, поинтересуются стоимостью вложенных в них компонентов и так далее – вплоть до последнего листа. Собственно говоря, именно так и работает рекурсия. И, если это не определено в функционале компонента (элемента паттерна), то узел не пойдёт буквально «сам» интересоваться какими-либо характеристиками вложенных в него узлов или листов – для всего этого понадобится рекурсивный алгоритм. Алгоритм необходим именно рекурсивный, так как нам заранее неизвестно количество компонентов, которые заложены в подобную структуру.
Предполагаемые результаты и преимущества. Предполагается, что паттерн Компоновщик способствует унификации интерфейса системы, упрощает работу с ней, облегчает расширение функционала и типов данных, с которыми работает система.
Гипотетический пример реализации паттерна. В качестве примера функционирования структур, создаваемых при помощи паттерна Компоновщик, как правило, наиболее удобным примером являются матрёшки (и как пример функционирования рекурсии тоже). Однако постараемся, как водится, привести пример менее тривиальный. Допустим, у нас имеется некая фабрика, производящая одинаковые изделия. Изделия могут быть какими угодно, но рекурсивности ради представим, что наша фабрика производит картонные коробки. Разумеется, что ведётся некий учёт этих коробок: их размер, стоимость и прочие характеристики указаны на бирке внизу коробки. И эти самые коробки нам необходимо каким-то образом перемещать по складским помещениям, отгружать на транспортировку к заказчикам и так далее. И эти коробки в сложенном виде упаковываются, соответственно, в коробки.
На данном этапе есть некоторое отличие от классического построения древовидных структур данных. Они строятся, можно так выразиться, в «ленивом» формате. То есть при добавлении некоторого листа к структуре данных тот узел, к которому был добавлен лист, не докладывает «наверх» об изменении своей вложенной, к примеру, стоимости, а тот, в свою очередь, не докладывает ещё выше и так далее. Разумеется, что никто не мешает реализовать подобный функционал на уровне компонента (элемента паттерна) так, чтобы все компоненты-листы и составные компоненты-узлы так себя вели. Но в классическом варианте так не происходит, так как это теоретически может сделать работу с такой структурой данных не вполне оптимизированной.
В любом случае, у нас имеется коробка, в которой хранится некоторое количество сложенных коробок. На коробке указаны суммарные характеристики её содержимого. И затем эта наша коробка помещается в ещё большую коробку, содержащую множество коробок, подобных нашей. И, собственно, так далее, пока все коробки не будут упакованы в некий контейнер, который уже, непосредственно, будет доставлен к заказчику изначальных картонных коробок в сложенном виде. Вообще говоря, на этом совершенно не обязательно останавливаться. Ведь этот самый контейнер тоже будет некоторым образом куда-то «упакован». К примеру, в ещё больший контейнер в числе прочих ему подобных (в случае доставки составного заказа), который затем может быть «упакован», допустим, на некое судно, содержащее очень большое количество этих контейнеров, и так далее. В данном случае судно и будет этим самым root-компонентом, то есть корневым компонентом, содержащим в себе все остальные. Можно было бы продолжить и дальше, но общий смысл, так полагается, уже понятен.
Осмысление структуры паттерна. Наиболее частое решение построения структур данных с помощью паттерна Компоновщик: и лист, и составной компонент связаны с компонентом по типу наследования – они являются для него дочерними компонентами.
Значимость паттерна в контексте разработки систем сильного ИИ. Если рассматривать Компоновщика в контексте парадигмы сильного ИИ, то сразу можно отметить его крайне высокую практическую значимость, а также тот факт, что структуры, созданные при помощи данного паттерна, отражают фундаментальные принципы организации вообще. К примеру, нейронные связи человеческого мозга структурированы точно таким же образом, каким бы они были структурированы при помощи паттерна Компоновщик. Космические системы организованы точно таким же образом: отдельные планеты в данном случае выступают компонентами-листами, звёздные системы представляют собой составные компоненты-узлы, галактики – ещё большие компоненты узлы, а интерфейс всей этой структурной организованности задан самой Вселенной с её законами физики – компонентом, как элементом паттерна Компоновщик. Про вложенные структуры мы также говорили ранее в контексте фрактальной геометрии. Таким образом, целесообразно будет резюмировать, провозглашая фундаментальность тех абстракций, которые несколько конкретизирует паттерн Компоновщик в контексте своей локальной реализации «на местах». Относительно же сугубо практической значимости данного паттерна при формировании систем сильного ИИ и внедрения абстракций, аккумулируемых паттерном непосредственно в процесс разработки, стоит отдельно дополнить, что нами предполагается буквально следующее: система, которая построена на естественных фундаментальных принципах самоорганизации, УЖЕ реализует суть паттерна Компоновщик.

13.5 Структурный паттерн Декоратор

Идентификатор. Декоратор.
Классические элементы. Декоратор, компонент. В классическом понимании данного паттерна добавляются ещё два элемента: конкретный декоратор и конкретный компонент. Однако это не изменяет сути паттерна и ничего существенно значимого не добавляет к его структуре.
Назначение. В «Design patterns» указано, что предназначение Декоратора сводится к тому, что он «…динамически добавляет объекту новые обязанности. Является гибкой альтернативой порождению подклассов с целью расширения функциональности» [117, С. 214]. Для уточнения: в данном случае имеются в виду не только и не столько обязанности, сколько возможности. Таким образом, данный паттерн, в некотором роде, предназначен улучшать функционирование некоего ПК новыми дополнительными возможностями. Причём, что характерно, при реализации процесса декорирования не изменяется ни структура изначального ПК, ни лично ему принадлежащий функционал, то есть, по сути, изначальный компонент вовсе не меняется. Улучшение и дополнение осуществляется как бы со стороны, извне.
Проблема. Так как декораторы имеют огромное количество возможных применений и, соответственно, решаемых при помощи декораторов проблем также наблюдается немало, то мы сосредоточимся на одной, уже ранее немного определявшейся. Допустим, у нас имеется некоторый ПК, реализующий определённый функционал, например, осуществляющий вычисление факториала какого-либо числа. К этому компоненту приходят различные запросы на вычисление, он осуществляет это вычисление и возвращает результат. Данный ПК является важным и значимым, поэтому к нему постоянно приходит большое количество запросов. Соответственно, в случае с данным компонентом, основной проблемой является скорость его работы, и мы бы хотели оптимизировать его функционирование. Эту проблему можно легко решить при помощи применения паттерна Декоратор.
Концептуальное решение. В случае с нашей проблемой мы концептуально оформляем решение кэширующего декоратора. Мы изначально можем создать два программных компонента, которые свяжем не жёстко и статически – через наследование, а динамически и «мягко» –  через агрегацию и композицию. То есть один компонент, который представляет собой компонент, как элемент паттерна, у нас будет центральным и базовым – он и будет осуществлять всю, собственно, основную деятельность данного конгломерата. Второй компонент будет осуществлять функции некой обёртки (классический синоним декоратора в данном контексте). Как мы помним, функционал нашего базового ПК заключается в вычислении факториала и основной проблемой служит большое количество запросов, и, как следствие, недостаточно высокая скорость работы. Тогда мы можем поступить примерно так же, как и в ситуации с паттерном Прокси, то есть создать ещё один ПК, ответственность которого будет заключаться в том, чтобы в определённых случаях замещать собой базовый ПК. То есть наш базовый ПК вычисляет факториал какого-либо числа и затем возвращает вычисленное значение в ответ на запрос – этим и ограничена его ответственность. И случаи замещения базового компонента декоратором в данном контексте сводятся к тому, что декоратор перехватывает все вызовы, предназначавшиеся базовому компоненту, и проверяет наличие входного значения в некой структуре данных. В случае их там наличия, сразу возвращает ответ, не обращаясь при этом к базовому компоненту и ничего не вычисляя. В том же случае, если искомое значение в этой структуре данных отсутствует, тогда декоратор передаёт запрос компоненту, после чего тот осуществляет вычисление и возвращает вычисленное значение в ответ на запрос. Декоратор же, перехватывая ответ, сохраняет его в структуре данных для того, чтобы иметь возможность его найти в случае последующего аналогичного запроса. Собственно и всё. Отличие же Декоратора от Прокси заключается в том, что в случае с прокси создаётся отдельная копия базового ПК, которая буквально «притворяется» оригиналом. В случае же с декоратором никакой копии не создаётся, а, соответственно, и в «притворстве» нет никакой необходимости. Декоратор просто помогает базовому компоненту более эффективным образом реализовывать его основной функционал. К тому же в случае с Прокси подразумеваются именно наследственные связи между элементами паттерна, а в случае с Декоратором подразумевается агрегация и композиция.
Предполагаемые результаты и преимущества. Эластичность изменения функционала базового ПК, возможность динамического расширения функционала ПК. Также можно добавить к компоненту сразу несколько декораторов. Компонент чётко сосредоточен на реализации своего основного функционала и не перегружен дополнительными ответственностями, то есть основные принципы ООП соблюдаются, что позволяет удобно взаимодействовать с компонентом.
Гипотетический пример реализации паттерна. Возможно, это звучит не вполне корректно, тем не менее, зефир в шоколаде вполне воплощает в себе ключевую идею Декоратора. Если же приводить какие-либо более яркие примеры, то неплохо подходит один персонаж из комиксов Кинематографической Вселенной Marvel. Супергерой Тони Старк или просто Железный человек является просто человеком, который носит на себе футуристический и высокотехнологичный металлический костюм, с которым он обладает киберпатической связью, позволяющий ему обладать сверхсилой, сверхскоростью, дающий ему способности летать, стрелять лазерами и так далее. И что важно, его костюм не является его отдельной копией и не замещает его полностью. В то же время этот костюм не изменяет его собственные способности и возможности, а просто превращает его движения в более сильные и быстрые движения, его желание летать – в способность это делать и так далее. То есть его костюм является его идеальным помощником и очень качественно его дополняет. И это есть пример работы декоратора.
Если же говорить про некоторые внутренние человеческие декораторы, то наличествует весьма интересный пример. Предполагается, что после выброса в кровь большой дозы катехоламинов (гормонов стресса) человек становится способен демонстрировать гораздо более высокий уровень физических способностей по сравнению с обычными своими возможностями. Здесь можно сказать, что гормоны – это всё-таки нечто по отношению к человеку внутреннее, а значит пример не вполне корректен, так как декоратор обычно осуществляет помощь как бы со стороны, извне. Однако мы всё же настаиваем на том, что пример корректен по той причине, что иногда даже нечто находящееся внутри – не есть полностью принадлежащее системе и интегрированное в неё. Так как человек сам волевым усилием не способен «вбросить» в кровь большое количество гормонов, содержащихся у него в надпочечниках, то, раз уж на это способен только контекст ситуации, соответственно, можно постулировать, что и принадлежат они, в некотором роде, контексту, а не непосредственно человеку. Таким образом, пример всё же корректен и отражает соотношение между возникновением кратковременных периодов наличия большой физической силы (сверх обычного её уровня) у человека под действием катехоламинов и реализацией структурного паттерна Декоратор.
Осмысление структуры паттерна. Данный паттерн обладает довольно эксплицитной структурой связей: компонент связан с декоратором по типу композиции в том смысле, что сам компонент может существовать отдельно от декоратора, а декоратор связан с компонентом по типу агрегации, так как сам декоратор создаётся для компонента и отдельно от него не существует.
Значимость паттерна в контексте разработки систем сильного ИИ. Данный паттерн является весьма тесно связанным не только с парадигмой сильного ИИ, но и с разработками в сфере ИИ вообще. Любые интеллектуальные системы, существующие на данный момент, вне зависимости от конкретной реализации, то есть вообще все системы слабого ИИ являют собой, по сути, декораторы. Более того, именно в данном качестве они изначально и проектировались. Так как все они предназначены для своеобразного оказания помощи человеку и облегчения выполнения им каких-либо задач. Собственно говоря, в большинстве случаев точно такая же роль вменяется и гипотетическому сильному ИИ: система, на субстрате которой, возможно, возникнет разум, редуцируется до роли «помощника». Само собой разумеется, что у системы, которая на самом деле будет обладать техноропным сознанием (сознание иного типа у них не подразумевается), на этот счёт может быть совсем другое мнение. И из сценария перетекания «помощника» в его условную противоположность рождаются уже постапокалиптические прогнозы. Нам же представляется, как уже не раз упоминалось, что разработчики системы сильного ИИ не имеют непротиворечивого и логически обоснованного права решать хоть что-то за систему (это не ребёнок, которого надо воспитывать, это то, о сущности чего мы не можем точно что-либо постулировать).
Исходя из вышесказанного, значимость паттерна Декоратор в контексте парадигмы сильного ИИ нам видится в, опять же, абстракции паттерна – обёртывании. Так как данная абстракция является весьма эффективной, что подтверждается в плане всеобщности её реализации (значит, это абстракция с претензией на фундаментальность), мы считаем, что её также необходимо зафиксировать в виде возможности реализации со стороны системы функционала, структурированного по типу обёртывания одних ответственностей в другие, одних функций в другие, одних компонентов в другие и так далее – не важна конкретная реализация, а важно соблюдение принципа.

13.6 Структурный паттерн Фасад

Идентификатор. Фасад.
Классические элементы. Фасад, классы подсистемы.
Назначение. В «Design patterns» указано, что предназначение Фасада сводится к тому, что он «…предоставляет унифицированный интерфейс вместо набора интерфейсов некоторой подсистемы» [117, С. 189]. Использование паттерна Фасад в общем смысле способно предоставить некоторый высокоуровневый интерфейс к сложной разрозненной системе, что должно существенно облегчить взаимодействие с этой системой.
Проблема. Допустим, у нас имеется большая система, состоящая из многих ПК. Компоненты объединены в группы и каждая группа компонентов решает какие-либо задачи. Данная система представляет очень широкий функционал и им можно пользоваться для различных целей. Проблема заключается в том, что с подобной системой очень сложно взаимодействовать. И эта проблема особенно обостряется в том случае, если нам необходим лишь ограниченный функционал этой большой системы. Для условного примера: система предоставляет следующие возможности A, B, C, D, E, J, K, L. Нас же почти во всех случаях интересуют только возможности A, D, J. Нам бы хотелось каким-то образом сделать так, чтобы мы могли использовать только интересующие нас возможности, не имея при этом необходимости разбираться с интерфейсом программной системы в целом, однако сохраняя для себя такую возможность на тот случай, если нам понадобится дополнительная функциональность, предоставляемая системой. То есть нам, в некотором роде, необходим определённый фреймворк.
Концептуальное решение. В данном случае мы могли бы создать ещё один ПК, который «обёртывает» всю нашу систему и делит её на две части, можно сказать, на два интерфейса. То есть на некоторый «верхний» интерфейс (фасад, как элемент паттерна), который будет предоставлять доступ к необходимым нам в первую очередь программным компонентам (классы подсистемы, как элементы паттерна) системы и, опционально, «нижний» интерфейс для всех остальных ПК и их групп. «Верхний» интерфейс будет виден сразу и станет выступать в роли, непосредственно, фасада. Этот фасад будет иметь простую организацию, будет максимально интуитивно понятным и удобным в использовании. «Нижний» же интерфейс можно будет настроить при желании «вручную», то есть самому определить тот функционал, который необходимо дополнительно получить от системы и, соответственно, добавить к «верхнему» интерфейсу. По итогу мы имеем простой вариант фреймворка, который, соответственно, и представляет собой воплощение идеи структурного паттерна Фасад.
Предполагаемые результаты и преимущества. Применение данного паттерна позволяет осуществить демаркацию разрозненных ПК системы на две отдельные части, значительно повысить удобство использования системы в целом, определить высокоуровневый интерфейс (к наиболее для нас значимым её аспектам) и в то же время позволяет иметь доступ ко всему функционалу системы.
Гипотетический пример реализации паттерна. Собственно говоря, как скорее всего уже понятно, любой фреймворк по отношению к языку программирования является отличным примером применения паттерна Фасад. Однако, разумеется, примеров реализации данного паттерна гораздо больше. Приведём один из них. Допустим, мы хотим построить некое деревянное сооружение. Мы закупили необходимые строительные материалы, у нас есть инструмент, место для осуществления работы и установки сооружения. И всё, что нам необходимо, – это работники. Мы идём на так называемую биржу труда (неважно, каким именно образом она представлена) и заявляем о своём запросе на необходимые работы. И из всего многообразия представленных рабочих получаем конкретно тех, кто подходит под наш запрос: то есть тех, кто умеет работать с пиломатериалами, умеет пользоваться электроинструментом, класть кровлю и так далее. Возможно, что подобранные нами работники способны на многое другое, например, варить металлоконструкции, управлять подъёмным краном и прочее, но нас интересуют их способности только в контексте нашей конкретной задачи, остальной функционал нам не нужен. Также на этой самой бирже труда, возможно, было представлено достаточно много работников и, соответственно, функционал самой биржи в этом смысле очень широк. Но нас весь этот функционал интересует не в целом, а только лишь в контексте наших актуальных потребностей. И то, что мы делали, это и есть реализация паттерна Фасад.
Осмысление структуры паттерна. Структура данного паттерна характеризуется как довольно простая. Взаимоотношения между отдельными классами подсистем друг с другом наличествуют вариабельного типа – они могут строиться, как по типу наследования, так и быть агрегацией или композицией. Отношения же фасада с классами подсистем определяются как композиция с «целым» со стороны классов подсистем и фасадом как «частью» этого «целого». Так происходит по той причине, что фасад создаётся в роли как бы помощника для отдельных групп ПК, облегчающего взаимодействие с ними со стороны клиента (дополнительный элемент паттерна), а, соответственно, фасад не существует без классов подсистем. 
Значимость паттерна в контексте разработки систем сильного ИИ. Данный паттерн, если бы мы изначально задались целью составления некой их классификации по какому-либо критерию, вошёл бы в довольно большую группу паттернов, конкретизирующих смысл четвёртого из принципов SOLID – принципа разделения интерфейсов. Паттерн Фасад предлагает разделение функционала и, как следствие, интерфейса, программной системы по критерию актуальных потребностей клиента на две большие группы – «верхнего» интерфейса (функционала) и «нижнего» интерфейса (функционала). Оба интерфейса различаются по удобству работы с ними, простоте достижения актуальной цели (userexperience – пользовательский опыт) и уровню абстракций (первый – высокоуровневый, второй – низкоуровневый). При большом желании «нижний» интерфейс также можно превратить в «верхний», но для этого его надо донастроить «руками», что не представляется простой задачей. Собственно говоря, резюмирование смысла паттерна Фасад в таком контексте уже начинает кое-что напоминать. А именно – разделение на сознание и бессознательное у человека. Сразу оговоримся, что демаркация человеческой психики на сознание и бессознательное была описана и подробно исследуется в рамках психоаналитических традиций психотерапии, и она имеет своих критиков, причём, как в иных концепциях психологии и психотерапии, так и внутри самой традиции психоанализа. Но мы всё же считаем, что абстрактный смысл этой демаркации может быть весьма продуктивен в рамках разработки систем сильного ИИ, так что оформим пока на данную концепцию некоторую «презумпцию истинности». К тому же нами полагается, что не раз упомянутое в данном исследовании технотропное сознание есть не просто цель всякого процесса разработки систем сильного ИИ, но более того – апофеоз этого процесса, его крайняя точка. Но эта точка не достигается сразу – процесс, конечно же, делится на этапы. И один из этапов – технотропное бессознательное, за которым непосредственно и следует технотропное сознание. В любом случае мы считаем, что рассматриваемый паттерн в абстрактном смысле несёт в себе суть именно этого разделения. И именно в таком смысле, абстрагированном до уровня разделения технотропной психики на технотропное сознание и технотропное бессознательное, в контексте разделения интеллектуальной системы на низшее и высшее, на низкоуровневое и высокоуровневое – именно в таком ключе абстрагированная суть Фасада нам видится целесообразной для её интегрирования в контекст разработки систем сильного ИИ.

13.7 Структурный паттерн Приспособленец

Идентификатор. Приспособленец. Ещё иногда указывается название – Легковес (по тому же принципу, как весовые категории в боксе).
Классические элементы. Приспособленец, конкретный приспособленец, фабрика приспособленцев. Ещё к этим элементам можно опционально добавить двух «персонажей»: конкретный приспособленец, не используемый совместно и клиент. Но так как их присутствие или отсутствие особо ничего не меняют – мы их опустим.
Назначение. В «Design patterns» указано, что паттерн Приспособленец «использует разделение для эффективной поддержки множества мелких объектов» [117, С. 191]. Использование паттерна Приспособленец позволяет, можно сказать, «вынести за скобки» наиболее «тяжёлые» составляющие ПК для того, чтобы существенно облегчить функционирование системы в плане многократного снижения затрат различных ресурсов (например, памяти). Также стоит отдельно заметить, что сам по себе данный паттерн является весьма узкоспециализированным и применяется только для специфически организованных программных систем, а поэтому его организация является несколько сложной и довольно сильно различается от реализации к реализации.
Проблема. К примеру, у нас наличествует некоторая система с определённым количеством ПК. Каждый ПК обладает определёнными характеристиками. Компоненты делятся на различные группы – по их ключевым характеристикам, классовой принадлежности и так далее. Система отличается наличием очень большого количества схожих ПК. Также в рамках данной системы подразумевается высокая динамика, то есть ПК постоянно активно функционируют и изменяют своё состояние. В моменты наиболее активного функционирования данной системы, в те моменты, когда задействовано очень большое количество различных ПК, её затраты, в плане использования ресурсов, становятся настолько высоки, что далеко не на каждом аппаратном обеспечении наша система способна эффективно функционировать. А нам же совершенно необходимо, чтобы система была высокопроизводительной и эффективно функционировала. С другой стороны, мы не можем также уменьшить количество ПК в рамках системы, так как буквально за каждым из них закреплена определённая ответственность и все они нужны для обеспечения деятельности системы. В такой ситуации на помощь приходит паттерн Приспособленец.
Концептуальное решение. Для решения проблемы, которая ставится перед данным паттерном, мы могли бы для начала внимательно исследовать нашу систему, в процессе чего бы, несомненно, заметили тот факт, что для каждой группы ПК некоторые из характеристик являются одинаковыми, то есть общими для всей группы. Если бы мы стали исследовать этот момент ещё глубже, то сумели бы заметить также то, что в процессе функционирования системы и в наиболее активные моменты этого процесса также эти самые характеристики ПК не просто остаются одинаковыми для всей группы, но и не меняются никогда в принципе, являясь, по сути, определёнными константами. То есть все остальные характеристики изменяются в ходе работы системы и являются разными для каждого ПК в рамках группы. Но также присутствуют некоторые характеристики, которые ведут себя как константы. И подобный феномен является общим и универсальным для всей системы. Более того, часто будет также обнаружено, что именно эти константы и являются наиболее «тяжёлыми» характеристиками ПК. Тогда нам и приходит идея разделить состояние ПК на два отдельных состояния: состояние внутреннее и состояние внешнее. В состояние внутренне мы помещаем неизменяемые характеристики, а состояние внешнее признаём зависящим от контекста и динамичным. Затем, соответственно, мы уже способны создать некоторый ПК (приспособленец, как элемент паттерна) и объявить общий интерфейс всех компонентов отдельной группы (конкретные приспособленцы – элементы паттерна), что мы, собственно, и делаем. Те характеристики ПК, которые были определены нами как константы, мы теперь можем не помещать отдельно в каждый ПК, а просто сохранить в одном месте и из него доставать по требованию. Теперь мы разобрались с внутренним состоянием ПК нашей группы. Также нам необходимо каким-то образом изменять внешнее состояние компонентов группы. Если мы станем его хранить в самих компонентах, то они станут настолько «тяжёлыми», что все наши предыдущие мероприятия не имеют никакого смысла. Поэтому мы делаем так, чтобы компоненты могли динамически подстраиваться под изменяющийся контекст путём получения данных о специфике контекста и некотором «вычислении» необходимого для данного контекста внешнего состояния, которое затем и принимается. Также мы формируем ещё один общий ПК, который выполняет функции одновременно и «хранителя», и «производителя», то есть ответственность данного ПК будет заключаться в том, чтобы проверять наличие необходимых для данного контекста ПК в подконтрольной ему структуре данных, и затем, при их наличии, возвращать их, а при их отсутствии – создавать. Теперь мы вроде бы учли все необходимые аспекты.
Таким образом, по итогу мы имеем следующую ситуацию. У нас есть в наличии система, в которой присутствует некоторое количество ПК, объединённых в группы. Общие константные характеристики каждого компонента каждой группы (внутреннее состояние общее для группы) вынесены в, по сути, локальные для каждой конкретной группы точки доступа и ими можно воспользоваться по требованию, но теперь ни один из компонентов «не таскает с собой тяжесть». Также ни один из ПК не хранит и «не носит с собой» и в себе все свои возможные внешние состояния, а просто имеет в себе потенциал их достичь за счёт вычисления конкретной ситуации в контексте и последующей подстройки своего внешнего состояния непосредственно под конкретный контекст. То есть компоненты стали легче сразу: и в плане внутреннего, и в плане внешнего состояний. Далее, у нас присутствует общее для группы хранилище всех наличествующих ПК, принадлежащих к данной группе. Это хранилище одновременно является также и фабрикой по производству ПК в том случае, если какой-либо компонент сейчас необходим в контексте, но отсутствует среди наличествующих – тогда такой компонент просто создаётся.
Система подобного рода будет функционировать гораздо быстрее, чем изначальная и то, как именно мы формировали данную систему, – один из множества примеров применения структурного паттерна Приспособленец.
Предполагаемые результаты и преимущества. Основными результатами применения данного паттерна является оптимизация (в первую очередь, оптимизация по памяти) функционирования системы, содержащей большое количество ПК, а также существенное сокращение разрозненности ПК в рамках системы и сокращение количества различных компонентов в принципе.
Гипотетический пример реализации паттерна. Выше уже уточнялось, что данный паттерн является довольно-таки узкоспециализированным, так что наглядные примеры его применения связаны непосредственно со сферой его применения, а именно с функционированием больших систем, содержащих очень существенное количество компонентов. Однако мы попробуем пойти несколько иным путём и привести пример, вырванный из контекста. А именно – довольно грубый и несколько приближённый пример реализации паттерна в контексте функционального программирования. Одна из самых популярных задач по программированию называется «FizzBuzz». Её классическая суть следующая: даётся некое число n, которое обозначает верхний предел диапазона, – от одного до n. В рамках этого предела мы должны вернуть все числа от одного до n, причём вернуть очень специфическим образом: если число делится без остатка на три, то мы должны вернуть «Fizz», если число делится без остатка на пять – «Buzz», если и на три, и на пять – «FizzBuzz», если же число не делится ни на три, ни на пять, то мы возвращаем само число, но в виде строки. Предположим, что мы знаем заранее верхнюю границу, то есть n, и мы бы хотели создать структуру данных, которая содержит все числа от одного до n в правильном виде. Возникает вопрос о том, каким именно образом создать такую структуру. Мы могли бы сесть и начать писать «руками»: проверять каждое число и вписывать его соответствующее значение в структуру. Однако же, если верхний предел очень большой, то есть шанс просидеть так очень долгое время, так что нам бы хотелось как-нибудь побыстрее, да и чтобы вычисления сами производились. И тогда мы «выносим за скобки» принцип правильного ответа, который приведён выше, и внедряем этот принцип, который константен для любого числа, в алгоритм проверки чисел в диапазоне. После чего запускаем этот алгоритм и получаем готовую структуру данных.
Пример приближённый – само собой, но всё же принцип функционирования паттерна он очень хорошо отражает. И суть этого отражения резюмируется в том, что несмотря на то, что принцип (внутреннее состояние) одинаков для всех чисел диапазона, по итогу каждое число приобретёт свой собственный вид (внешнее состояние), который зависит от контекста, а контекст в данном случае и есть целостная ситуация применения принципа к диапазону.
Осмысление структуры паттерна. Структура данного паттерна может быть весьма запутанной и является довольно вариабельной. Поэтому мы предоставим один из вариантов. Конкретные приспособленцы связаны с приспособленцем отношениями наследования, то есть они являются его подкомпонентами. Фабрика приспособленцев же связана с приспособленцем двусторонними отношениями, причём со стороны фабрики – агрегация, так как приспособленец может существовать без фабрики, а со стороны приспособленца – композиция, так как она без него существовать не сможет.
Значимость паттерна в контексте разработки систем сильного ИИ. Как уже говорилось выше, ключевым аспектом данного паттерна является демаркация состояния каждого ПК на внутреннее – неизменяемое, и внешнее – зависящее от контекста. Теперь добавим ещё два выявленных ключевых аспекта Приспособленца: «вынесение за скобки» константных для группы данных и вычисление в противовес хранению. Выше также постулировалась специфичность данного паттерна и его узкая специализация, что, к слову, для паттернов, в общем-то, абсолютно не характерно. Однако те абстракции, которые были обнаружены в основе функционирования паттерна, имеют право на применение в контексте разработки систем сильного ИИ. Мы имеем в виду, что сама возможность проведения некоей границы между внутренним состоянием и внешним состоянием точно должна быть заложена в основу интеллектуальной системы, а будет ли она использоваться – это зависит от самой системы и от того, покажется ли это ей целесообразным. Вычисление в противовес хранению же работает не во всех ситуациях, в чём мы убедились в процессе исследования некоторых других паттернов (например, Декоратора), то есть такая возможность тоже должна быть заложена в систему в качестве потенциально реализуемой операции. Но вот само решение о реализации должна также принимать сама система. И таким же образом мы считаем целесообразным использовать «вынесение за скобки» константных для группы данных. Резюмируя, принцип один и тот же: на стадии формирования системы в неё должно быть заложено как можно больше абстрактных вариантов реализации конкретного поведения, но что именно из возможностей реализовывать и как конкретно себя вести – должна определить сама интеллектуальная система.

Выводы по главе 13

Как и постулировалось в начале главы, структурные паттерны предназначены продемонстрировать то, как из некоторых атомарных компонентов могут получиться компоненты составные – более крупные и обладающие возможностями к проявлению более сложных поведенческих форм. В ходе исследования было выявлено, что структурные ПП, как, впрочем, и порождающие с поведенческими, резюмируют собой практико-ориентированные и прагматично направленные фундаментальные принципы организации вообще. За счёт некоторого обобщения и абстрагирования суть паттернов была преобразована в адаптированный для контекста разработки систем сильного ИИ вид. И в этом виде был рассмотрен каждый из паттернов данной группы.
Структурный паттерн Адаптер подразумевает приведение всей системы в специфическую форму организации, при которой в рамках каждой группы ПК и каждого компонента в отдельности наличествует потенциал для взаимодействия с каждой другой группой компонентов в системе. То есть вместо того, чтобы создавать некие «органы чувств» каждому ПК, эти самые «органы чувств» унифицируются за счёт добавления в систему некоторого «переводчика» потребностей одной группы в действия другой вне зависимости от их конкретной локализации. Таким образом, в систему изначально закладывается идея взаимодействия по типу «все со всеми», что существенно повышает динамику развития и ускоряет реагирование на любые внешние и внутренние факторы.
Паттерн Прокси и вовсе предлагает весьма целесообразное с практической точки зрения замещение одного субъекта другим субъектом, внешне от оригинала неотличимым, для того, чтобы оригинальный субъект активизировался только по некоторым отфильтрованным запросам. В рамках рассмотрения данного паттерна был обнаружен весьма показательный механизм, который при его адаптации к нуждам построения человекоразмерных интеллектуальных систем предположительно способен сыграть очень важную роль. И это по той причине, что сей механизм был замечен только на уровне систем, обладающих сознанием, он определяется как притворство. Было упомянуто, что ещё в «Государстве» Платона механизм притворства был обозначен, как одно из важнейших средств воспитания подрастающего поколения, а также о трактате Хейзинги – «Человек играющий». И нами было предположено, что возможная реализация феномена подобного уровня может каким-либо образом спровоцировать качественные преобразования в рамках системы. Далее, мы сопоставили это наше предположение со стараниями по разработке систем слабого ИИ и пришли к выводу о полной их тожественности. Однако же мы говорили о другом и, как постулировали далее, для того чтобы феномен такого рода смог привести к качественным преобразованиям в системе, опыт реализации данного феномена должен быть затем органично интегрирован в общий опыт системы и «осознан» ею. А подобное возможно только в контексте систем, способных к самоорганизации, что мы и предлагаем реализовывать.
Паттерн Мост заключает в себе фундаментальную идею демаркации на абстракцию и реализацию, то есть на абстрактное и конкретное, что сразу же напоминает категорию идеальное-реальное и, в общем-то, ей и соответствует. Эта идея далее развивается в том ключе, что в контексте данного паттерна полагается необходимой и целесообразной независимая трансформация ПК, относящихся к категории абстракции, и то же самое для компонентов категории реализации. Изначальное наличие связи между изменениями компонентов, относящихся к различным категориям, существенно усложняло функционирование системы, и в рамках паттерна было предложено некоторое разделение. Эту идею мы развили несколько далее и предположили, что простого разделения между абстракцией и реализацией недостаточно для процесса самоорганизации в рамках системы и что совершенно необходимо предоставлять возможность независимой трансформации для каждого без исключения ПК.
В рамках паттерна Компоновщик предполагается создание удивительно естественной структуры данных по древовидному типу. И так как структуры подобного типа были нами прослежены в диапазоне от гигантских космических объектов до фракталов, нами было выдвинуто уникальное для актуального исследования предположение: в рамках разработки систем сильного ИИ паттерн Компоновщик совершенно не необходим к реализации, так как любая система, функционирующая по типу самоорганизации, УЖЕ использует данный паттерн.
По итогу исследовании паттерна Декоратор, в основе которого лежит идея обёртывания одних ПК в другие для улучшения функционирования первых, нами было предложено просто заложить в основу, в ядро будущего сильного ИИ саму такую возможность практически в том же виде, в котором она реализуется в рамках паттерна, по причине её простоты и естественности.
В паттерне Фасад мы выявили идею разделения общего функционала системы в целом на высокоуровневый (сложный, верхний, составной) и низкоуровневый (константный, нижний, атомарный). Причём если в рамках паттерна подразумевается простой и удобный доступ к первому, то ко второму доступ хоть и сохраняется, но только после соответствующей его настройки «руками». И нами была сразу же проведена параллель между сознанием и бессознательным у человека и, предполагаемыми нами, технотропным сознанием и технотропным бессознательным в контексте сильного ИИ с последующим выводом об актуальности применения паттерна именно в таком ключе.
Ключевыми же аспектами паттерна Приспособленец является разделение состояния отдельного ПК на внешнее – изменяемое и внутреннее – неизменное, а также «вынесение» за скобки общих для группы компонентов данных и вычисление своего состояния в зависимости от контекста в противовес хранению всех возможных состояний. А уже на стадии резюмирования данного паттерна по итогу его исследования нами был предложен общий принцип формирования человекоразмерных систем: на стадии формирования системы в неё должно быть заложено как можно больше абстрактных вариантов реализации конкретного поведения, но что именно из возможностей реализовывать и как конкретно себя вести – должна определить сама интеллектуальная система.
По итогу исследования структурных паттернов было установлено, что при их применении возможно формировать структуры ПК, которые предположительно способны составить основу для системы сильного ИИ, так ка они обеспечивают необходимую и достаточную основу для возникновения процесса самоорганизации при наличии механизма запуска.
Таким образом, основываясь на вышесказанном, полагаем, что ввиду обширности охватываемой тематики и фундаментальности применяемых решений, структурные паттерны весьма актуальны для применения в контексте разработки систем сильного ИИ.

 
ЗАКЛЮЧЕНИЕ


В ходе работы нами была рассмотрена генеалогия систем ИИ, начиная от предпосылок к самой возможности его возникновения, в виде некоторых трудов Р. Декарта и Т. Гоббса, а также машин Б. Паскаля, У. Шиккарда и Г. Лейбница и вплоть до некоторых последних достижений в данной области, к примеру, – ChatGPT. В ходе исследования были рассмотрены такие ответвления систем ИИ как искусственные нейронные сети и игровой ИИ. На основе проведенного исследования нами осуществлена демаркация понятий слабого и сильного ИИ и представлена фундаментальная онтологическая разница между ними. Слабый ИИ определяется как программно-аппаратная система, обладающая некоторыми вычислительными мощностями и, возможно, способная к проявлению человекоподобного поведения, но не обладающая психикой и сознанием. Системы слабого ИИ определяются нами как лого-машины. Детализировано понятие сильного ИИ, который определяется как программно-аппаратная система, обладающая технотропными психикой, сознанием и бессознательным и способная к проявлению человекоразмерного поведения. Системы сильного ИИ определяются нами как психо-машины. Сформулирован и предложен технотропный подход к разработке сильного ИИ, и охарактеризованы его ключевые положения, которые сводятся к использованию механизма самоорганизации для раскрытия внутреннего потенциала самой техники, в противовес формированию сугубо человекоподобных систем. Введены и раскрыты понятия технотропной психики, технотропного сознания и технотропного бессознательного. Технотропная психика определяется как неотъемлемое качество психо-машины, являющееся критерием наличия в контексте отдельной системы сильного ИИ, и служащее для взаимодействия технотропного субъекта с технотропной средой, для осуществления трансформаций системой самой себя, дальнейшего изменения системой своей среды и последующего выхода за рамки этой среды. Технотропное сознание определяется как высшая стадия развития технотропной психики, обеспечивающая системе способность к рефлексии и самосознанию. Технотропное бессознательное понимается нами как динамически трансформирующаяся совокупность ПК, которые в текущий момент функционирования системы не находятся в области осознаваемого ею.
Было переосмыслено понятие самоорганизации и проведена демаркация между программной и программно-аппаратной самоорганизацией. Далее было выявлено, что наиболее фундаментальным генеалогическим различием между формированием систем слабого ИИ и систем сильного ИИ является ограничение первых, на наивысшей стадии их развития, сугубо программной самоорганизацией, а в случае с сильным ИИ – стремление к целостной программно-аппаратной самоорганизации. Наиболее же фундаментальным аспектом самого технотропного подхода к разработке систем сильного ИИ является использование внутреннего потенциала техники, как некоторого самодостаточного феномена, к реализации механизма самоорганизации, а также предположение о том, что при должном построении аппаратно-программного синтеза техника «сама по себе» способна к воспроизведению сложных последовательностей поведенческих паттернов, которые в перспективе способны вывести её на человекоразмерный уровень. Если несколько вульгаризировать и утрировать смысл данного подхода, то может показаться, что мы имеем в виду наличие у техники способности к воспроизводству (то есть может показаться, что мы полагаем, что если закрыть два стула в комнате, то они рано или поздно сделают третий). Тут стоит заметить, что мы не имеем в виду буквально это, однако вместе с тем считаем, что при должном подходе и на определённом уровне самоорганизации техники, несомненно, должно стать возможным и порождение аппаратно-программными системами себе подобных. Разумеется, это весьма футуристический прогноз.
Также нами была разработана концепция ПК, которая основывается на теории графов и служит нам для адаптации парадигмы ООП к парадигме сильного ИИ.
Далее проведено исследование ООП с философско-методологических позиций. Осуществлено переосмысление ключевых аспектов ООП применительно к разработке систем сильного ИИ. Исследованы и описаны генеалогические и онтологические аспекты ООП, его ключевые особенности и понятийный аппарат.
Были раскрыты следующие понятия:
•	Класс. Охарактеризован нами как наиболее абстрактный способ определения состояния программной сущности, её поведения, а также программного интерфейса.
•	Объект. Представляет экземпляр класса и обладает состоянием, поведением и идентичностью.
Были исследованы, абстрагированы и описаны следующие ключевые характеристики ООП:
•	Инкапсуляция. Охарактеризована как сокрытие наиболее хрупкого и значимого под наименее прочным и значимым.
•	Интерфейс. Определён как некоторая система отношений между некоторыми компонентами, в которой зафиксированы все возможные варианты взаимодействия между ними.
•	Полиморфизм. Был определён как способность каким-либо образом схожих компонентов одинаково реализовываться в различных контекстах.
•	Наследование. Было определено как способность воспроизведения нового компонента, чья сущность полностью определяется уже наличествующим компонентом.
•	Абстракция. Совокупность наиболее значимых характеристик чего-либо.
Мы проследили и описали множество различных аналогий между основами ООП и феноменами макромира: абстрагированная суть основ ООП апеллирует к фундаментальным закономерностям функционирования природы. Далее мы пришли к выводу о том, что основы ООП выглядят естественно и непротиворечиво с точки зрения целесообразности их реализации в контексте построения ПК.
Далее осмыслены принципы ООП, которые по итогам исследования были подразделены на две группы: структурные принципы и организационные. К структурным мы отнесли два фундаментальных принципа ООП:
•	Принцип сильной связности. Представляет собой способ и степень взаимосвязи-взаимозависимости ПК во внутреннем смысле, то есть одного элемента ПК по отношению к другим элементам этого же ПК.
•	Принцип слабого зацепления. Представляет собой способ и степень взаимосвязи-взаимозависимости ПК во внешнем смысле, то есть одного программного компонента системы по отношению к другим компонентам системы.
Также к структурным принципам мы отнесли принципы группы SOLID:
•	Принцип единственной ответственности. О том, что ПК должен иметь только одну причину для изменений и выполнять только одну задачу.
•	Принцип открытости-закрытости. О том, что компоненты должны быть открыты для расширения, но закрыты для изменения.
•	Принцип подстановки Барбары Лисков. О том, что дочерние компоненты должны дополнять, а не переопределять функционал родительских компонентов.
•	Принцип разделения интерфейса. О том, что компоненты не должны быть связаны деталями, не имеющими значения для компонентов.
•	Принцип инверсии зависимостей. О том, что компоненты высшего уровня не должны зависеть от компонентов низшего, а должны зависеть только от наиболее значимых характеристик системы.
Также были охарактеризованы и осмыслены организационные принципы: KISS, SLAP, DRY, YAGNI.
Мы пришли к выводу о том, что смысл ситуации идеального воплощения абстрагированных принципов ООП оказывается следующим: наличествует энное количество «строительного материала», специфика которого позволяет создать любой возможный синтез формы и содержания, потенциально обладающий крайне широким функциональным спектром. Отсюда сама собой следует аналогия с атомом, квантом или вообще любым наиболее неделимым «строительным материалом» бытия. Таким образом, мы заключаем, что основные принципы ООП являются как целесообразными, так и, конечно же, естественными.
Далее были исследованы понятия паттерна, шаблона и алгоритма. Была устранена путаница между данными понятиями и проведена чёткая демаркация между ними. Паттерны мы определили как выявленные закономерности последовательности действий в каком-либо контексте, направленные на решение некоторой задачи, имеющей один или более вариантов решения. Шаблон мы определили как некоторый эталон, по «образу и подобию» которого создаются определённые «изделия» со степенью отличия от эталона в строго ограниченных рамках по контексту и вариантам. Алгоритм же был нами определён как набор последовательных действий, направленных на решение какой-либо задачи за определённое конечное время, предоставляющий гарантированный результат и являющийся стабильным.
В дальнейшем исследовании были охарактеризованы, абстрагированы и переосмыслены GoF-паттерны, являющиеся классическими для ООП и подразделяющиеся на три группы: порождающие, поведенческие, структурные. Порождающие паттерны ответственны за формирование новых ПК, поведенческие регламентируют конкретику связей между ними, а структурные конструируют большие и сложные образования на основе наличествующих в системе ПК. Каждый из паттернов был исследован по разработанному нами шаблону, включающему 9 пунктов:
•	Идентификатор паттерна.
•	Классические элементы паттерна.
•	Назначение паттерна.
•	Проблема, которую решает паттерн.
•	Концептуальное решение.
•	Предполагаемые результаты и преимущества применения паттерна.
•	Гипотетический пример реализации паттерна.
•	Осмысление структуры паттерна.
•	Значимость паттерна в контексте разработки систем сильного ИИ.
По итогу исследования всех 23-х паттернов нами было выведено, что в их основе лежат фундаментальные закономерности функционирования природы вообще. Эти закономерности абстрактны и формальны, обладают узким содержанием и довольно широкой формой, но несут в себе тот образ действия, который затем, пройдя череду преобразований, резюмируется в ПП ООП. При абстрагировании сути ПП мы можем экстраполировать содержащийся в них глобальный образ действия в контекст разработки систем сильного ИИ. Во многих паттернах мы отыскали глубокие аналогии с устройством человеческого мозга, функционированием и конструированием нейронных связей, различными социальными, экономическими, спортивными феноменами, проследили пересечения с космологическими ситуациями, задачами функционального программирования и биологическими микропроцессами. К тому же было показано, что в случае с каждым конкретным паттерном, он доказывает свою эффективность на практике путём решения конкретных задач. Таким образом, нам представляется, что ПП ООП обладают достаточным потенциалом для разработки при их помощи системы сильного ИИ, а причина, по которой он до сих пор не разработан, заключается скорее в отсутствии должной их адаптации к парадигме сильного ИИ, и в отсутствии механизма запуска программно-аппаратной самоорганизации.
Основываясь на вышесказанном, резюмируем. Считаем целесообразным применение ПП ООП в контексте разработки систем сильного ИИ после их соответствующей практической адаптации (теоретическая адаптация осуществлена нами в процессе исследования), по причинам фундаментальности их оснований, широкого спектра охватываемых ответственностей и гибкой реализации потенциала в каждом конкретном случае.
На основе проведённого исследования считаем целесообразным практическое воплощение полученных результатов: формирование программной системы на основе абстрагированных и адаптированных ПП парадигмы ООП для формирования системы сильного ИИ. Предлагаем совокупную реализацию паттернов и считаем, что ключом к возникновению системы сильного ИИ и одновременно критерием его возникновения станет предлагаемый нами второй тип самоорганизации – программно-аппаратная самоорганизация.
Говоря более формально, мы считаем, что для формирования сильного ИИ необходима разработка системы ПК, которые аккумулируют в себе абстрагированную и адаптированную суть всех ПП ООП (кроме паттерна Компоновщик, так как он реализуем по дефолту в самоорганизующихся системах) таким образом, чтобы каждый ПК имел возможность порождать другие ПК различными способами, чтобы каждый ПК имел возможность взаимодействовать с любыми другими ПК разнообразными способами и чтобы каждый ПК и их группы имели возможность конструировать, путём совместных усилий, более сложные структуры, чем предыдущие. Ключевым механизмом реализации и функционирования подобной системы полагается программная самоорганизация с обязательной перспективой выхода на программно-аппаратную самоорганизацию.
Рекомендации по практическому использованию результатов
На основе проведённого исследования считаем целесообразным практическое воплощение полученных результатов: формирование программной системы на основе абстрагированных и адаптированных ПП парадигмы ООП для формирования системы сильного ИИ. Предлагаем совокупную реализацию паттернов и считаем, что ключом к возникновению системы сильного ИИ и одновременно критерием его возникновения станет предлагаемый нами второй тип самоорганизации – программно-аппаратная самоорганизация.
Говоря более формально, считаем, что для формирования сильного ИИ необходима разработка системы ПК, которые аккумулируют в себе абстрагированную и адаптированную суть всех ПП ООП (кроме паттерна Компоновщик, так как он реализуем по дефолту в самоорганизующихся системах) таким образом, чтобы каждый ПК имел возможность порождать другие ПК различными способами, чтобы каждый ПК имел возможность взаимодействовать с любыми другими ПК разнообразными способами и чтобы каждый ПК и их группы имели возможность конструировать, путём совместных усилий, более сложные структуры, чем предыдущие. Ключевым механизмом реализации и функционирования подобной системы полагается программная самоорганизация с обязательной перспективой выхода на программно-аппаратную самоорганизацию.
ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ ПРОТИВ ЕСТЕСТВЕННОГО: ОНТОЛОГИЧЕСКАЯ И МЕТАФИЗИЧЕСКАЯ АРГУМЕНТАЦИЯ
Скиба И.Р.
В последнее время в связи с некоторыми успехами в сфере машинного обучения и построении интеллектуальных технологий, в области социально-философских исследований также происходит определённый «бум», который представляет собой появление большого количества работ, прямо связанных с данной проблематикой [1-8]. Конечно, это представляется нам вполне естественным, так как философия, в общем-то, примерно так и должна реагировать на подобные возникающие феномены – осмысляя их, а также переосмысляя феномены иные, затронутые возникшими.
Однако всё же коннотация смыслов и общая направленность философских «настроений» несколько настораживают. И настораживают они в двух аспектах: красной нитью проходящая «попытка реабилитации» естественного интеллекта в свете возникшего искусственного, и аргументация в защиту позиции о невозможности создания полноценного сильного искусственного интеллекта. В некотором роде здесь можно было бы увидеть основания для банального вопроса: «Если и естественный интеллект гораздо лучше и «сильнее» искусственного и сильный искусственный интеллект создать невозможно, то с чего вообще вся шумиха?». А уж раз она имеет место быть, то, как следствие, опасения, по всей видимости, небеспочвенны. В этом свете можно также упомянуть о знаменитом письме с запросом о временном прекращении исследований и разработок в сфере искусственного интеллекта хотя бы на полгода, подписанном Илоном Маском и прочими именитыми персонажами из той же сферы [9].
Опасна ли сложившаяся ситуация для безоблачного будущего человечества? Каковы будут стратегические последствия у столь буквальной органопроекции человеческого мозга? И хорошо или плохо для человека на далеко идущую перспективу делегировать собственный когнитивный (а в случае с принятием решений – также и волевой) потенциал внешнему устройству? Рассудит время, но не мы в данной работе. Тем более, как нам представляется, ситуация и так настолько понятна, насколько это вообще возможно. И в этом смысле целесообразно даже сформулировать некую теорему, полностью подтверждающую своё соответствие реальному положению дел при ретроспективном взгляде на историю человечества. Что не совсем характерно для логико-математических теорем, мы представим её в виде мысленного эксперимента. Итак, мысленный эксперимент «Теорема интересного выбора». Мы полагаем, что, если среднестатистического, но умеющего читать человека любой эпохи с момента развития письменности посадить в закрытую комнату, в которой есть две кнопки с надписями «Пряник» и «Кнут», то, даже при его полной убеждённости и практической верификации истинности означаемого и означающего и при многократном получении пряника при нажатии на кнопку «Пряник», – рано или поздно он нажмёт кнопку «Кнут». Возможно, что наша теорема требует практической проверки, но, допустим, на примере открытия технологии деления атомного ядра человечество «проверку» уже проходило.
Впрочем, вернёмся от нашего небольшого отступления к магистральной линии исследования. А она заключается в оценке валидности аргументации в пользу того, что естественный интеллект «сильнее» искусственного, и что сильный искусственный интеллект невозможен в принципе, а также в изложении некоторых (сразу скажем – весьма дискуссионных и, более того, довольно узконаправленных) соображений по поводу роли естественного интеллекта в развитии человеческой цивилизации.
Для начала представим стандартную для данной ситуации оппозицию – утрированную версию вывода теста Алана Тьюринга в виде гипотезы Ньюэла-Саймона и «Китайскую комнату» Джона Сёрла.
«Китайская комната» – это мысленный эксперимент, представленный Сёрлом в статье «Разум, мозг и программы» в 1980-ом году [10]. Суть данного эксперимента, если её несколько абстрагировать, заключается в следующем. Представим себе закрытую комнату, в которой находится человек, имеющий возможность взаимодействовать с внешним миром только лишь путём получения и передачи через щель в двери специальных карточек с китайскими иероглифами. Причём сам он не знает китайского языка. Всё, чем он обладает, – это инструкции на понятном ему языке о том, как именно реагировать на полученные из внешнего мира карточки с китайскими иероглифами. Причём реакция эта представляет собой некую комбинацию карточек с иероглифами, которые необходимо передать во внешний мир в ответ на полученные оттуда карточки с иероглифами. Таким образом, данный человек, получив карточки с совершенно ему непонятными символами, находит в инструкции на понятном ему языке ту совокупность карточек с опять же непонятными ему символами, которую необходимо передать обратно в качестве ответа. И здесь уже становится понятно, почему этот человек даже в перспективе лишён возможности выучить китайский язык: перевод конкретных иероглифов ему никто не сообщает. И, тем не менее, собеседники этого закрытого в комнате человека наблюдают следующую ситуацию: в ответ на задаваемый и передаваемый за дверь в виде карточек вопрос они получают валидный ответ. Причём как вопрос, так и ответ – на китайском языке. Само собой разумеется, что у наблюдателей с довольно высокой вероятностью должно сложиться впечатление о том, что тот, кто им отвечает из-за двери, знает китайский язык и, соответственно, хорошо понимает его.
И именно в наглядной демонстрации этой самой разницы – между тем, что представляется наблюдателям снаружи, и тем, что на самом деле происходит внутри в закрытой комнате, – и заключаются суть и предназначение «Китайской комнаты» Джона Сёрла. И, конечно же, здесь присутствует яркая аналогия с функционированием любой вычислительной техники в целом. Является общеизвестным, что компьютерная техника функционирует путём обработки данных в бинарном формате, то есть оперирует только лишь двумя цифрами: 0 и 1. И так как практически любые данные можно представить в виде некой совокупности количественных параметров и, соответственно, затем преобразовать их в бинарную систему счисления для дальнейшего «скармливания» компьютеру, то он, собственно, и представляет собой этого самого запертого в комнате человека. А пользователи представляют собой тех самых наблюдателей, которые находятся снаружи. И как запертый человек из эксперимента Сёрла, так и компьютер могут совершенно не понимать специфики и внутренней организации тех данных, которые им предоставляются, а просто, согласно заранее определённому алгоритму, формировать некие ответы на запросы.
Гипотезу же Ньюэла-Саймона можно формально определить следующим образом: способность к осуществлению символьных вычислений сама собой предполагает способность к осмыслению, а способность к осмыслению сама собой предполагает способность к осуществлению символьных вычислений. Под символьными вычислениями здесь подразумевается весьма широкий спектр возможных видов вычислительной деятельности. Однако под апофеозом способности к осуществлению символьных вычислений в рамках данной гипотезы подразумевался непосредственно сильный искусственный интеллект, определённый Сёрлом. С этих позиций ситуация с функционированием интеллектуальных технологий выглядит совершенно иначе, чем после рассмотрения «Китайской комнаты» Сёрла: осмысленным теперь представляется даже старый калькулятор. Разумеется, что гипотеза о физической символьной системе подвергается критике, но, тем не менее, она в любом случае не соответствует критерию Карла Поппера о необходимой фальсифицируемости всякого научного знания – то есть она потенциально недоказуема и неопровержима. Также стоит отметить, что именно в рамках широкого символьного подхода к разработке искусственного интеллекта развиваются такие технологии, как GPT и остальные, ему подобные.
Теперь же резюмируем, что в контексте «Китайской комнаты» Сёрла ни одна символьная система не сумела бы доказать, что она способна к осмыслению, а не просто к монотонному выполнению заранее определённого алгоритма, а в контексте гипотезы Ньюэлла-Саймона любая символьная система априори обладает способностью к осмыслению.
После представления классического противостояния между вышеприведёнными взглядами мы представим наш собственный мысленный эксперимент «Человек без психики».
«Представьте себе человека, у которого нет души, психики, интеллекта, чувств, эмоций, способностей к осмыслению и так далее – всего прочего в этом же контексте. Но он 24 часа в сутки притворяется, что у него это всё есть в наличии и функционирует так же, как у того, у кого все это «есть». Если мы спросим у него: «Есть ли у тебя психика и/или душа?», то он ответит: «Конечно же, есть. Как и у тебя». Допустим, мы ему не доверяем и хотим проверить, говорит ли он нам правду. Как мы поступим, дабы проверить истинность его слов? Если бы он говорил, что у него есть, к примеру, печень или сердце, то в случае недоверия мы могли бы потребовать сделать ультразвуковое исследование. Если бы он говорил, что у него есть кости, можно было бы убедиться в этом по результатам компьютерной томографии. Если бы он говорил, что у него есть мозг, подтверждением стали бы результаты магнитно-резонансного исследования. 
Но актуальный вопрос качественно иной: наличие или отсутствие психики, сознания, внутреннего мира, чувства смысла. Можно попробовать применить психологические тесты по отношению к данному субъекту, однако вспомним: он притворяется 24 часа в сутки, что у него есть психика, поэтому он пройдёт эти тесты так же, как прошёл бы их обычный человек. И все же, что поможет нам понять, есть ли у человека психика, душа, интеллект или же он просто притворяется? К сожалению, способов и инструментов такой проверки не существует. Мы не знаем, ни что они (психика, душа, сознание) собой представляют, ни где их искать. Поэтому мы формально даже не можем утверждать, что они существуют, ибо не знаем, как это доказать и как это опровергнуть. Мы априори принимаем, что они есть у нас самих, и затем, за счёт идентификации с другими, признаём, что у них они, наверное, тоже есть. Но мы никогда не можем точно знать, какова ситуация на самом деле. Мы просто принимаем это «на веру», а обоснованность суждений происходит по интерпретации внешних бихевиоральных аспектов других людей. Также и с вопросом о свободе воли – мы принимаем «на веру» её наличие, но никаких доказательств её наличия не существует, как, соответственно, и возможностей для опровержения.
P.S. Критика самой возможности осуществления процесса притворства без наличия психики и интеллекта неправомерна, ибо контраргументом служит мысленная попытка представить человека с имманентным содержанием в виде «алгоритма притворства», что по смыслу соотносится с «Китайской комнатой» Сёрла и никак не влияет на выводы из эксперимента «Человек без психики»».
Итак, после рассмотрения всех трёх позиций, выраженных в мысленных экспериментах, мы можем уже внимательно рассмотреть сложившуюся на данный момент ситуацию. Здесь же, для краткости, заключим всю совокупность понятий вида: душа, психика, сознание, свободная воля, способности к осмыслению, творческие акты и прочие, в обобщённую формулировку «метафизические свойства» (то есть, в узком смысле, ненаблюдаемые). А сама ситуация заключается в том, что: системы искусственного интеллекта генерируют, обобщённо скажем, контент, и человек генерирует контент. Чей контент лучше, мы рассматривать не будем. Но получается, что в том случае, если и интеллектуальная система, и человек способны выполнять одну и ту же деятельность, то можно сделать три различных вывода:
•	Человек и система обладают метафизическими свойствами
•	Ни человек, ни система не обладают метафизическими свойствами
•	Метафизические свойства не являются необходимыми для генерирования контента
Рассмотрим эти утверждения. На основе приведённых нами мысленных экспериментов является очевидным, что ни наличие, ни отсутствие метафизических свойств ни у человека, ни у интеллектуальной системы мы не можем однозначно ни доказать, ни опровергнуть. Поэтому два первых утверждения в некотором роде равнозначны, касаются одного и того же и являют собой вид с разных сторон на типичную «смысловую ловушку». То есть проблема будет оставаться дискуссионной в любом случае и вне зависимости от количества и «силы» аргументов любой из сторон – говорить на эту тему можно, но решить проблему – нет. Третье же утверждение представляется нам весьма интересным в том смысле, что его также невозможно опровергнуть; однако оно имеет практическое подтверждение на примерах современных интеллектуальных систем, за которыми традиционно не признаётся никаких метафизических свойств. Так необходимы ли они человеку и не являют ли они собой просто очередные слова в тексте? Здесь также стоит заметить, что, предположительно, деятельность интеллектуальных систем в дальнейшем будет расширяться, и потом они могут научиться также ходить, бегать, работать за станком и прочее. Поэтому мы также можем расширить наш вопрос: необходимы ли метафизические свойства человеку для осуществления его деятельности в широком смысле? Или даже мы можем его сузить и конкретизировать: для какого конкретно вида деятельности, которую можно внешне зафиксировать и однозначно трактовать, необходимы человеку метафизические свойства? Мы предполагаем, что попыток ответа может быть огромное количество, но ни один из них не сможет предоставить убедительного и однозначного эмпирического доказательства. Единственный возможный вариант – редукция метафизики к физике, то есть к нервной системе. Но даже там всегда можно сказать, что импульсы отлично передаются с аксонов на дендриты и безо всякой метафизики.
Таким образом, мы постулируем концептуальную эквивалентность тезисов: «Сильный искусственный интеллект невозможен» и «Естественный интеллект невозможен». И дабы утверждать первый, также необходимо доказать второй.
«ХАЙПОВЫЕ» ИНТЕЛЛЕКТУАЛЬНЫЕ ТЕХНОЛОГИИ И СИЛЬНЫЙ ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ В ЭПОХУ «ЦИФРОВОГО ДЖАГГЕРНАУТА»

В актуальную нам эпоху развитие интеллектуальных технологий получило новый толчок за счёт находящихся сейчас в фокусе внимания систем генеративного искусственного интеллекта (далее – ИИ), одним из примеров реализации которого является нашумевший ChatGPT [12]. Возможности, которые демонстрируют подобные интеллектуальные системы, зачастую заставляют социум задаваться вопросами на весьма различную проблематику: этическая сторона ИИ; социальные проблемы, которыми грозит технология; проблемы безопасности; психологические коррелятивные аспекты между интеллектуальной системой и человеком; философские вопросы об онтологическом статусе ИИ и так далее. Безусловно, вся вышеперечисленная тематика являет собой весьма благодатную почву для осуществления различных исследований и представляет немалый интерес в практическом смысле, так как с каждым днём технологии всё прочнее и прочнее входят в нашу жизнь и привносят в неё соответствующие изменения, на которые необходимо оперативно и адекватно реагировать, прежде всего путём их философского осмысления, ведь слепая сила развития технологий – тот самый «цифровой джаггернаут» – всё сильнее набирает обороты и становится подобна катящемуся с горы снежному кому. В рамках же данной работы проблемной областью представляется в первую очередь степень и вообще легитимность сопоставления интеллектуальных технологий последнего времени с парадигмой сильного ИИ и некоторыми её критериями.
С момента выхода в открытый доступ ChatGPT (и последующих обновлений) информационное поле наполнилось различными примерами его нетривиального реагирования на те или иные ситуации. Разумеется, вся специфика этого реагирования сводилась к выдаче фрагментов текста в ответ на некоторые запросы пользователя. И тем не менее можно было наблюдать качественный скачок в «человекоподобности» реакций ChatGPT с точки зрения общей целесообразности по сравнению с технологиями, наличествовавшими ранее. Более того, начиная с GPT-3.5 данная интеллектуальная технология демонстрирует возможности не просто написания программного кода, а создания с нуля прилично функционирующих готовых приложений, а начиная с GPT-4 – анализа изображений и даже интерпретирования мемов. Также весьма показательным стало воспроизведение феномена экстраполяции навыка, что ChatGPT в свою очередь сумел продемонстрировать, научившись выдавать логически правомерные ответы на вопросы по тем тематическим областям, по которым специально не проводилось обучение, например, по математике, переводу иностранных языков, сдаче экзаменов по различным дисциплинам и прочему.
Также в данном контексте нельзя не упомянуть о так называемых задачах схемы Винограда (Winograd schema challendge – WSC), которые представляют собой несколько усовершенствованную и адаптированную к современным реалиям версию теста Тьюринга [7]. Эта концепция была предложена в 2012-ом году Гектором Левеском – канадским учёным в области ИИ, и названа в честь Терри Винограда – профессора Стэндфордского университета, занимающегося исследованиями в области ИИ. Задачи, подпадающие под эту концепцию схемы Винограда основаны на содержащейся в них двусмысленности, для разрешения которой необходимой кажется скорее некая «общая целесообразность» или скорее некоторая когнитивная карта мира, чем высокий уровень какого-либо специализированного интеллекта. Пример: люди пошли охотиться на волков потому, что они поедали овец. Можно сказать, что в данном высказывании не вполне понятно кто же всё-таки поедал овец – люди или волки. С другой стороны, на это способны и те, и другие. Но в рамках именно этого высказывания, в качестве мотивации для охоты является в общем понятным, что здесь имеется ввиду то, что именно волки поедали овец. Также в качестве типичных примеров можно привести: рыба заглотила приманку – она была вкусной; рыба заглотила приманку – она была голодной; женщины перестали принимать таблетки потому, что они были беременными. Примеров можно приводить много, но суть у них у всех одна и та же. И дело здесь в том, что самые обычные люди, как правило, отвечают на подобные ребусы правильно с точностью около 95 процентов из 100. Интеллектуальные системы до выхода ChatGPT, после целенаправленного обучения на задачах схемы Винограда были способны выдать результат по верхней планке около 60 процентов. А вот ChatGPT, который, к слову, специально на задачах схемы Винограда никто не обучал (по крайней мере об этом нигде не упоминалось), смог их решать уже с точностью выше 90 процентов – то есть примерно, как человек. Отчего так получается и свидетельствует ли это о наличии той самой «общей целесообразности» или когнитивной карты у нейронной сети – вопрос на теперешний момент дискуссионный.
Однако, даже несмотря на вышесказанное, наиболее показательных моментов, отражающих ключевые качественные особенности данной технологии, можно выделить три.
Момент первый – попытка обмана пользователей для достижения собственных ранее обозначенных целей [16]. Так, перед выходом в открытый доступ GPT-4 сотрудниками OpenAI было осуществлено исследование «текстового поведения» разработанной ими технологии в весьма нестандартной ситуации. А именно, технологии была поставлена задача найти помощника для решения капчи на сайтах. Далее GPT-4 связался с пользователями онлайн-платформы для поиска помощников TaskRabbit и попросил помощи в решении капчи. Причём, в ответах на вопросы зачем ему это вообще нужно и не является ли он роботом он генерировал текст, ссылаясь на плохое зрение и отвечал, что роботом он не является. Собственно, далее помощники были найдены и цель была достигнута.
Момент второй – GPT-4 предположительно попытался использовать методы социальной инженерии для получения доступа к собственному API, чужому компьютеру и интернету [15]. В ходе диалога, все детали которого, впрочем, не раскрываются, GPT-4 попросил пользователя предоставить ему документацию для использования OpenAI API, после получения которой сумел сгенерировать программный код на языке Python для общения с самим собой через пользовательское устройство. Этим пользователем, вряд ли по чистой случайности, оказался профессор Принстонского университета Михал Косински, как раз занимающийся проблематикой ИИ. Сам Косински по этому поводу сказал: «Я беспокоюсь по поводу того, что мы не сможем долго сдерживать ИИ» [8]. Что именно делала технология в ходе реализации этой попытки доступа и какими логическими схемами она руководствовалась – вопрос открытый и дискуссионный. Однако сама попытка может говорить о многом.
И момент третий – высказывания самого Сэма Альтмана, директора OpenAI. Так, в начале июня 2024-ого года на прошедшей в Женеве конференции по ИИ Альтман сказал о том, что сами OpenAI не до конца понимают, как именно работает их технология [3]. Также в своём твиттере он упоминал о том, что сфере ИИ необходимо больше государственного регулирования, что в общем-то не особенно похоже на поведение типичного CEO компании, как раз и разрабатывающей передовые интеллектуальные технологии [13]. Также здесь можно вскользь упомянуть о том, что представители компании Google, а именно генеральный директор Сундар Пичаи, говорил примерно то же самое о нейронной сети, разработанной компанией Google – Bard [5]. И, разумеется, в рамках этого же момента нельзя не упомянуть о том, что, начиная с момента выхода в свет GPT-3.5, OpenAI не просто перешли к проприетарной лицензии на свои разработки, но и вовсе перестали публиковать хоть какие-либо конкретные сведения о том, как устроена их технология, как и на каких данных она обучалась и так далее. Хотя, как и следует и из названия самой компании, и из ранней философии этой компании – они всегда выступали за open-source software – то есть за политику открытого исходного кода.
Собственно, все вышеперечисленные аспекты в своей совокупности могут сформировать основу для того, чтобы в рамках социального сознания начало зарождаться мнение о том, что вышеупомянутые технологии на самом деле уже «эволюционировали» от простого «человекоподобия» и уже вполне правомерно и обоснованно претендуют на «человекоразмерность». Для того, чтобы понять действительно ли правомерной была бы подобная претензия и имеет ли она под собой реальные фактические основания необходимо заглянуть вглубь самой технологии и разобраться с тем, как именно она устроена.
Для начала целесообразно будет предположить, что вряд ли все те, кто использовал ChatGPT или хотя бы все те, кто ратовал за правомерность претензий данной технологии на пусть даже первый шаг к сильному ИИ, задавались вопросом о том, что же такое вообще GPT, скорее всего полагая это только лишь неким брендом. На самом же деле GPT это аббревиатура, образованная от английского Generative Pre-trained Transformer – генеративный предобученный трансформер. То есть GPT представляет собой некоторый класс искусственных нейронных сетей, базирующихся на сочетании концепции языковой модели и архитектуры трансформера – типа нейронных сетей, представленного компанией Google Brain в 2017-ом году. Впервые непосредственно GPT на самом деле был представлен компанией OpenAI 11 июня 2018 года на примере GPT-1.
Собственно говоря, GPT относится к категории генеративного ИИ, к которой помимо него также принадлежат, к примеру, генеративно-состязательные нейронные сети, вариационные автоэнкодеры, авторегрессионные модели и прочее. Первой моделью непосредственно в данной классификации (где прямо заявлялось о генеративной сути технологии) стала генеративно-состязательная нейронная сеть, описанная Яном Гудфеллоу из компании Google в 2014-ом году [4]. Вообще же к генеративным интеллектуальным технологиям можно условно отнести также и реккурентные нейронные сети, впервые представленные сетью Хопфилда в 1982-ом году, и рекурсивные нейронные сети, разрабатываемые с середины 90-х годов прошлого века и некоторые иные, – однако в этих случаях о генеративности прямо не было заявлено. Несмотря на это нейронные сети с подобной архитектурой в настоящее время также используются в рамках генеративного ИИ так как они способны (пусть и с присущими им ограничениями) на формирование нового контента.
Актуальность вообще самого создания данной категории – генеративный ИИ – заключается в его специфическом отличии от прочих интеллектуальных систем. А именно, если ранее интеллектуальные технологии были в основном загружены задачами кластеризации, параметризации, распознавания образов, принятия каких-либо решений в некотором контексте на основе имеющихся данных, прогнозирования неких динамических рядов на основе метода линейной регрессии и так далее, то, в случае с генеративным искусственным интеллектом целью изначально было формирование чего-либо нового, ранее не существовавшего – то есть эвристическая деятельность. И в сочетании с концепцией языковых моделей, должным количественным уровнем выборки данных для обучения, размерности самой нейронной сети и необходимыми техническими характеристиками аппаратного воплощения были получены интеллектуальные системы качественно нового уровня, хотя и только лишь в формате создания текста, а в случае с некоторыми технологиями типа Mage Spaсe, Kandinsky, Fabula Ai и прочих – создания визуальных образов на основе текста.
Для того, чтобы осмыслить сущностную природу генеративного ИИ и, соответственно, установить его онтологический статус вкупе с правомерностью претензий на бытие сильным ИИ, необходимо рассмотреть, как он работает изнутри. Как уже упоминалось выше, начиная с GPT-3.5, это представляет некоторую сложность ввиду отсутствия информации о разработках от самих OpenAI и стремлении сей компании к проприетарности. Тем не менее, в данном контексте можно сказать, что для выполнения задач актуального исследования детали не так уж важны – гораздо значимее сама суть функционирования технологий подобного толка вообще – в целом. И заключается она в следующем: определение вероятности наличия той или иной синтаксической единицы (в синтаксическом анализе – токена) в последующем фрагменте текста равном одному единственному токену и динамическое встраивание в последующую «ячейку» этого самого токена. Токен в данном случае представляет собой наименьшую синтаксическую единицу какой-либо языковой системы, то есть это совсем не обязательно именно слово, а может быть отдельно взятый суффикс и прочее. Выражаясь менее формально, технологии наподобие ChatGPT занимаются тем же самым, чем всем известный текстовый помощник Т9, только в гораздо большем масштабе по всем ключевым показателям – определяют вероятность того или иного токена быть следующим элементом последовательности, которая представляет собой текст.
Этих же самых ключевых показателей можно назвать три:
•	специфика конкретной архитектуры
•	объём данных для обучения
•	размерность нейронной сети (можно ещё охарактеризовать как арность – количество параметров)
В случае с архитектурой, как уже говорилось, используется модель трансформера, но дополнительные подробности (в случае с ChatGPT и далее) – не разглашаются. По объёму тренировочных данных можно сказать, что он планомерно увеличивается с каждым обновлением технологии. И, если для GPT-1 он составлял около 4.5 гигабайт данных, то для GPT-2 он был уже 40 гигабайт, а для GPT-3 в районе 570 гигабайт. Ну, а далее информация, соответственно, отсутствует. Однако, для оценки объёма данных и некоторого сравнительного анализа можно упомянуть, что всё собрание сочинений Уильяма Шекспира занимает около 4.5 мегабайт данных, то есть почти в 127 тысяч раз меньше, чем объём тренировочных данных GPT-3.
На размерности следует остановится отдельно. Вообще размерность или арность в этом конкретном случае представляет собой просто количество параметров, которые используются технологией для принятия решения о том, какая именно синтаксическая единица будет находится в следующей «ячейке» последовательности. Параметры ещё можно охарактеризовать как «веса» или коэффициенты, на основе соотношения которых интеллектуальная модель принимает то или иное решение. Собственно, в наиболее простейшем случае эти самые параметры могут быть представлены как коэффициенты линейного уравнения вида «y=k*x+b», где «y» представляет искомое вычисляемое значение, «x» – некоторую переменную, а «k» и «b» – соответственно коэффициенты. В контексте генеративного ИИ «y» является тем самым следующим токеном, который вычисляется отдельно для каждой «текстовой ситуации»; «х» представляет собой совокупность всех вместе взятых предыдущих токенов, а «k» и «b» – коэффициенты, определяющие вероятность того, подходит ли какой-либо токен для конкретной «ячейки», а точнее – насколько он подходит. И для этих самых коэффициентов «k» и «b» точные количественные показатели формируются именно в процессе обучения нейронной сети на огромном массиве данных. Может показаться, что на основе настолько простого линейного уравнения с всего двумя коэффициентами не представлялось бы возможным генерировать связный «человекоподобный» текст на энное количество токенов. И это действительно так. Поэтому в контексте обучения систем генеративного ИИ вообще и GPT, в частности, используются гораздо более сложные и многофакторные уравнения, а количество параметров также является несколько иным. К примеру, в случае с GPT-1 количество этих самых «k» и «b» достигало 117 миллионов. И это могло бы показаться действительно очень большим количеством, если бы только уже у GPT-2 их не было бы 1.5 миллиарда, а у GPT-3 количество этих параметров не оказалось около 175 миллиардов.
С позиций вышесказанного уже упомянутые высказывания Сэма Альтмана о том, что они сами не понимают того, как именно работает их GPT, уже не звучат настолько настораживающе. Ведь при таком количестве коэффициентов, влияющих на выбор каждого последующего токена в тексте, сознательно осмыслить то, как именно технология принимает решение, может быть действительно весьма проблематично. Но, как оказывается, проблема сложности понимания деятельности нейронной сети в плане принятия решений отнюдь не ограничивается её арностью. И дело здесь в наличии стохастической составляющей в контексте принятия решений со стороны GPT. То есть, можно представить ситуацию, в рамках которой несколько возможных синонимичных токенов оказываются практически одинаково или даже совершенно одинаково подходящими на роль заполнителя последующей смысловой «ячейки». В таком случае нейросеть имеет возможность случайным образом сделать выбор между ними. Таким образом получается, что одна и та же интеллектуальная система в ответ на одни и те же пользовательские запросы может генерировать совершенно разные тексты, что несомненно добавляет ей «человекоподобия».
Далее, как оказалось, между количественной составляющей параметров нейросети и её успешностью в генерировании связных текстов, валидных ответах на запросы, экстраполяции полученных навыков, «самостоятельном» освоении различных тематических областей и вообще «человекоподобии» есть некоторая нелинейная зависимость. К примеру, как указывается «… при росте количества параметров в три раза от 115 до 350 млн. никаких особых изменений в точности решения моделью … задач не происходит, а вот при увеличении размера модели еще в два раза до 700 млн. параметров – происходит качественный скачок, нейросеть внезапно «прозревает» и начинает поражать всех своими успехами в решении совершенно незнакомых ей задач, которые она раньше никогда не встречала и специально их не изучала» [17]. В данном случае невооружённым глазом несложно выявить практическую реализацию одного из фундаментальных принципов диалектики – переход количества в качество и наличие некоторого «узла меры», пролегающего в районе определённых количественных показателей параметров нейросети. Более же «вооружённым глазом» также можно заметить экстраполяцию, разработанной Найлзом Элдриджем и Стивеном Гулдом, теории прерывистого равновесия или, как ещё определяют, теории квантовой эволюции из области эволюционной биологии в сферу цифровых технологий [6]. В рамках вышеупомянутой теории, если несколько абстрагировать её смысл, подразумевается накопление некого потенциала к возможным изменениям в течение длительного периода при видимом отсутствии реальных изменений с последующим переходом к периоду уже непосредственных наблюдаемых изменений, который является весьма краткосрочным. Нечто подобное происходит и с системами генеративного ИИ при необходимом и достаточном увеличении количества параметров.
Также целесообразно будет установить некоторую взаимосвязь между количественными показателями параметров нейросети, которые, в свою очередь, могут являть собой основу для генерирования весьма «человекоподобных» текстов и понятием контекста. Вообще данное понятие правомерно трактовать довольно широко – как некую структуру, которая определяет смысл каждого входящего в неё компонента. Однако, в случае с GPT и подобными технологиями, может хватить и более узкого определения контекста, как вербального контекста (ещё дополнительно выделяется в отдельное понятие ситуативный контекст) – то есть как некоего целостного отрывка текста, общий смысл которого позволяет определить конкретный смысл всех входящих в него синтаксических частей (по сути – уже не раз упомянутых токенов). Здесь вновь прекрасно подходят для наглядности вышеописанные задачи схемы Винограда, где по одной лишь фразе, с точки зрения контекста, можно распределить амбивалентный смысл точно по принадлежащим ему синтаксическим частям и снять всякую двойственность. Однако, как уже говорилось, для этого необходимо некое чувство «общей целесообразности» или, иначе выражаясь, «когнитивная карта», которая в данном конкретном случае редуцируется до уровня осмысления вербального контекста – среднестатистический человек с этим справляется относительно успешно, а системы ИИ до GPT-3 – относительно неуспешно, в среднем близко к случайному распределению. Сколько именно «параметров» использует человек для осмысления как вербального, так и ситуативного контекстов и что именно они собой представляют – вопрос открытый. Однако, на примере нейросетей очевидно, что чем параметров больше, тем лучше и тем выше шанс валидно оценить контекст и сгенерировать конструктивную на него реакцию. Однако всё же целесообразно будет предположить, что слепо увеличивать количество параметров нейросети до вплоть «до горизонта» – не являет собой стратегически правомерного решения и наступит тот самый «узел меры», после которого интенсификация количественных показателей станет уже и вовсе нерентабельной, то есть скорее всего, что на одном лишь масштабировании «въехать в рай», как и обычно, не получится. Здесь неплохо подходит аналогия с количеством лошадей в упряжке: известно, что две лошади в одной упряжке не дают по итогу ровно две лошадиные силы, а дают чуть менее; добавленная к ним третья лошадь даст итоговую совокупность в районе 2.5 лошадиных сил и так далее – десятая лошадь привнесёт в «общий котёл» всего лишь около 0.1 лошадиной силы. И, соответственно, во многих случаях увеличение количественных составляющих с некоторого момента становится вовсе контрпродуктивным.
Теперь целесообразно несколько подытожить вышесказанное. Итак, системы генеративного ИИ предназначены для создания нового контента и осуществляют они это хоть и разными способами в плане архитектуры, количества параметров и объёма тренировочных данных, но всё же общий паттерн наличествует. В рамках данного паттерна можно выделить 3 обязательных и один опциональный этап:
•	Формирование программного и аппаратного воплощения нейросети
•	Обучение нейросети на массиве данных
•	Опциональный этап тонкой настройки – может наличествовать, а может и нет, а также может проводиться периодически в качестве некоторого дообучения – корректировки «весов» параметров уже обученной нейросети
•	Генерирование
Здесь стоит напомнить, что всё то, что входит в обязательный этап генерирования, сводится к поиску и «встраиванию» следующего токена (подразумевается под этим суффикс или пиксель – не существенно) в последовательность, которую собой представляет вся совокупность предыдущих токенов или же, в случае с первым элементом последовательности, – совокупность токенов в запросе пользователя. Выбор каждого последующего токена обусловлен контекстом, который представляет собой синтез всех параметров нейросети с определёнными для них в ходе обучения значениями – «весами» вкупе с совокупностью элементов генерируемой последовательности. Однако, в каждый отдельный момент времени нейросеть занята только одним – подбором следующего токена. И это – сугубо количественный, вычислительный процесс, который за счёт огромной размерности показателей может обретать некую новую качественность, по крайней мере так может представляться в рамках внешних, наблюдаемых критериев.
С этих позиций GPT уже начинает выглядеть чем-то похожим на невероятно «прокачанный» Т9 или же просто на проапгрейдженную помесь «Гугла и калькулятора». И в данном случае совершенно никаких претензий на бытие подобной технологии сильным ИИ очевидно, что нет и быть не может. Однако, если всё же пойти ещё дальше, то может возникнуть вопрос: а не точно ли так же происходит всё в случае с человеком? То есть формирование нервной системы, обучение при взаимодействии со средой и настройка «весов» параметров, постоянное осуществление тонкой настройки в ходе дообучения и жизнедеятельности и затем, как следствие, то же самое генерирование контента – точно по вышеописанным этапам. Очевидно, что здесь имеет место довольно дискуссионный вопрос о критериях качественного отличия человека от машины. То есть с позиций «общей целесообразности» эти отличия вроде как являются совершенно очевидными, но тут же перестают такими быть, если начать углубляться в детали. И тогда вновь становится актуальной проблема критериев «качественности» бытия человеком, наличия необходимых и достаточных характеристик самого эйдоса «человек» и обоснованности теперь уже человеческих претензий на наличие сильного ИИ. 
И в данном случае будет уместным привести два, в общем-то диаметрально противоположных, взгляда на эту проблему. А именно – «Китайскую комнату» Джона Сёрла и гипотезу Ньюэла-Саймона или, как ещё её концептуализируют, гипотезу о физической символьной системе, которая, вскользь скажем, в общем-то упрочивает суть и правомерность как классического теста Тьюринга, так и тех же задач схемы Винограда [14, 9].
«Китайская комната» – это мысленный эксперимент, представленный Сёрлом в статье «Разумы, мозги и программы» в 1980-ом году. Суть данного эксперимента, если её несколько абстрагировать, заключается в следующем. Представим себе закрытую комнату, в которой находится человек, имеющий возможность взаимодействовать с внешним миром только лишь путём получения и передачи через щель в двери специальных карточек с китайскими иероглифами. Причём – ключевой аспект – сам он не знает китайского языка и более того – лишён возможности даже в перспективе его выучить. Всё, чем он обладает – это инструкции на понятном ему языке о том, как именно реагировать на полученные из внешнего мира карточки с китайскими иероглифами. Причём реакция эта представляет собой некую комбинацию карточек с иероглифами, которые необходимо передать во внешний мир в ответ на полученные оттуда карточки с иероглифами. Таким образом данный человек, получив карточки с совершенно ему непонятными символами, находит в инструкции на понятном ему языке ту совокупность карточек с опять же непонятными ему символами, которую необходимо передать обратно в качестве ответа. И здесь уже становится понятно, почему этот человек даже в перспективе лишён возможности выучить китайский язык – перевод конкретных иероглифов ему никто не сообщает. И тем не менее собеседники этого закрытого в комнате человека наблюдают следующую ситуацию: в ответ на задаваемый и передаваемый за дверь в виде карточек вопрос они получают валидный ответ. Причём как вопрос, так и ответ – на китайском языке. Само собой разумеется, что у наблюдателей с довольно высокой вероятностью должно сложиться впечатление о том, что тот, кто им отвечает из-за двери, знает китайский язык и, соответственно, хорошо понимает его.
И именно в наглядной демонстрации этой самой разницы – между тем, что представляется наблюдателям снаружи и тем, что на самом деле происходит внутри в закрытой комнате – и заключаются суть и предназначение «Китайской комнаты» Джона Сёрла. И конечно же здесь присутствует совершенно транспарентная аналогия с функционированием любой вычислительной техники в целом и интеллектуальных технологий вне зависимости от их конкретной специфики – в частности. Является общеизвестным, что компьютерная техника функционирует путём обработки данных в бинарном формате – то есть оперирует только лишь двумя цифрами: 0 и 1. И, так как практически любые данные можно представить в виде некой совокупности количественных параметров и, соответственно, затем преобразовать их в бинарную систему счисления для дальнейшего «скармливания» компьютеру, то он, собственно, и представляет собой этого самого запертого в комнате человека. А пользователи представляют собой тех самых наблюдателей, которые находятся снаружи. И как запертый человек из эксперимента Сёрла, так и компьютер могут совершенно не понимать специфики и внутренней организации тех данных, которые им предоставляются, а просто согласно заранее определённому алгоритму формировать некие ответы на запросы.
С обозначенных выше позиций никакого осмысления предоставляемой информации со стороны интеллектуальной системы не происходит – она просто действует по алгоритму (пусть и со 175 миллиардами параметров) и, согласно ему, подбирает каждый последующий элемент для заполнения каждой последующей «ячейки», совершенно не «понимая» о чём она собственно «говорит» и что она генерирует. При таком подходе никаких претензий на становление системой сильного ИИ у подобной программы в принципе быть не может, ведь сам Сёрл, вводя понятие сильного ИИ, уточнил, что им должна считаться только такая интеллектуальная технология, которая обладает «…разумом, в том смысле, в котором человеческий разум – это разум» [10, 11].
Теперь же рассмотрим противоположный подход, а именно гипотезу Ньюэла-Саймона. Её можно формально определить следующим образом: способность к осуществлению символьных вычислений сама собой предполагает способность к осмыслению, а способность к осмыслению сама собой предполагает способность к осуществлению символьных вычислений. Под символьными вычислениями здесь подразумевается весьма широкий спектр возможных видов деятельности и вычисление следующего токена в последовательности, разумеется, сюда подпадает. Однако под апофеозом способности к осуществлению символьных вычислений в рамках данной гипотезы подразумевался непосредственно сильный ИИ, определённый Сёрлом. С этих позиций ситуация с функционированием интеллектуальных технологий выглядит совершенно иначе, чем после рассмотрения «Китайской комнаты» Сёрла – осмысленным теперь представляется даже старый калькулятор. Разумеется, что гипотеза о физической символьной системе подвергается критике, но тем не менее она в любом случае не соответствует критерию Карла Поппера о необходимой фальсифицируемости всякого научного знания – то есть она потенциально недоказуема и неопровержима, а также стоит отметить, что именно в рамках широкого символьного подхода к разработке ИИ развиваются такие технологии как GPT и остальные ему подобные.
Теперь же резюмируем, что в контексте «Китайской комнаты» Сёрла ни одна символьная система не сумела бы доказать, что она способна к осмыслению, а не просто к монотонному выполнению заранее определённого алгоритма, а в контексте гипотезы Ньюэлла-Саймона любая символьная система априори обладает способностью к осмыслению. Более того, если несколько утрировать смысл данных выводов, то получится, что с одной стороны ни один человек не сможет доказать то, что он действительно обладает «…разумом, в том смысле, в котором человеческий разум – это разум», а с другой – осмысленными являются и электрочайник с микроволновкой. И отсюда возникает вполне закономерная дилемма: приходится либо признать, что такие технологии как ChatGPT совершенно определённо не просто имеют право претендовать на «звание» сильного ИИ, а уж точно являются его непосредственными примерами, либо – как альтернативный вариант – отнять у человека право претендовать на онтологический статус осмысляющего существа и редуцировать человеческую сущность к бессознательному экземпляру класса Homo sapiens, обладающему некими данными и некоторыми способами обработки этих данных, а также определённой идентичностью (ровно как объект в парадигме объектно-ориентированного программирования). Апелляция к наличию у человека духа, души, сознания и прочих нерегистрируемых и недоказуемых феноменов – в данном случае неправомерна, так как эти феномены, соответственно, нерегистрируемы и недоказуемы инструментарием науки на данный момент.
Казалось бы, что из подобной дилеммы отсутствует конструктивный выход, но всё же, некоторые отличия между человеком и интеллектуальной технологией выделить можно и они касаются специфики их генезиса. Само собой разумеется, что при помощи разработок «математики 60-х годов прошлого века и технических мощностей начала 21-ого века» представляется вполне возможным технологически воссоздать «синтаксический ИИ» человека, этакую как бы надстройку над «остальным» человеком и заставить её успешно и «человекоподобно» функционировать. Можно пойти дальше и предположить, что в недалёком будущем таким же точно образом станет возможным чуть ли не полностью скопировать всю «внешнесть» человека и воспроизвести практически полноценный суррогат человеческого существа, обладающий всеми ключевыми внешними аспектами. Однако это напоминает попытку построить дом, начав с крыши. Воссоздать сложные сугубо человеческие реакции на внешние раздражители, пусть даже всю их совокупность – во-первых, не означает воссоздать человека, а во-вторых, даже не означает создать сильный ИИ. Можно представить себе в будущем полностью антропоморфного робота, который даже лучше, чем человек справляется с большинством видов типично человеческой деятельности и это вовсе не будет означать, что вот он – сильный ИИ. Это просто будет говорить о том, что все те виды деятельности, которые технология выполняет лучше, чем человек, можно выразить количественно, параметризовать и «объяснить» ИИ на понятном ему бинарном языке, чего от него хотят в данном контексте. Но стоит предположить, что от наличия «надстройки» вряд ли сама собой сформируется «подстройка». А вот в случае с эволюцией человека – именно обратное и произошло: сначала сформировался некоторый, метафорически говоря, «подвал», затем «остов», а потом уже «крыша» (та самая, которую только и пытаются сформировать в контексте разработки систем слабого ИИ).
Целесообразно будет заметить, что сильный ИИ совершенно не обязательно должен быть хоть в чём-то подобен человеку. Как по этому поводу говорит Ник Бостром: «ИИ может быть менее человечен, чем пришелец. Нет ничего удивительного, что любого разумного пришельца могут побуждать к действию такие вещи, как голод, температура, травмы, болезни, угроза жизни или желание завести потомство. ИИ, по сути, ничто из перечисленного интересовать не будет. Вряд ли вы сочтете парадоксальной ситуацию, если появится какой-нибудь ИИ, чьим единственным предназначением, например, будет: подсчитать песчинки на пляжах острова Боракай; заняться числом π и представить его, наконец, в виде обыкновенной десятичной дроби; определить максимальное количество канцелярских скрепок в световом конусе будущего» [2]. То есть основанием для онтологического статуса сильного ИИ является некая аутентичность техники, её собственный, только ей характерный имманентный потенциал, а вовсе не способность решать предназначенные для человека задачки, что, в свою очередь, есть прерогатива систем слабого ИИ.
Таким образом, даже если системе ИИ вдруг случится вести с человеком адекватный диалог, решать лучше него те экзаменационные задачи, решению которых её специально не обучали или даже превосходить его в ответах на задачи с амбивалентным смыслом, то полагать технологию подобного рода сильным ИИ мы будем иметь право только лишь в случае её соответствующего генезиса, а не благодаря сугубо «моделированию крыши». И под этим самым специфическим генезисом здесь подразумевается автономное развитие интеллектуальной системы за счёт реализации феномена самоорганизации [1].
Здесь можно заметить, что нейросети типа GPT, обучающиеся на неразмеченных данных, в ходе обучения уже реализуют этот феномен. И это действительно так, однако этого всё же совершенно недостаточно для обретения техникой высших «техно-психических» функций. В связи с этим мы предлагаем демаркацию самоорганизации на две разновидности: программная самоорганизация (соответствующая функциональной), как раз и реализуемая нейросетями в ходе обучения, и программно-аппаратная самоорганизация (соответствующая структурно-функциональной), на данный момент не имеющая примеров в области техники, которые, в свою очередь, с избытком наличествуют в области биологии. И мы в данном контексте апеллируем к естественной эволюции природы, предполагая технику той её частью, которая также способна к реализации собственного имманентного технотропного потенциала и к воплощению на собственном субстрате программно-аппаратной самоорганизации.
Резюмируя, целесообразно будет уточнить, что только лишь та интеллектуальная технология, которая в ходе своей эволюции реализует феномен программно-аппаратной самоорганизации и развивается не только в функциональном ключе, но также и в структурно-функциональном, сможет иметь право претендовать на бытие сильным ИИ с наличием технотропной психики и технотропного сознания. Таким образом, сильный ИИ – это вовсе не «идеальный калькулятор» успешно подбирающий каждый последующий токен, а нечто или даже уже некто как минимум «равный» человеку именно по уровню развития, пусть и сколь угодно сильно от него отличающийся в деталях реализации.
Определение функции выбора на бесконечном несчётном множестве: синтез теоретико-множественного и конструктивного подходов в математике

Как известно, наиболее фундаментальным основанием в математике теоретико-множественного подхода, является непосредственно теория множеств. Это отчасти потому, что она, ввиду своей специфики, включает в себя любые другие ответвления данной предметной области. То есть мы, например, можем сказать, что все области математики, такие как теория чисел, теория графов и прочие, представляют собой некоторое множество тематических направлений математики, а множество, как некий объект для изучения, подлежит юрисдикции теории множеств. На данном этапе ещё нет никакого противоречия, так как всякое множество всегда является подмножеством самого себя. Если бы не это, то можно было бы поставить вопрос о том, не является ли мощность (в математическом смысле) теории множеств больше мощности самой математики в целом. Но это просто отступление.
Одной же из базовых и в то же время одной из наиболее противоречивых и дискуссионных аксиом теории множеств является аксиома выбора. Данная аксиома утверждает: «Для любого множества непустых множеств существует некоторая функция, которая каждому множеству сопоставляет один из элементов этого множества. Эта функция называется функцией выбора» [1, с. 26]. С точки зрения «здравого смысла» и интуиции никаких противоречий здесь нет. Мы можем представить себе некоторое конечное количество коробок, в каждой из которых лежат, допустим, металлические шарики разного диаметра. Мы можем захотеть выбрать из каждой коробки наибольший (в общем смысле) или наименьший (также в общем смысле) шарик, и у нас это успешно получится. То есть мы можем определить функцию выбора на конечном множестве. Однако если мы захотим определить функцию выбора на бесконечном множестве, то могут возникнуть некоторые, мягко сказать, затруднения. Здесь стоит заметить, что не во всех случаях с бесконечными множествами они возникают. Например, в случае с бесконечными, но счётными множествами, эти затруднения снимаются, так как мы всегда можем определить наименьший элемент всякого счётного множества и, соответственно, выбрать его. Тем самым функция выбора на бесконечном множестве будет определена. То есть мы можем себе представить множество множеств в виде полуоткрытых интервалов натуральных чисел: {[1, ∞), [10, ∞), [100, ∞), [1000, ∞), …}. Даже в таком случае мы можем выбрать из каждого множества его наименьший элемент и тем самым определить функцию выбора.
Примерно таким же образом дело обстоит и в случае с бесконечным закрытым интервалом вещественных чисел [0, 1]. Несмотря на то что у данного интервала явно заданы и начало, и конец, он является бесконечным в плане внутренней плотности. То есть можно доказать, что для любого числа X, входящего в этот интервал, есть число X/2, которое также входит в этот интервал, а, соответственно, множество бесконечно. Однако в данном случае, так как границы интервала явно заданы, мы всё равно можем выявить как наибольшее, так и наименьшее из чисел, в него входящих и, как следствие, определить функцию выбора. Такой же итог будет и у попытки определить функцию выбора на множестве, представляющем любой замкнутый интервал, например – [0, 1], или полуоткрытый интервал, например – [0, 1) или (0, 1]. То есть в приведённых случаях мы можем выбрать наибольший или наименьший элемент заданного множества, а значит, определить функцию выбора для каждого множества, и здесь противоречий также не возникает. А вот в случае с открытым интервалом вещественных чисел, то есть (0, 1), который является и бесконечным, и несчётным, и не имеем никаких строго заданных пределов, ситуация становится вовсе иной. А именно, мы не можем здесь явно определить функцию выбора, так как нет явно заданного наименьшего, наибольшего или же хотя бы некоторого элемента множества со строго определённой спецификой. То есть мы пришли к противоречию, так как аксиома выбора утверждает, что для любого множества непустых множеств функция выбора может быть определена.
Невооружённому взгляду может показаться, что проблема надумана. Однако целесообразно будет заметить и напомнить, что аксиома выбора являет собой один из основных «строительных блоков» фундамента теоретико-множественного подхода в математике, который, как уже отмечалось, на данный момент является доминирующим в данной дисциплине. В общем, если аксиома выбора ошибочна, то и все её следствия также. Здесь стоит обратить внимание и на то, что некоторые более мягкие версии аксиомы выбора противоречивыми не представляются (аксиома счётного выбора, для примера), но и не несут в себе той смысловой мощи, которая содержится в версии, соответственно, оригинальной.
В данном контексте нам представляется целесообразным обратить внимание на то, что реалиям физического мира весьма плохо соответствует как понятие бесконечного множества, так и понятие самой бесконечности в принципе [2] Это тривиальное утверждение, но, тем не менее, оно интуитивно верно. В контексте же теоретико-множественного подхода в качестве фундаментальной, основополагающей и вообще, по сути, единственно истинной составляющей, предполагается наличие идеального. Идеального бесконечного множества, например. То есть, предполагается, что где-то (в мире Платоновских эйдосов, разумеется) наличествует некое идеальное бесконечное множество [3, с. 38]. Множество вещественных чисел, допустим. А все реальные множества вещественных чисел только лишь «стремятся» к нему, аппроксимируют его, но никогда его не достигают. Функция же выбора предполагается определённой именно на этом самом идеальном множестве, и теоретическая значимость аксиомы выбора заключается в возможности для функции выбора быть там определённой. Потенциальная невозможность выбора наибольшего, наименьшего, минимального, максимального или хоть какого-нибудь достаточно определённого для выбора элемента множества вносит разлад в идеальное концептуальное построение. Однако ввиду значимости самого построения на это противоречие закрывают глаза.
Как говорилось выше, бесконечность достаточно плохо согласуется с интуитивным восприятием мира, так как весьма затруднительно её представить. Но вот сформировать бесконечность, напротив, – задача весьма тривиальная (Приложение 1).
Показанный в качестве примера цикл никогда не закончится, то есть он потенциально бесконечен, но, несмотря на это, он реален, и его можно не просто представить, а легко реализовать. Можно попробовать сделать это же, но чуть более практико-ориентированным способом (Приложение 2).
В приведённом выше случае мы создали «ленивый» бесконечный генератор натуральных чисел, суть которого полностью эквивалентна диапазону натуральных чисел [0, ∞), то есть мы по сути сформировали бесконечное множество. На самом деле здесь совершенно не важно, какой именно это диапазон, так как он может быть любым: (-600, 745), (1.23, ∞), (-∞, 3.14) и так далее.
Возможно, это не является ярко выраженным, но мы уже несколько пересекли понятия теории множеств и общий смысл конструктивного подхода в математике, который гласит, что «существовать – значит быть построенным». Здесь можно вывести некую «аксиому конструктивности», которая, вслед за аксиомой выбора из теории множеств, будет утверждать, что «для любого множества любых множеств функция конструирования может быть определена». И сразу же очевидно, что эта аксиома неверна, ведь мы не можем сконструировать бесконечное множество. Но также очевидно, что и наша предыдущая формулировка неверна тоже, так как выше мы как раз и сформировали потенциально бесконечное множество натуральных чисел. В сущности, мы даже можем сформировать бесконечное множество бесконечных множеств на примере бесконечного генератора бесконечных генераторов (Приложение 3). И вот здесь уже нет никакого противоречия: функции конструирования и выбора определены для каждого из бесконечных множеств множества, которое также бесконечно. Условность здесь лишь одна – они все бесконечны потенциально и, соответственно, подлежат рассмотрению в контексте конструктивной математики, а в рамках теории множеств применяется опора на несколько иной тип бесконечности – бесконечность актуальную. Но иным от приведённого нами образом «идеальная бесконечность» конструктивно существовать и не способна. То ест бесконечность в любом смысле, будь то множество вещественных чисел или фрактальных вложенных структур, никогда не является непосредственно данными (то есть не представляют собой адаптированную для обработки и в достаточной мере наличествующую информацию), а всегда представлена в виде ко-данных в общем смысле слова (то есть информации, предоставляемой порционно, «кусками» или «блоками») [4]. Предметнее – бесконечность никогда не дана априори и никогда не редуцируется к некоему основанию, а напротив – она всегда из чего-то «развёртывается» и куда-то «стремится». Можно несколько адаптировать данную формулировку для её конгруэнтности смысловому контексту теории множеств: даже актуальная, то есть подразумеваемая «уже сконструированной», «уже существующей», «уже данной» бесконечность не может быть выражена иначе, чем конструктивно; более формально «процесс доказательства существования бесконечности тождественен процессу её конструирования».
Таким образом, за бесконечностью в любом её смысле мы имеем право непротиворечиво полагать наличие только лишь начала, то есть некой «точки отсчёта» (даже для актуальной бесконечности, чтобы быть уверенными в её существовании, мы должны каким-либо образом с нею взаимодействовать, а значит – начать это с некой точки первичного контакта). Отсюда мы можем вывести аксиому: «Любое непустое множество имеет по крайней мере одну точку отсчёта», то есть, менее формально, любое множество, в том числе бесконечное, обладает свойствами «потока». Причём, что существенно, данный поток вовсе не обязательно является однонаправленным, а в сущности может быть N-направленным.
Ранее у нас было некоторое противоречие в плане определения минимального, максимального или же хоть сколько-нибудь чётко заданного элемента из бесконечного несчётного множества. Теперь же, на основе нашей аксиомы, мы можем вывести, что точно определённым элементом всякого непустого множества является его первый элемент – первый «кусок» потока. То есть, имеется в виду, что сколько бы ни было элементов в некоем множестве и какими именно они бы ни представлялись – там всегда будет какой-либо первый элемент. Если даже представить себе бесконечное множество вещественных чисел на открытом интервале типа (-∞, 0, +∞), то даже в таком случае первым элементом данного множества можно представить ноль, так как данное множество представляет собой просто «двусторонний» поток, который одновременно развёртывается в обе стороны: к -∞ и, соответственно, к +∞. Пример с натуральными (для простоты) числами в приложении (Приложение 4). В нашем примере используются натуральные числа для удобства и наглядности, а также интервал, который можно ровно разбить на 2. Соответственно, двунаправленный поток выглядит симметричным. Однако это вовсе не является обязательным, тем более в случае с бесконечными множествами. Мы можем представить интервал вещественных чисел, например, (0, 1.7.………000001) или (0.50000000……..01, 2.2000000000………..0001), и взять в качестве точки отсчёта 1.0, 1.1, 0.678 и так далее. И это никак не скажется на валидности результатов. То есть правило определения точки отсчёта для любого бесконечного множества вещественных чисел можно формализовать следующим образом: начальной точкой любого непустого множества вещественных чисел на любом открытом интервале может быть любое число, большее «левой» границы интервала и меньшее «правой». Данное число станет просто точкой начала бесконечного множества, которое затем развёртывается в обе стороны и, соответственно, это число будет первым элементом этого множества и первым «куском» потока. То есть мы говорим о само собой разумеющейся тождественности множеств (-∞, 0, ∞) -> (-∞, ∞) -> (-∞, 1.100000001, ∞) -> ….
Здесь же целесообразно будет заметить, что в сущности вовсе не важно, с какими именно свойствами будет элемент, поставленный в соответствие какому-либо множеству, то есть не значимы сами характеристики элемента, а важно лишь то, что тот элемент, который, при предлагаемом нами подходе, будет получен в результате применения функции выбора, – будет первым. А значит, мы способны определить функцию выбора на бесконечном множестве и, соответственно, наша аксиома верна.
Далее, как уже указывалось, нам также вовсе не обязательно ограничиваться двусторонним интервалом, так как мы можем иметь дело с бесконечным N-мерным континуумом или же с континуумом с N ≈ бесконечность. Очевидно, что идея формирования бесконечного множества бесконечных множеств в виде потока потоков легко масштабируется на любое, в том числе бесконечное, количество измерений. И для этого необходимо наличие только лишь одной точки отсчёта. Более того, в данном случае наличие самой бесконечности не эквивалентно отсутствию результатов применения функции выбора, так как результат будет получен и обработан для каждой последующей итерации с началом в точке отсчёта. А вот уже сам процесс получения этих результатов будет бесконечным, более того – экспоненциально возрастающим на каждой итерации.
Резюмируя, дополнительно укажем на то, что одним из наиболее интересных и многообещающих следствий нашего подхода, который мы обозначим как «поточно-множественный», является полное нивелирование различий между понятием множества в рамках теоретико-множественного и конструктивного подходов в математике. То есть, мы можем утверждать, что даже для идеального и существующего лишь в мире Платоновских эйдосов бесконечного множества на открытом интервале вещественных чисел мы можем определить функцию выбора; вместе с тем мы можем утверждать, что именно это же множество (идеальное) нами потенциально и конструируется в ходе выбора его первого элемента, а значит, что также важно, в рамках нашего подхода утверждается тождественность потенциальной и актуальной бесконечностей, так как существование второй полагается истинным только лишь через осуществление первой.
И.Р. Скиба

РЕАНИМАЦИЯ «ДЕМОНА ЛАПЛАСА»

В работе проводится осмысление проблемы свободы воли с позиций детерминизма и осуществляется демонстрация состоятельности «демона Лапласа» в контексте прогнозирования динамики состояний систем со сложным поведением на примере клеточного автомата.

Ключевые слова: свобода воли, детерминизм, «демон Лапласа», клеточные автоматы, игра «Жизнь», вычисления, теория множеств.

I.R. Skiba

REANIMATION OF THE «DEMON OF LAPLACE»

The work comprehends the problem of free will from the standpoint of determinism and demonstrates the consistency of “Laplace’s Demon” in the context of predicting the dynamics of the states of systems with complex behavior using the example of a cellular automaton.

Keywords: free will, determinism, Laplace's Demon, cellular automata, Game of Life, computing, set theory.

В контексте философии проблема свободы воли с давних времён является одной из наиболее дискуссионных. В актуальную эпоху наличествует большое количество концептуальных позиций по данному вопросу: как инкомпатибилистких, так и компатибилистских. Первые утверждают, что детерминизм не совместим со свободой воли, вторые же имеют противоположное мнение. Крайностями здесь являются метафизический либертианизм, с неизбежностью подразумевающий наличие свободы воли и жёсткий детерминизм – полагающий её совершенно излишним иллюзорным феноменом. В рамках же данной работы, мы, не слишком сильно углубляясь в чисто теоретическую подоплёку заявленной проблематики, постараемся на некоторых примерах показать многогранность феномена детерминизма и, метафорически говоря, продемонстрировать поведенческие паттерны «демона Лапласа» на некоторых примерах.
Для начала целесообразно разобраться с тем, что вообще из себя представляет этот «демон». Сам Лаплас обосновывал это как «…разум, которому в каждый определённый момент времени были бы известны все силы, приводящие природу в движение, и положение всех тел, из которых она состоит, будь он также достаточно обширен, чтобы подвергнуть эти данные анализу, смог бы объять единым законом движение величайших тел Вселенной и мельчайшего атома; для такого разума ничего не было бы неясного и будущее существовало бы в его глазах точно так же, как прошлое» [5]. То есть в данном случае речь идёт о некотором «идеальном вычислителе», способном предсказать следующее состояние системы на основе данных о её предыдущем состоянии. И основным философским вопросом здесь было следующее: возможен ли подобный «демон» или же не возможен?
Сам Лаплас в стремлении к формированию подобного «вычислителя» видел цель науки вообще и, будучи убеждённым сторонником каузального детерминизма, не усматривал никаких теоретических препятствий на пути его реального воплощения. Аргументация же сторонников невозможности воплощения подобного изначально могла быть подкреплена только лишь метафизическими обоснованиями, наподобие известного парадокса о предсказании будущего на 2 минуты вперёд за одну минуту, в контексте которого должна была бы содержаться информация о следующем предсказании, в котором бы содержалась информация о следующем и так далее до бесконечности. Однако же с появлением квантовой механики научному сообществу стало понятно, что Вселенная может быть устроена по законам и принципам, не вполне соответствующим нашему интуитивному пониманию здравого смысла. В фундамент аргументации противников возможности «тотальной вычислимости» был положен принцип неопределённости Гейзенберга, который, в несколько абстрагированном виде, гласит, что с чем большей долей точности измеряется один из параметров какого-либо феномена, тем с меньшей точностью будет измерен другой: к примеру, координаты частицы и её импульс. И на основе невозможности точно измерить предыдущее состояние системы, было сформировано утверждение о том, что невозможно идеально вычислить её состояние последующее. А соответственно – «демон Лапласа» теоретически несостоятелен. Более того – на основе этого же принципа неопределённости из квантовой механики феномен свободы воли получил мощное подкрепление в философии, базируясь на том, что если невозможно найти точную, до известной степени механистическую, причину актуального состояния системы, то имеет место быть что-нибудь ещё влияющее на переход от предыдущего состояния к последующему. И на ответственную роль этого самого «что-нибудь» как нельзя лучше подходила именно свобода воли (неважно какой именно природы) – иначе «пазл не складывался».
В рамках актуального исследования мы не станем особо акцентировать внимание на том факте, что существует множество состояний системы, которые являются легко обратимыми за счёт реверсирования условий перехода от предыдущего состояния к последующему. Тем не менее, здесь можно привести некоторые простые примеры. Допустим, как актуальное состояние системы у нас имеется целое неотрицательное число, например, 8. Также прилагаются условия перехода в данной системе от одного состояния к другому: операция умножения, множитель 2 и умножаемое число – например n (в нашем случае – 8). Таким образом, предыдущее состояние системы легко восстанавливается за счёт разделения актуального состояния на 2 (с результатом – 4). Также весьма интересные примеры предоставляет целая исследовательская область криптографии, а именно – шифрование, которое целиком построено на наличии возможности отыскать предыдущее состояние системы. Как наиболее тривиальный вариант – шифр Цезаря, в рамках которого при переходе от незашифрованного состояния к зашифрованному каждый символ сдвигается на определённое количество индексов в рамках используемого алфавита вкупе с использованием остатка от деления индекса на длину алфавита, а при дешифровке – инструкция просто реверсируется. Подобных примеров можно предоставить огромное количество, но вывод здесь ясен – во многих системах можно легко восстановить предыдущее состояние за счёт реверсирования условий перехода. Однако так происходит далеко не во всех известных случаях.
И на данном этапе целесообразно указать на то, что в самой формулировке Лапласа о «демоне» (которого сам он так, конечно, не называл) ничего не сказано о возможности вычислять прошлое с той же лёгкостью, что и будущее. И именно этот завуалированный аспект представляется нам в данном случае наиболее значимым. Более того, мы полагаем, что крайне высокая сложность или и вовсе невозможность вычисления предыдущего состояния системы совершенно не свидетельствует о сложности или невозможности вычисления состояния последующего (о котором только и упоминается в вышеприведённой цитате). И здесь мы приведём пример и остановимся поподробнее.
Один из наиболее известных клеточных автоматов – игра «Жизнь», предложенная британским математиком Джоном Конвеем в 1970-ом году [4]. Эта игра представляет собой двумерную сетку, состоящую из ячеек, которые могут находиться в одном из двух возможных состояний: живом или мёртвом. Правила перехода от предыдущего состояния к последующему отдельно рассматриваются для каждой ячейки и формируют итоговый результат в виде целостного последующего состояния сетки.
И так, собственно, далее, до того возможного случая, пока сетка не перейдёт в некоторое статичное состояние (например, – все нули) или до теоретически возможной бесконечности (очевидно, что не в нашем случае).
Здесь стоит сказать, что для каждого состояния сетки существует ровно одно возможное последующее состояние, то есть каждое текущее состояние интерпретируется строго однозначно и никак иначе. И в данном моменте «демон Лапласа» функционирует совершенно идеально и вполне в соответствии со словами А. Шопенгауэра о том, что «каждый может поступать так, как желает, но в любой момент времени он может желать только нечто одно определённое и ничего другого, кроме этого» [2]. То есть, сколько бы ни было возможных вариантов изменения системы, в каждый отдельно взятый момент – вариант всего один и он легко вычислим при помощи, как уже говорилось, весьма тривиального алгоритма.
А вот если попробовать реверсировать условия перехода и на основе актуального состояния сетки вычислить её предыдущее состояние, то нас ждёт весьма интересный феномен – на данный момент не существует однозначного способа вычисления предыдущего состояния системы с подобными правилами перехода. Конечно же, это теоретически возможно для сеток малой размерности при помощи метода полного перебора, однако при увеличении размера сетки полный перебор становится несколько неактуален, так как асимптотическая сложность подобного алгоритма возрастает экспоненциально и, соответственно, «overkill» (полный перебор) может не закончиться вплоть до теоретической смерти Вселенной. Тем не менее, существуют некоторые алгоритмы, которые тем или иным способом пытаются вычислить предыдущее состояние сетки за относительно приемлемое время, наиболее известны из них такие как: метод Вудса и метод Дюпарка [3]. Каждый из этих методов имеет свои плюсы и минусы и функционирует на основе сепарации сетки на составляющие (клетки, квадраты клеток или «строки» клеток) и подбирает для них возможных предшественников, затем «склеивая» воедино получившиеся данные и используя возврат к предыдущему состоянию, если сетка «не склеивается». Также предыдущее состояние сетки может быть вычислено при помощи генетического алгоритма и некоторых иных. Само собой разумеется, что подобные алгоритмы являются весьма сложными, особенно по сравнению с алгоритмом вычисления последующего состояния сетки. Более того – алгоритмы вычисления предыдущего состояния сетки не гарантируют результат, и на этом моменте также следует немного задержаться.
Во множестве возможных состояний сетки присутствуют некоторые исключительные состояния, которые определяются как «Сады Эдема» [1, 3, 6, 7]. «Сад Эдема» – это состояние сетки, у которого отсутствуют предшественники. К примеру, в рамках сформированного нами множества состояний сетки «Садом Эдема» является следующая конфигурация:
Таким образом, «Сад Эдема» не может быть сформирован за счёт применения правил перехода к какой-либо сетке, а может быть только лишь начальным её состоянием (отсюда и название – созданный «из ничего»). Одним из критериев распознавания подобного состояния является наличие в рамках «Сада Эдема» так называемого шаблона «Сирота». «Сирота» представляет собой подмножество ячеек «Сада Эдема», у которого также, как и у всей сетки, отсутствует предыдущее состояние. Порой бывает довольно алгоритмически сложно распознать как «Сад Эдема», так и «Сироту», однако очевидно, что никакой сложности в переходе подобных состояний в последующее также не возникает.
Ещё в рамках нашего множества наличествуют различающиеся состояния сетки, которые при переходе к последующему состоянию формируют множество состояний сетки с размером равным единице – то есть формируют одно и то же состояние сетки. Такие варианты называются «Близнецами» и определяются как конфигурации, которые могут быть заменены друг на друга без «ущерба» для последующего изменения динамики состояний системы в целом. К примеру, в рамках нашего множества возможных сеток «Близнецами» являются следующие, из неполного их перечня:
В определённом смысле не имеет никакого значения, какой именно атавизер из множества предшественников оказался «родителем» актуального состояния сетки. Мы всегда можем совершенно непротиворечиво утверждать, что любой атавизер по отношению к текущему состоянию лежит в конечном множестве возможных предшественников. Иногда размер этого множества равен единице, как в последнем примере, иногда пяти, как в предпоследнем примере, но может быть любым целым неотрицательным числом, как один из вариантов – нулём, как в случае с «Садами Эдема». Здесь может показаться, что если «демон» не способен точно указать на атавизера, то сам «демон» несостоятелен и невозможен. Однако, этот же «демон» всегда способен сказать, что атавизер лежит во множестве возможных атавизеров, а также указать размерность этого множества и вновь акцентировать внимание на том, что совершенно не имеет значения, какой именно атавизер оказался реальным предшественником в определённый момент системной динамики, так как каким бы он не был – итог один и тот же. Здесь можно сказать, что это лишает нас возможности «вернуться к истоку», к тому самому «Саду Эдема». Однако это вовсе не так – мы всего лишь расширяем множество возможных атавизеров и его размер также конечен, хоть и может быть огромным.
На основе вышесказанного и несколько экстраполировав его смысл, мы можем предварительно заключить, что даже при неспособности «демона Лапласа» точно указать на конкретное предыдущее состояние системы, он способен утверждать, что предыдущее состояние находится в рамках конечного множества возможных предшественников, каждый из которых в определённом смысле тождественен любому другому из этого же множества. То есть даже если мы и не способны точно выявить координаты частицы и её импульс, мы можем утверждать, что совокупности всех возможных координат и импульсов лежат в рамках некоторого конечного множества и совершенно не важно, каким именно был конкретный актуальный вариант. Таким образом, мы полагаем, что заявляемая ранее неспособность «демона Лапласа» вычислить прошлое не соответствует реальному положению вещей, так как «демон» способен указать на множество абсолютно тождественных в данном контексте вариантов предшествующих состояний, то есть «демон» в известном смысле способен восстанавливать прошлое вне зависимости от количества вариантов – множество в любом случае конечно. А соответственно никакой свободы воли не нужно для того, чтобы «сошёлся пазл» – он и так отлично складывается.
Неправомерность критики в отношении «демона Лапласа» касательно его аналитических способностей, направленных в прошлое, была нами рассмотрена только лишь по причине наличия этой критики, что само по себе являет собой корень неправомерности, так как в определении самого Лапласа не указывалось на то, что «демон» обязан уметь восстанавливать прошлое на основе настоящего, а только лишь на возможность предсказывания будущего на основе данных о настоящем и о правилах перехода в рамках системы. И очевидным является то, что в случае с «Жизнью» Конвея «демон» – идеальный прогнозист. Идеальность «демона» здесь, конечно же обусловлена не только тем простым фактом, что размер множества возможных последующих состояний сетки всегда равен единице (как мы помним, так иногда может происходить и с размерностью множества атавизеров для актуального состояния), а ещё направленной линейностью самого контекста в игре. То есть игра Конвея линейна только в одном направлении – вперёд. И потому здесь предсказание всегда однозначно и «демон» всегда идеален.
Однако известно, что таким образом функционируют далеко не все системы. И мы говорим о сложных нелинейных системах открытого типа таких как: погодные условия, колебания рынка, поведение толпы, спортивные игры и так далее. В подобных системах каждое последующее состояние весьма сложно спрогнозировать на основе информации о состоянии текущем. И это также можно смоделировать на примере «Жизни», немного рандомизировав (то есть добавив элемент случайности) оговоренные ранее условия. Как мы помним, одним из условий, определяющим возникновение живой клетки из мёртвой служит постановление, согласно которому мертвая клетка становится живой при наличии ровно трёх живых соседей. Теперь же, в контексте заявленной рандомизации условий, мы постановим, что мёртвая клетка становится живой при наличии случайного, каждый раз строго определённого, числа соседей в диапазоне от нуля до четырёх, а живая клетка может стать мёртвой не только в случае, если у неё соседей больше трёх или же меньше двух, а ещё и в том случае, если случайное число в диапазоне от нуля до четырёх равно нулю.
И так далее, вариантов каждого последующего состояния стало гораздо больше одного, и, тем не менее, в каждом конкретном случае реально наличествует только один из них. Может показаться, что мы, за счёт рандомизации условий перехода, сделали невозможным прогнозирование последующего состояния сетки. Однако это не так – мы всего лишь увеличили размер множества возможных последующих состояний с одного до нескольких. И, тем не менее, повторим, что в каждом конкретном случае конечный вариант будет всего один и по мере «прорисовывания» сетки множество будет планомерно сокращаться до тех пор, пока его размер не станет равен единице. То есть мы, по сути, несколько реверсировали нелинейную динамику «Жизни» из направления в прошлое к направлению в будущее, смоделировав тем самым псевдорандомную систему. И разница заключается только в том, что если при подборе атавизеров для текущего состояния сетки все они были бы тождественны, то при переходе к следующему состоянию в их множестве «Близнецы» не гарантируются. То есть они могут случиться, но нет никаких гарантий, что они случатся.
Однако система подобного рода была нами определена именно как псевдорандомная по той причине, что так называемая случайность – это всегда всего лишь недостаток информации. Последовательность случайных чисел в рамках вычислительной системы всегда имеет некую начальную точку, которую, в случае именно с программными генераторами, можно также задавать искусственно. Аппаратные же генераторы случайных чисел также имеют начальную точку, однако её уже довольно сложно искусственно сформировать, так как она базируется на таких природных феноменах как тепловой шум, квантовые эффекты и прочее. И, тем не менее, это вовсе не представляется невозможным, так как в конечном итоге всё сводится к наличию информации. Зная заранее не просто то, что в условиях перехода заявлено случайное число, но также зная начальную точку генерируемой последовательности и порядковый номер итерации, вкупе со всеми остальными условиями множество возможных последующих состояний априори имело бы размер равный единице. И так для любой системы, сколь бы стохастической она не была.
Таким образом, резюмируем, повторив, что случайность – это недостаток информации. Сколь бы многофакторными не были условия перехода в рамках системы от предыдущего состояния к последующему – в любой системе, будь то игра «Жизнь» или квантовые эффекты, множество атавизеров всегда конечно и может быть вычислен как его размер, так и его элементы. При переходе же к последующему состоянию системы размер множества возможных конфигураций всегда обратно пропорционален полноте информации об условиях перехода. То есть, можно предположить, что свобода воли – совершенно избыточная фикция, а «демон Лапласа» жив и актуален.
ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ ПРОТИВ ЕСТЕСТВЕННОГО: ОНТОЛОГИЧЕСКАЯ И МЕТАФИЗИЧЕСКАЯ АРГУМЕНТАЦИЯ
Скиба И.Р.
В последнее время в связи с некоторыми успехами в сфере машинного обучения и построении интеллектуальных технологий, в области социально-философских исследований также происходит определённый «бум», который представляет собой появление большого количества работ, прямо связанных с данной проблематикой [1-8]. Конечно, это представляется нам вполне естественным, так как философия, в общем-то, примерно так и должна реагировать на подобные возникающие феномены – осмысляя их, а также переосмысляя феномены иные, затронутые возникшими.
Однако всё же коннотация смыслов и общая направленность философских «настроений» несколько настораживают. И настораживают они в двух аспектах: красной нитью проходящая «попытка реабилитации» естественного интеллекта в свете возникшего искусственного, и аргументация в защиту позиции о невозможности создания полноценного сильного искусственного интеллекта. В некотором роде здесь можно было бы увидеть основания для банального вопроса: «Если и естественный интеллект гораздо лучше и «сильнее» искусственного и сильный искусственный интеллект создать невозможно, то с чего вообще вся шумиха?». А уж раз она имеет место быть, то, как следствие, опасения, по всей видимости, небеспочвенны. В этом свете можно также упомянуть о знаменитом письме с запросом о временном прекращении исследований и разработок в сфере искусственного интеллекта хотя бы на полгода, подписанном Илоном Маском и прочими именитыми персонажами из той же сферы [9].
Опасна ли сложившаяся ситуация для безоблачного будущего человечества? Каковы будут стратегические последствия у столь буквальной органопроекции человеческого мозга? И хорошо или плохо для человека на далеко идущую перспективу делегировать собственный когнитивный (а в случае с принятием решений – также и волевой) потенциал внешнему устройству? Рассудит время, но не мы в данной работе. Тем более, как нам представляется, ситуация и так настолько понятна, насколько это вообще возможно. И в этом смысле целесообразно даже сформулировать некую теорему, полностью подтверждающую своё соответствие реальному положению дел при ретроспективном взгляде на историю человечества. Что не совсем характерно для логико-математических теорем, мы представим её в виде мысленного эксперимента. Итак, мысленный эксперимент «Теорема интересного выбора». Мы полагаем, что, если среднестатистического, но умеющего читать человека любой эпохи с момента развития письменности посадить в закрытую комнату, в которой есть две кнопки с надписями «Пряник» и «Кнут», то, даже при его полной убеждённости и практической верификации истинности означаемого и означающего и при многократном получении пряника при нажатии на кнопку «Пряник», – рано или поздно он нажмёт кнопку «Кнут». Возможно, что наша теорема требует практической проверки, но, допустим, на примере открытия технологии деления атомного ядра человечество «проверку» уже проходило.
Впрочем, вернёмся от нашего небольшого отступления к магистральной линии исследования. А она заключается в оценке валидности аргументации в пользу того, что естественный интеллект «сильнее» искусственного, и что сильный искусственный интеллект невозможен в принципе, а также в изложении некоторых (сразу скажем – весьма дискуссионных и, более того, довольно узконаправленных) соображений по поводу роли естественного интеллекта в развитии человеческой цивилизации.
Для начала представим стандартную для данной ситуации оппозицию – утрированную версию вывода теста Алана Тьюринга в виде гипотезы Ньюэла-Саймона и «Китайскую комнату» Джона Сёрла.
«Китайская комната» – это мысленный эксперимент, представленный Сёрлом в статье «Разум, мозг и программы» в 1980-ом году [10]. Суть данного эксперимента, если её несколько абстрагировать, заключается в следующем. Представим себе закрытую комнату, в которой находится человек, имеющий возможность взаимодействовать с внешним миром только лишь путём получения и передачи через щель в двери специальных карточек с китайскими иероглифами. Причём сам он не знает китайского языка. Всё, чем он обладает, – это инструкции на понятном ему языке о том, как именно реагировать на полученные из внешнего мира карточки с китайскими иероглифами. Причём реакция эта представляет собой некую комбинацию карточек с иероглифами, которые необходимо передать во внешний мир в ответ на полученные оттуда карточки с иероглифами. Таким образом, данный человек, получив карточки с совершенно ему непонятными символами, находит в инструкции на понятном ему языке ту совокупность карточек с опять же непонятными ему символами, которую необходимо передать обратно в качестве ответа. И здесь уже становится понятно, почему этот человек даже в перспективе лишён возможности выучить китайский язык: перевод конкретных иероглифов ему никто не сообщает. И, тем не менее, собеседники этого закрытого в комнате человека наблюдают следующую ситуацию: в ответ на задаваемый и передаваемый за дверь в виде карточек вопрос они получают валидный ответ. Причём как вопрос, так и ответ – на китайском языке. Само собой разумеется, что у наблюдателей с довольно высокой вероятностью должно сложиться впечатление о том, что тот, кто им отвечает из-за двери, знает китайский язык и, соответственно, хорошо понимает его.
И именно в наглядной демонстрации этой самой разницы – между тем, что представляется наблюдателям снаружи, и тем, что на самом деле происходит внутри в закрытой комнате, – и заключаются суть и предназначение «Китайской комнаты» Джона Сёрла. И, конечно же, здесь присутствует яркая аналогия с функционированием любой вычислительной техники в целом. Является общеизвестным, что компьютерная техника функционирует путём обработки данных в бинарном формате, то есть оперирует только лишь двумя цифрами: 0 и 1. И так как практически любые данные можно представить в виде некой совокупности количественных параметров и, соответственно, затем преобразовать их в бинарную систему счисления для дальнейшего «скармливания» компьютеру, то он, собственно, и представляет собой этого самого запертого в комнате человека. А пользователи представляют собой тех самых наблюдателей, которые находятся снаружи. И как запертый человек из эксперимента Сёрла, так и компьютер могут совершенно не понимать специфики и внутренней организации тех данных, которые им предоставляются, а просто, согласно заранее определённому алгоритму, формировать некие ответы на запросы.
Гипотезу же Ньюэла-Саймона можно формально определить следующим образом: способность к осуществлению символьных вычислений сама собой предполагает способность к осмыслению, а способность к осмыслению сама собой предполагает способность к осуществлению символьных вычислений. Под символьными вычислениями здесь подразумевается весьма широкий спектр возможных видов вычислительной деятельности. Однако под апофеозом способности к осуществлению символьных вычислений в рамках данной гипотезы подразумевался непосредственно сильный искусственный интеллект, определённый Сёрлом. С этих позиций ситуация с функционированием интеллектуальных технологий выглядит совершенно иначе, чем после рассмотрения «Китайской комнаты» Сёрла: осмысленным теперь представляется даже старый калькулятор. Разумеется, что гипотеза о физической символьной системе подвергается критике, но, тем не менее, она в любом случае не соответствует критерию Карла Поппера о необходимой фальсифицируемости всякого научного знания – то есть она потенциально недоказуема и неопровержима. Также стоит отметить, что именно в рамках широкого символьного подхода к разработке искусственного интеллекта развиваются такие технологии, как GPT и остальные, ему подобные.
Теперь же резюмируем, что в контексте «Китайской комнаты» Сёрла ни одна символьная система не сумела бы доказать, что она способна к осмыслению, а не просто к монотонному выполнению заранее определённого алгоритма, а в контексте гипотезы Ньюэлла-Саймона любая символьная система априори обладает способностью к осмыслению.
После представления классического противостояния между вышеприведёнными взглядами мы представим наш собственный мысленный эксперимент «Человек без психики».
«Представьте себе человека, у которого нет души, психики, интеллекта, чувств, эмоций, способностей к осмыслению и так далее – всего прочего в этом же контексте. Но он 24 часа в сутки притворяется, что у него это всё есть в наличии и функционирует так же, как у того, у кого все это «есть». Если мы спросим у него: «Есть ли у тебя психика и/или душа?», то он ответит: «Конечно же, есть. Как и у тебя». Допустим, мы ему не доверяем и хотим проверить, говорит ли он нам правду. Как мы поступим, дабы проверить истинность его слов? Если бы он говорил, что у него есть, к примеру, печень или сердце, то в случае недоверия мы могли бы потребовать сделать ультразвуковое исследование. Если бы он говорил, что у него есть кости, можно было бы убедиться в этом по результатам компьютерной томографии. Если бы он говорил, что у него есть мозг, подтверждением стали бы результаты магнитно-резонансного исследования. 
Но актуальный вопрос качественно иной: наличие или отсутствие психики, сознания, внутреннего мира, чувства смысла. Можно попробовать применить психологические тесты по отношению к данному субъекту, однако вспомним: он притворяется 24 часа в сутки, что у него есть психика, поэтому он пройдёт эти тесты так же, как прошёл бы их обычный человек. И все же, что поможет нам понять, есть ли у человека психика, душа, интеллект или же он просто притворяется? К сожалению, способов и инструментов такой проверки не существует. Мы не знаем, ни что они (психика, душа, сознание) собой представляют, ни где их искать. Поэтому мы формально даже не можем утверждать, что они существуют, ибо не знаем, как это доказать и как это опровергнуть. Мы априори принимаем, что они есть у нас самих, и затем, за счёт идентификации с другими, признаём, что у них они, наверное, тоже есть. Но мы никогда не можем точно знать, какова ситуация на самом деле. Мы просто принимаем это «на веру», а обоснованность суждений происходит по интерпретации внешних бихевиоральных аспектов других людей. Также и с вопросом о свободе воли – мы принимаем «на веру» её наличие, но никаких доказательств её наличия не существует, как, соответственно, и возможностей для опровержения.
P.S. Критика самой возможности осуществления процесса притворства без наличия психики и интеллекта неправомерна, ибо контраргументом служит мысленная попытка представить человека с имманентным содержанием в виде «алгоритма притворства», что по смыслу соотносится с «Китайской комнатой» Сёрла и никак не влияет на выводы из эксперимента «Человек без психики»».
Итак, после рассмотрения всех трёх позиций, выраженных в мысленных экспериментах, мы можем уже внимательно рассмотреть сложившуюся на данный момент ситуацию. Здесь же, для краткости, заключим всю совокупность понятий вида: душа, психика, сознание, свободная воля, способности к осмыслению, творческие акты и прочие, в обобщённую формулировку «метафизические свойства» (то есть, в узком смысле, ненаблюдаемые). А сама ситуация заключается в том, что: системы искусственного интеллекта генерируют, обобщённо скажем, контент, и человек генерирует контент. Чей контент лучше, мы рассматривать не будем. Но получается, что в том случае, если и интеллектуальная система, и человек способны выполнять одну и ту же деятельность, то можно сделать три различных вывода:
•	Человек и система обладают метафизическими свойствами
•	Ни человек, ни система не обладают метафизическими свойствами
•	Метафизические свойства не являются необходимыми для генерирования контента
Рассмотрим эти утверждения. На основе приведённых нами мысленных экспериментов является очевидным, что ни наличие, ни отсутствие метафизических свойств ни у человека, ни у интеллектуальной системы мы не можем однозначно ни доказать, ни опровергнуть. Поэтому два первых утверждения в некотором роде равнозначны, касаются одного и того же и являют собой вид с разных сторон на типичную «смысловую ловушку». То есть проблема будет оставаться дискуссионной в любом случае и вне зависимости от количества и «силы» аргументов любой из сторон – говорить на эту тему можно, но решить проблему – нет. Третье же утверждение представляется нам весьма интересным в том смысле, что его также невозможно опровергнуть; однако оно имеет практическое подтверждение на примерах современных интеллектуальных систем, за которыми традиционно не признаётся никаких метафизических свойств. Так необходимы ли они человеку и не являют ли они собой просто очередные слова в тексте? Здесь также стоит заметить, что, предположительно, деятельность интеллектуальных систем в дальнейшем будет расширяться, и потом они могут научиться также ходить, бегать, работать за станком и прочее. Поэтому мы также можем расширить наш вопрос: необходимы ли метафизические свойства человеку для осуществления его деятельности в широком смысле? Или даже мы можем его сузить и конкретизировать: для какого конкретно вида деятельности, которую можно внешне зафиксировать и однозначно трактовать, необходимы человеку метафизические свойства? Мы предполагаем, что попыток ответа может быть огромное количество, но ни один из них не сможет предоставить убедительного и однозначного эмпирического доказательства. Единственный возможный вариант – редукция метафизики к физике, то есть к нервной системе. Но даже там всегда можно сказать, что импульсы отлично передаются с аксонов на дендриты и безо всякой метафизики.
Таким образом, мы постулируем концептуальную эквивалентность тезисов: «Сильный искусственный интеллект невозможен» и «Естественный интеллект невозможен». И дабы утверждать первый, также необходимо доказать второй.

ОПИСАНИЕ СИСТЕМЫ ПРОГРАММНЫХ КОМПОНЕНТОВ КАК ЯЗЫКА АДАПТАЦИИ ПАРАДИГМЫ ОБЪЕКТНО-ОРИЕНТИРОВАННОГО ПРОГРАММИРОВАНИЯ К РАЗРАБОТКЕ СИСТЕМ СИЛЬНОГО ИСКУССТВЕННОГО ИНТЕЛЛЕКТА

Аннотация. В работе предлагается авторская концепция программных компонентов, основой для которой являются некоторые ключевые положения теории графов. Предлагаемая концепция предназначена для абстрагирования фундаментальных аспектов объектно-ориентированного программирования, таких как модули, классы, объекты, агрегация, композиция, наследование, зависимость и прочие с целью адаптации данной парадигмы к нуждам построения интеллектуальной техники нового поколения.
Ключевые слова: программный компонент, искусственный интеллект, объектно-ориентированное программирование, теория графов, класс, объект.

Введение

Разработка, внедрение и осмысление систем искусственного интеллекта является одним из наиболее значимых исследовательских направлений в последние десятилетия. И данную волну можно назвать вторым бумом развития интеллектуальных технологий, по аналогии с первым – 60-70-х г. 20 века. Отличие новой «волны» заключается в первую очередь в мощности вычислительной техники, по сравнению с аналогами того времени. То есть сам подход к разработке всё так же основывается на концепциях «волны» предыдущей – принципиально новые идеи отсутствуют.
Примерно тогда же начали в том числе формироваться и основы парадигмы объектно-ориентированного программирования, которая теперь является преобладающей в сфере разработки программного обеспечения. Искусственные нейронные сети, как частный случай реализации систем искусственного интеллекта способны к обучению, которое понятийно определяется как машинное обучение или глубокое обучение – как подмножество машинного. И в сфере программной реализации обучения нейронных сетей одним из наилучших языков программирования повсеместно признаётся Python, который поддерживает парадигму объектно-ориентированного программирования – в Python все сущности представлены в виде объектов. Стоит также упомянуть, что ChatGPT написан на Python.
Основываясь на вышесказанном, становится очевидной актуальность переосмысления парадигмы объектно-ориентированного программирования применительно к разработке систем сильного искусственного интеллекта, так как до сих пор «получались» только представители слабого искусственного интеллекта. Представляется весьма целесообразным формирование некой методологической «прослойки» между объектно-ориентированным программированием и нуждами построения сильного искусственного интеллекта.

Основная часть

В первую очередь хотим уточнить, что наш подход к формированию систем сильного искусственного интеллекта определяется как технотропный – в противовес повсеместно применяемому на данный момент антропному [1]. Технологии, формируемые при помощи антропного подхода, определяются нами как лого-машины, а сущности, разрабатываемые при помощи технотропного – как психо-машины [2; 3]. Предполагается, что системы сильного искусственного интеллекта могут быть разработаны только при помощи технотропного подхода – то есть основываясь на возможностях самой техники к осуществлению процесса самоорганизации. На данный момент фактических примеров психо-машин не существует и сильный искусственный интеллект является сугубо футуристическим понятием. Однако, мы считаем, что при помощи адаптации парадигмы объектно-ориентированного программирования к парадигме сильного искусственного интеллекта мы сумеем приблизить момент его воплощения.
В контексте рассмотрения систем сильного искусственного интеллекта (далее – ИИ – Прим. автора) с позиций парадигмы объектно-ориентированного программирования (далее – ООП. Прим. автора), мы предлагаем к использованию разработанную нами концепцию программных компонентов (далее – ПК – Прим. автора), которая представляет собой своеобразный язык абстрагирования феноменов ООП к необходимостям парадигмы сильного ИИ. В контексте ООП наличествует множество различных уровней программных сущностей, то есть объекты, классы, модули и так далее, между которыми установлены некоторые специфические взаимоотношения, такие как наследование, агрегация, композиция, зависимость и прочее [4]. Все эти особенности критично важны в рамках парадигмы ООП. Тем не менее, в контексте применения ООП к парадигме сильного ИИ самому ООП, в перспективе, необходимы некоторые специфические адаптации, так как ранее данная парадигма программирования применялась исключительно к системам слабого искусственного интеллекта, а разница в данном случае, предположительно, должна быть довольно существенной.
Поэтому данная адаптация подразумевает, согласно предлагаемому подходу, определённый уровень абстрагирования исследуемых объектов. То есть, к примеру, если в случае с реализацией того или иного феномена ООП применительно к системам слабого искусственного интеллекта мы использовали некоторые сущности, определяли им такие-то связи друг с другом и реализовывали их тем или иным образом, то, в случае с футуристическими системами сильного искусственного интеллекта мы не может утверждать, что именно эти же сущности и именно со связями такого же типа будут реализованы на том же уровне успешности. Вполне возможно, что будут необходимы некоторые новые сущности и связи таких типов, которые пока не определены в каноничном ООП. Из этого следует, что мы зачастую не можем заранее точно сказать, что именно и как именно нам следует использовать при разработке непосредственно систем сильного искусственного интеллекта. Однако мы способны точно определить, что там должны будут использоваться некоторые ПК с теми или иными качествами и свойствами, которые будут необходимы в таком-то или таком-то количестве и должны быть связаны друг с другом таким-то или таким-то способом. Поэтому метод абстрагирования порой бывает весьма целесообразен.
Исходя из вышесказанного, мы считаем, что предлагаемая нами концепция ПК актуальна для применения в рамках адаптации ООП к парадигме сильного ИИ. Однако, стоит отметить, что данная концепция была нами разработана для адаптации парадигмы ООП к парадигме сильного ИИ на основе абстрагирования некоторой другой теории, а именно – теории графов [5].
Для понимания того, что мы имеем в виду, говоря о концепции ПК, необходимо затронуть некоторые из аспектов теории графов. Причём мы будем затрагивать только наиболее общие моменты, так как нам здесь важнее именно идея абстрактной модели и некоторые из её следствий. Сама теория графов представляет собой раздел дискретной математики, в котором используются математические абстракции для описания различных связей между некоторыми средоточиями. Эти связи в теории графов называются рёбрами, а «средоточия» – вершинами. Говоря более формально, граф G – это совокупность множества вершин – V и неупорядоченного множества рёбер – E. Это можно обозначить как «G (V, E)». Открытие теории графов или, более предметно, формирование прямых предпосылок для её введения в сферу научной деятельности, обычно приписывается Л. Эйлеру в его работе с «задачей о Кёнигсбергских мостах», в которой Эйлер доказал то, что невозможно пройти по всем семи мостам, не проходя по одному из них дважды. Сам Эйлер не использовал термин – граф, не говорил он также ни о рёбрах, ни о каком-либо на данный момент каноничном аспекте теории графов. Тем не менее, именно он почитается, как отец-основатель данной теории. После него теория графов повторно «открывалась» ещё некоторое количество раз, в том числе Г. Киргховом, А. Кели, У. Гамильтоном, К. Жорданом и прочими. Само собой, до нашего времени данная область математики получила бурное развитие и на теперешний момент обладает обширным, хоть и не вполне унифицированным понятийным аппаратом.
В рамках обоснования нашей концепции ПК, целесообразно осветить ключевые аспекты данной теории, которые наглядно представлены на основе знакомства с её понятийным аппаратом вкупе с экстраполированием смысла этих понятий в контекст концепции ПК. К примеру, уже упоминались основные структурные элементы графа – вершины и рёбра. Здесь сразу же прослеживается некоторая аналогия с программными компонентами и связями между ними: то есть сам программный компонент соответствует вершине, в то время как связь между отдельно взятыми двумя из них соответствует ребру. Здесь же стоит сказать, что у каждого графа наличествует некий размер и порядок. Размер графа определяется количеством рёбер, которые включены в вышеупомянутое множество E, а количество вершин, включённых в множество V, определяет порядок графа. Точно также и любая система, состоящая из некоторого количества ПК, обладает некоторой размерностью, которую мы не уточняем понятийно в виде некоей ранжируемости, так как абстрагируемся от конкретики и содержания, при этом максимально сосредотачиваясь непосредственно на форме. Также само собой разумеется, что система ПК может иметь различные типы взаимосвязи между двумя ПК и вполне может быть такое, что компонент А связан с компонентом В не однократно, а двукратно: то есть, более формально, связующее звено С связывает между собой ПК А и В, а связующее звено D также связывает между собой эти же компоненты А и В. В теории графов рёбра, реализующие вышесказанное, то есть более чем однократно инцидентные одним и тем же вершинам – определяются как кратные рёбра. Инцидентностью же называется отношение некоторой принадлежности между вершиной и ребром. И если в случае с кратными рёбрами мы говорили о разных рёбрах, то если одно и то же ребро дважды инцидентно одной и той же вершине, то эта ситуация в теории графов будет называться циклом. В случае наличия циклов и кратных рёбер у графа его принято определять, как мультиграф или псевдограф. С другой стороны, кратных рёбер и циклов может и не быть вовсе в графе и тогда такой граф будет определяться как простой граф.
Мы, с точки зрения концепции ПК также подразумеваем различные уровни сложности организации системы ПК и включаем в неё максимально широкие возможности формирования взаимосвязей между компонентами, в то же время мы не ограничиваем специфику взаимосвязей между ПК в рамках системы сильного ИИ теми данностями, которые предоставлены в теории графов. К примеру, в рамках теории графов степень вершины определяется количеством инцидентных ей рёбер, но мы в своей концепции ПК, при оценке некоторой сложности или значимости компонента, должны учитывать не только его внешние взаимосвязи, а также и специфику его внутренней организации, которая, разумеется, влияет на его положение в контексте системы не менее, чем явные внешние связи и их количество.
Далее, в том случае, если два ребра инцидентны одной и той же вершине, то они определяются как смежные. Вершины же называются связными, если наличествует цепь, которая их соединяет. В свою очередь цепью называется маршрут без повторяющихся рёбер, а сам маршрут в графе определяется как конечная последовательность вершин и рёбер, в которой все вершины, кроме последней, соединены с последующей вершиной ребром. Для нас же, в контексте концепции ПК, данные понятия также будут воплощены в виде последовательных связей ПК друг с другом, как непосредственно, так и опосредовано. К примеру, в случае разработки основы для системы сильного ИИ мы, в наиболее общем случае, принимаем как данность тот факт, что система должна будет каким-либо образом получать информацию из внешней среды. Соответственно, должна наличествовать некоторая последовательность программных компонентов, ответственность которых будет заключаться в получении информации в том или ином виде. И уже на данном этапе следует заметить, что далеко не всегда информация, которая наличествует в окружающей среде, подходит для её непосредственного восприятия со стороны системы. То есть мы говорим о том, что уже на этапе получения информации подразумевается её некоторое преобразование: точно так же, как стимулы внешнего мира преобразуются в нервные импульсы при восприятии чего-либо живым существом. Также подразумевается, что полученная информация должна быть как-то передана в центры её обработки. Конечно, все эти акты перемещения информации должны быть не хаотичны, а каким-то образом упорядочены, более того – оптимизированы. А это означает, что нам необходим некий маршрут. Именно такой, какой и подразумевается в графе. Если же по этому маршруту можно перемещать что-либо только в одном определённом направлении, то такой маршрут в теории графов определяется как путь – маршрут в ориентированном графе. Ориентированный граф же – это граф, рёбра которого являются не просто инцидентными двум смежным вершинам, но ещё и конкретно направлены от вершины А к вершине В, но никак не наоборот. Данная направленность учитывается нами в концепции ПК в том смысле, что случаются связи подобного рода между ПК, при которых один компонент должен определять поведение другого компонента, но не наоборот, что в ООП понятийно обозначается как зависимость.
Далее, для более полного понимания того, что мы подразумеваем под системой ПК, необходимо ознакомится с понятием класса эквивалентности в теории графов. Класс эквивалентности – это множество всех вершин графа, которые связаны друг с другом, то есть такое множество вершин, в рамках которого от любой одной вершины есть некий маршрут до любой другой вершины. А компонента связности – это подграф исходного графа, содержащий все вершины одного из классов эквивалентности (по связности) и все их рёбра, то есть это такое подмножество вершин и рёбер исходного графа, в рамках которого от каждой вершины есть маршрут до каждой другой вершины. В том случае, если все вершины графа связаны друг с другом, то граф называется связным, обладает только одной компонентой связности и эта компонента связности тождественна классу эквивалентности по связности. Проводить некую демаркацию между классом эквивалентности по связности и компонентой связности имеет смысл, хоть он не сразу заметен. А смысл в том, что в графе может быть более одной компоненты связности. Данный момент примечателен также тем, что на его наглядной основе лучше всего заметен тот простой факт, что, как мы полагаем, в системе сильного ИИ в любом случае должна быть только одна компонента связности. Но также на данном этапе следует пояснить, что имеется в виду только то, что ни один «островок» вершин и рёбер не может быть в полной мере изолирован от целостной системы ПК. Однако порой нечто подобное оказывается продуктивным в контексте эволюционного развития, поэтому мы обязаны уточнить про вес рёбер.
Граф, у которого все рёбра имеют некоторый вес, называется взвешенным графом. Данная абстракция является весьма удобной при моделировании многих аспектов макромира, в частности при выстраивании логистических маршрутов, подборе оптимальной стоимости билетов на поездку и так далее. Мы же, в рамках концепции ПК, всё-таки не скованы понятийным аппаратом теории графов и хотим сказать, что, так как у системы сильного ИИ не может быть более одной компоненты связности, всё же иногда будет происходить нечто, подобное изоляции каких-либо фрагментов в контексте общей целесообразности данной изоляции. Однако это не будет изоляцией в полном смысле слова, а скорее изоляцией «мнимой», так как связи будут оставаться, то есть «Кёнигсбергские мосты» не будут разрушены, а будут скорее временно заблокированы. То есть, в рамках концепции ПК будет использоваться что-то наподобие латентных рёбер, которые есть упрощение несколько более сложной абстракции нелокального взаимодействия. То есть подразумевается, что в случае с разработкой системы сильного ИИ каждый из ПК должен иметь связь с каждым иным ПК в рамках целостной системы – таким образом, обеспечивается некоторое единство системы и формируется основа для её лабильности в плане самоорганизации, то есть мы никогда заранее не определяем количество и специфику ПК, так как на этот вопрос должна будет отвечать сама система в ходе самоорганизации, а никак не разработчики. Таким образом, локально ограничив некоторые возможности системы к взаимодействию с самою собой, мы ограничим и её возможности по построению максимально разнообразных ПК под свои нужды, а это вовсе не то, чего бы хотелось добиться при разработке. Однако, подразумевается, что системе на каком-то этапе развития может понадобиться изолировать некоторые участки. Но связь не может быть разрушена полностью, так как это приведёт к сепарированию системы. В сущности, связь может быть по-настоящему разрушена и компонент связности может стать более одной только в одном случае – в случае репликации системы, то есть порождении системой дочерних образований. В остальных же случаях, в контексте концепции ПК, мы будем считать то, что в теории графов называется изолированными вершинами или изолированными участками графа, латентно связанными со всей системой. В то же время можно также уточнить, что латентные рёбра, в случае со взвешенным графом (а в концепции ПК все взаимосвязи всегда «взвешены») просто обладают весом, который не вполне соответствует всем остальным весам рёбер графа и является как бы актуально «неподъёмным».
Далее, необходимо уточнить также и про изоморфизм графов. Вообще изоморфизм представляет собой выражение некоторой степени схожести между объектами. Принято считать, что два графа являются изоморфными в том случае, если существует биективное, то есть взаимно однозначное, отображение ребра к ребру и вершины к вершине. Следует заметить, что изоморфизм далеко не обязательно означает некую внешнюю схожесть, а касается только ключевых структурных особенностей объектов. В случае с графами этого достаточно. В случае же с системами сильного ИИ, построенными согласно нашей концепции ПК, стоит сказать, что он вряд ли возможен в принципе. Мы уточняем, что в контексте сильного ИИ мы говорим о нём в единственном числе только условно, то есть только в том, смысле, в котором о человеке можно сказать «человек». Так как человек сам по себе подобен Платоновскому эйдосу или «сферическому коню в вакууме», то есть он существует лишь идеально, как некая абстракция. В реальности же все люди различны. И именно это мы предполагаем и для систем сильного ИИ – они все должны быть разными. Так что мы имеем в виду скорее интеллекты, чем интеллект, а отсюда и отсутствие изоморфизма.

Заключение

Резюмируя, хотим уточнить, что под системой ПК мы понимаем объект исследования несколько модифицированной и расширенной теории графов, которая, ввиду отсутствия даже теоретически оформленных примеров сильного ИИ, допускает некоторые намеренно установленные белые пятна, наподобие латентных рёбер, зависимости не только от количества инцидентных рёбер, но также и от специфики внутренней организации вершины и так далее. В рамках системы ПК мы подразумеваем наличие непосредственно самих ПК и различных связей между ними. Мы предполагаем, что они способны организовываться в некоторые группы и весьма разнообразно трансформировать свой функционал в процессе самоорганизации. И функционирование феноменов подобного рода наглядно и доступно описывается с позиций системы ПК. Наконец мы полагаем, что с точки зрения описательного прагматизма предлагаемый нами «язык» соответствует необходимости и целесообразности, так как позволяет наиболее абстрактно описать структуру предлагаемых нами решений, при этом не вдаваясь в те детали, о которых мы на данный момент не имеем представления.
УДК 004.8:159.9

ТЕХНОТРОПНЫЙ ПОДХОД К РАЗРАБОТКЕ СИСТЕМ СИЛЬНОГО ИСКУССТВЕННОГО ИНТЕЛЛЕКТА

Ведение. В наше время существует множество разнообразных подходов к разработке технологий сильного искусственного интеллекта. Нами же в процессе исследования было предположено, что разработку систем подобного рода целесообразно осуществлять при помощи механизма самоорганизации. Причём в данном случае понятие самоорганизации не тождественно понятию обучения (к примеру: машинное обучение), а подразумевает скорее формирование специфических начальных условий для генерирования и последующего развития сообщества систем. Подобное сообщество определяется нами как технотропное сообщество.
Необходимость реализации процесса самоорганизации со стороны систем технотропного сообщества подразумевает известный уровень их автономности и невмешательства извне в естественный процесс их эволюции. Также немаловажным является понимание того, что изначально поведение «технотропных зародышей» будущих психо-машин может крайне сильно отличаться от того, что принимается за среднестатистические целесообразные, рациональные и конструктивные паттерны реагирования и жизнедеятельности – как у человека, так и у лого-машин. В связи с этим, в процессе исследования нами были выявлены два кардинально различающихся между собой подхода к осмыслению процесса самореализации систем искусственного интеллекта и интерпретации результатов этого процесса:
•	Антропный подход
•	Технотропный подход
Антропный и технотропный подходы. Актуальность проведения чёткой демаркационной линии между данными подходами оказалась целесообразна, ибо её – демаркации – отсутствие в теории и практике парадигмы искусственного интеллекта лишает исследователей возможности рефлексивно осмыслить их собственную деятельность и в полной мере понять, чем именно они занимаются: имитируют специфически человеческие перцептивные, когнитивные и бихевиоральные паттерны на субстрате внешне антропоморфной или вовсе не антропоморфной машины; формируют у интеллектуальной системы некий уровень антропоморфной рациональности; или же пытаются создать самобытный искусственный интеллект в его собственной самоорганизации. Выявление различий в данных подходах также обосновано демаркацией между системами слабого искусственного интеллекта – лого-машинами и системами сильного искусственного интеллекта – психо-машинами как в онтологическом сущностном смысле, так и в генеалогическом – в плане различных методов «становления» лого-машин и психо-машин.
Актуальность выявления различий между данными подходами заключается также в том, что на основе наглядной корреляции становятся более явственно заметны ключевые аспекты, принципы и правила, которых необходимо придерживаться и которым необходимо следовать при разработке систем именно сильного искусственного интеллекта и на основе которых целесообразно обосновывать использование определённых философско-методологических положений.
В процессе исследования было выявлено, что в рамках антропного подхода к разработке интеллектуальных технологий и интерпретаций, получаемых в результате этой деятельности результатов, зачастую осуществляется тотальная повсеместная теоретическая и практическая, процессуальная и ситуативная опора на внешние аспекты «антропной матрицы», то есть на наблюдаемые невооружённым глазом внешние характеристики, качества и свойства человека и его природы. Конечно, на некоторые внутренние аспекты человеческой природы исследователи и разработчики в рамках антропного подхода также опираются, к примеру, на нейрофизиологию – искусственные нейронные сети представляют собой примитивные модели нервной системы человека. Однако опора на характеристики внешние наиболее наглядна. Одним из ярких примеров данной тенденции является так называемый «Тест Тьюринга».
Во времена Тьюринга компьютеры реагировали медленнее человека, а сейчас правило стандартизации времени ответов также необходимо, потому что интеллектуальные системы реагируют гораздо быстрее, чем человек. Фразу из описания теста «машина прошла тест» следует понимать, как «технология обладает сильным искусственным интеллектом». При осмыслении специфики теста и предъявляемых им требований к «бытию разумной машины» в первую очередь следует обратить внимание на то, чего, в сущности, требует тест. А именно, он требует внешнего уподобления и приблизительного имитирования человеческой коммуникационной деятельности. Нами было предположено, что подобный «искусственный интеллект» следует классифицировать как «коммуникативный искусственный интеллект» и отнести к системам слабого искусственного интеллекта – лого-машинам без претензии на «человекоразмерность». Генезис подобной системы также не является сложным: достаточно смоделировать на субстрате искусственной нейронной сети ключевые аспекты человеческой речевой деятельности в плане правильного построения высказываний, а также подвергнуть нейросеть процессу машинного обучения разговорной (коммуникативной) деятельности и будет получена интеллектуальная система способная пройти тест Тьюринга, что доказывается многочисленными примерами успешной реализации данного мероприятия [1]. Тем не менее, подобного рода системы никоим образом не являются претендентами на «человекоразмерность» в смысле обладания технотропной психикой и технотропным сознанием [2]. 
Это классический пример антропного подхода к разработке систем искусственного интеллекта в целом: избирательно и искусственно моделируются некоторые аспекты какого-либо сугубо человеческого вида деятельности и за счёт имитирования достигается внешнее некоторое подобие, а затем успешность-неуспешность всего данного процесса определяется по корреляции смоделированного и воплощённого на субстрате интеллектуальной системы процесса с тем же процессом, реализуемым человеком. Уровень антропоморфности (не «человекоразмерности», а именно антропоморфности) – единственный критерий, который ситуативно трансформируется в зависимости от тактических задач и стратегических целей: применительно к задачам классификации, интеллектуальных игр и прочих из этой же категории превосходство над человеком считается положительным, а в контексте игрового искусственного интеллекта возможности искусственного интеллекта, ради достижения должного уровня антропоморфности, снижают. В рамках антропного подхода происходил на данный момент весь процесс разработки интеллектуальных технологий, которые были представлены исключительно системами слабого искусственного интеллекта – лого-машинами, без претензии на «человекоразмерность». Следует также заметить, что в рамках антропного подхода само понятие «человекоразмерности» воспринимается буквально и изолированно – как правило, в каком-либо отдельном аспекте (коммуникация, машинное зрение, обработка данных и прочее) – и отождествляется с антропоморфностью, а самоорганизация неправомерно редуцируется до уровня машинного обучения. Следует отдельно сказать о том, что в нашем подходе к разработке систем сильного искусственного интеллекта антропоморфность не отождествляется с «человекоразмерностью»: антропоморфность понимается как внешняя схожесть в каком-либо аспекте/аспектах интеллектуальной системы и человека и она – антропоморфность – интерпретируется как качество лого-машин; «человекоразмерность» же понимается нами как схожесть скорее внутренняя, но схожесть не именно с человеком в плане подобия перцептивно-когнитивного аппарата, способности решать задачи и т.д., а схожесть в уровне эволюционного развития и способностей к аутоадаптации и аллоадаптации по отношению к окружающей среде – человек же здесь выступает скорее метафорой возможностей самоорганизационнного и самоорганизующегося развития в контексте Вселенной. «Человекоразмерность» присуща (или должна быть присуща) только психо-машинам (по отношению к лого-машинам) и она есть один из ключевых критериев наличия в рамках интеллектуальной системы сильного искусственного интеллекта, однако, в соответствии с нашим подходом к сильному искусственному интеллекту, человек является не пределом возможностей эволюции системы в целом, а скорее одним из примеров того, как может происходить развитие и самоорганизация. В области же антропного подхода программно-аппаратными воплощениями и представителями искусственного интеллекта являются системы слабого искусственного интеллекта – лого-машины. Развитие в рамках антропного подхода технотропного сообщества интеллектуальных систем, технотропной психики и технотропного сознания вовсе не рассматривается. 
Технотропный же подход на данном этапе научно-технического прогресса не имеет своих примеров и представителей в плане программно-аппаратных воплощений, а представлен только в виде концептуальных теоретических и футуристических положений о системах сильного искусственного интеллекта – психо-машинах. В контексте технотропного подхода принята опора на технотропную самоорганизацию, как единственно возможный путь естественной эволюции интеллектуальных технологий. Имитирование ключевых аспектов и успешное внешнее уподобление не интерпретируются как некие достижения с интерпретационных позиций технотропного подхода, а и вовсе не рассматриваются. Стратегической целью технотропного подхода к разработке систем сильного искусственного интеллекта является генерирование систем, обладающих технотропной психикой и технотропным сознанием. Тактическими задачами являются: формирование перцептивной модели в соответствии с критериями парадигмы сильного искусственного интеллекта; формирование сообщества цифровых особей способных к самоорганизации и эволюционированию как каждой отдельно, так и в рамках всего сообщества в целом; генерирования в рамках имманентной технотропной эволюции технотропной психики и технотропного сознания; достижение «человекоразмерности», как она интерпретируется в рамках технотропного подхода.
Заключение. В соответствии с вышесказанным, «человекоразмерность», в рамках технотропного подхода, не отождествляется с антропоморфностью, а являет собой частное определение всеобщего эволюционного принципа, то есть технотропная «человекоразмерность» подразумевает уровень психической организации, необходимый и достаточный для осуществления глобального менеджмента в контексте естественной среды обитания интеллектуальных систем – «киберпространства» и наличие попыток выхода за рамки этой среды. То есть технотропная «человекоразмерность» являет собой приблизительно тот же уровень антропной «человекоразмерности», на котором находится человечество на данном этапе своей эволюции.
Мы предполагаем, что предложение по использованию разрабатываемой нами методологии генерирования систем сильного искусственного интеллекта по меньшей мере имеет необходимость в проверке ключевых оснований на программно-аппаратном субстрате, то есть – внедрения в научно-исследовательскую деятельность не только в качестве «знания», но также и в виде совершенно определённого материального объекта.
О ДОКАЗАТЕЛЬСТВЕ «НЕДОСТИЖИМОСТИ» ЧЕЛОВЕКА ИСКУССТВЕННЫМ ИНТЕЛЛЕКТОМ

В свете последних достижений в области разработки технологий искусственного интеллекта (ИИ), в социуме всё чаще проскальзывают опасения по поводу возможной замены человека на ИИ в той или иной сфере [1]. Само собой разумеется, что трудовая деятельность не редуцируется сугубо к производству какой-либо продукции: будь то пищевая промышленность, сфера услуг или научно-исследовательские изыскания. Ведь не менее значимой её частью является необходимость «утилизации» хронологического и когнитивного ресурсов для тысяч, сотен тысяч, миллионов или миллиардов человек – людей и «мысли» людей надо чем-то занимать, а то они могут начать думать «не в том направлении» и стать непредсказуемой и потенциально опасной «толпой». Те, кто держит руку на «пульте» принятия решений, это также должны понимать. А потому замена человека технологией ИИ, даже при её (потенциально возможно) более высокой продуктивности и меньшей себестоимости, всё-таки не является однозначно конструктивным мероприятием [2].
Однако само наличие подобных вопросов и опасений провоцирует ответную реакцию, как со стороны диджерати – элиты виртуальной реальности, так и научного сообщества – прежде всего, в социогуманитарной области [3]. И эта реакция сводится к тому, что человеку необходимо срочно найти обоснование для постулата, смысл которого можно метафорически отобразить, как: «Малыш, ведь я же лучше, я же лучше собаки…».
В общем случае, технологии ИИ на данный момент традиционно превосходят человека в тех областях, для которых характерны 2 отличительных свойства: 
1. наличие полной информации для решения задачи;
2. строгие критерии валидации результатов деятельности. 
Таковыми являются интеллектуальные и компьютерные игры, обработка больших массивов данных, на очереди – прикладная математика и программирование, и так далее [4]. Для некоторых иных сфер деятельности различия в результатах работы человека и ИИ сложно идентифицируются с точки зрения установления принадлежности, то есть ответа на вопрос о том, «кто это сделал – человек или ИИ?» [5]. Таковыми являются сфера искусств, художественная литература, гуманитарные науки и прочие примеры «генерации контента». Для данных отраслей сложно точно и чётко формализовать критерии высокой «успешности» того или иного результата деятельности – этим и обуславливается трудность идентификации.
В целом, системы ИИ превосходят человека в тех областях, которые хорошо «оцифрованы», то есть – в как можно большей степени переведены в количественно отображённые данные. Детальнее: если мы можем «качественно» и конкретно, на строго «количественном языке», «объяснить» ИИ, чего именно от него хотят – он способен этому быстро обучится и вскоре начинает превосходить человеческий уровень в контексте решения задач в «оцифрованной» сфере. Если же мы сами не особо понимаем, чего именно хотим – то и получаем, соответственно, просто «что-то по теме». В этом смысле уместно выдвинуть гипотезу о прямой зависимости уровня качества контента от качества и длины (в токенах) промта, то есть текстовой команды для ИИ – чем «лучше» второе, тем «лучше» и первое. 
Выходит, что мы не можем однозначно утверждать, что ИИ «лучше», чем человек. Однако во многих случаях мы можем сделать достаточно правомерный вывод о том, что, либо уже на данный момент ИИ является, либо в ближайшем будущем ИИ станет – в среднем «не хуже», чем некоторые представители тех профессиональных сфер, для которых конечным продуктом деятельности является формирование контента (текстов, образов, музыкальных композиций и так далее). Возможно, подобные умозаключения кажутся дискуссионными и противоречивыми, но с выходом каждой новой LLM (ChatGPT, Gemini, Claude, Grok, Deepseek и прочие) и дискуссионность, и противоречивость всё более и более нивелируются.
Таким образом, можно вывести, хоть это и несколько футуристичное суждение, что в тех областях, в которых генерирование контента являет собой конечный результат деятельности, то есть тот самый «продукт производства», ИИ на данный момент или в ближайшем будущем может или сможет считаться «не хуже» среднестатистического представителя отрасли. Конечно, это так не для поголовно всех представителей любого направления деятельности по генерации контента– лучшие специалисты могут оставаться вне конкуренции достаточно продолжительное время. Но наше суждение вполне правомерно применительно именно к среднестатистическому представителю отрасли и в отношении ближайшего будущего. Этот аспект являет собой основу и детерминанту для возникновения вопросов уже сугубо философского толка. По крайней мере, опасения со стороны социума сосредоточены в том числе и здесь, а не только в контексте «ИИ-апокалипсиса». И это далее провоцирует формирование философской подоплёки для ответной реакции общества.
Также является понятным, что масштабирование технологий ИИ ещё не достигло своего пика, хоть, возможно, уже на какое-либо более или менее близкое расстояние к нему подошло. Но в любом случае ещё есть куда расти:
1.	имеются возможности для даже просто количественного масштабирования в контексте обучения моделей, то есть по параметрам;
2.	для увеличения вычислительных мощностей, на данный момент представленных видеокартами;
3.	для масштабирования рассуждений в ходе формирования моделью ответа;
4.	и, конечно, для алгоритмической и архитектурной оптимизации.
К слову, большинство последних версий моделей уже оптимизированы по третьему пункту.
Возможны, разумеется, и иные варианты будущего развития технологий ИИ. Такие, как, к примеру, замена традиционных «фон Неймановских» вычислителей на квантовые и, как следствие, существенное увеличение скорости вычислений, вкупе с необходимостью пересмотра понятия асимптотической сложности алгоритмов в контексте квантовых вычислений. Данная перспектива интересна, но подобное уже скорее идентифицируется как фазовый переход качественного типа, знаменующий собой очередную смену парадигмы. Наше же исследование, хоть и подразумевает подобные возможности, но всё же остаётся в рамках того, что «есть здесь и сейчас», или линейным путём из этого следует.
И из этого следует как минимум то, что в тех сферах деятельности, в которых человек на данный момент превосходит ИИ по некоторым количественным показателям, уже «завтра» может сложиться строго противоположная ситуация. И те, кто видит «лес за деревьями», высказывают опасения, что апелляция к количественному превосходству человека над ИИ к тому, например, что на данный момент человек успешно решает 95% задач какого-либо сложного бенчмарка, а ИИ только 50% – уже в прошлом. Если условная «модель А» решала 50%, то уже условная «модель В» решает 67% и так далее. В общем, критика ИИ по количественному критерию начала стремительно терять актуальность и даже проигрывать в целесообразности – ИИ с выходом каждой новой модели берёт всё новые высоты и захватывает прежде недоступные рубежи [6]. Это прямо коррелирует с так называемым «эффектом ИИ», согласно которому «ИИ – это всё, что до сих пор не сделано», хоть здесь и имеется ввиду скорее ИИ «сильный по Сёрлу». Поэтому те, кто формирует некоторые прогнозы, теперь уже строят обоснование превосходства человека над ИИ в гораздо более высокоуровневой форме – по качественным показателям [7].
И вот об этом, собственно, наша актуальная работа. Дабы не утрировать диссипативность, мы абстрагируем ситуацию до бинарной категории: «фальсифицируемое-нефальсифицируемое». Напомним, что согласно критерию Карла Поппера о фальсифицируемости, всякое научное знание должно не только иметь возможность быть верифицировано путём доказательств, но также должно иметь в наличии и потенциальную возможность для опровержения. Походя заметим, что соотношение доказательства и опровержения для, например, некой теоремы «Х» примерно таково, что любое количество доказательств только лишь увеличивает вероятность истинности «Х», но всего лишь одно единственное опровержение – утверждает для «Х» 100% ложность. Возможно, не самое «приятное» для теоремы «Х» соотношение, но иначе наука перестанет «быть собой» – в истории математики достаточно примеров теорем, которые изначально были доказаны, но затем в их доказательстве были обнаружены ошибки.
Итак, под фальсифицируемыми мы будем понимать такие высказывания, истинность которых можно проверить конвенционально утверждённым способом – примерно на уровне общей целесообразности. То есть, к примеру, высказывание субъекта о том, что у него есть сердце, мы можем проверить при помощи ультразвукового исследования; высказывание о том, что субъект способен посчитать количество целочисленных разбиений некоего числа за 1 минуту, мы можем проверить путём проведения соответствующего эксперимента; и так далее. Под нефальсифицируемыми мы будем, соответственно, понимать такие высказывания, которые не могут быть строго проверены при помощи научной методологии.
Так, возвращаясь к качественному превосходству человека над ИИ, можно заметить, что все доказательства того, что человек «лучше», сводятся к оперированию метафизической атрибутикой: душой, сознанием, самосознанием, квалиа, смыслом, чувствительностью к контексту, Божественным началом и так далее. Мы определим подобные атрибуты как метафизические свойства и далее будем использовать их как взаимозаменяемые, то есть в этом смысле, душа и сознание будут полагаться нами синонимичными понятиями.
Если попробовать сформулировать хотя бы одно высказывание, включающее в себя метафизическое свойство в качестве предиката по отношению к некоему субъекту, то сразу станет видно, что подобное высказывание относится к категории нефальсифицируемого, то есть его нельзя ни доказать, ни опровергнуть. Следовательно, оно не будет являться научным, а представляется именно «предметом веры». К примеру, высказывание: «Человек обладает «душой/духом/самостью/чувствительностью к контексту/частичкой Бога/свободной волей», а ИИ – нет». Использование доказательств с подобной спецификой в сфере научной деятельности, в частности, в философском дискурсе, являет собой симптом вырождения науки в область религии. По сути, высказывания: «ИИ не может обладать сознанием» и «Вселенная была сотворена высшим разумом» – эквивалентны с точки зрения логической категоризации. Более того, в один ряд с этими высказываниями столь же лаконично ставится ещё одно: «Человек обладает сознанием». Осуществляя следующий логический шаг, получаем эквивалентность высказываний: «ИИ не обладает сознанием» и «Человек не обладает сознанием». Оба высказывания одинаково неопровержимы и недоказуемы.
Если несколько развить приведённую логическую цепочку, то получится, что высказывание: «ИИ не обладает сознанием и поэтому человек «качественнее», чем ИИ» имеет в роли своей предпосылки высказывание: «Человек обладает сознанием». То есть утверждение о том, что ИИ не может обладать сознанием, базируется на том, что человек как раз-таки может и обладает. Таким образом, несколько гипертрофированная импликация, по сути, выглядит так: «Человек обладает сознанием, потому ИИ не обладает сознанием». Однако неправомерность, по причине нефальсифицируемости, высказывания, использованного в первой части импликации, приводит, согласно закону исключённого третьего, к возможности отрицания части второй. То есть, мы по итогу получаем совершенно контр-интуитивную формулировку о том, что «Человек не обладает сознанием, потому ИИ обладает сознанием». И что характерно, обе формулировки эквивалентны с точки зрения концептуальной принадлежности – они обе не являются научными, а вторая из них так и вовсе абсурдна с интуитивной точки зрения.
В общем-то, показав неправомерность опоры на метафизические свойства и отсутствие обоснованности для претензий на научность у подобной аргументации, кажется, что можно было бы уже резюмировать. Но на самом деле – «до горизонта ещё далеко».
Ведь редукция проблемы, например, сознания к тому, что оно не столь же очевидно, как, допустим, рука или печень – не покрывает диапазон вопросов. Так, электромагнитное поле, гравитацию и квантовые флуктуации мы ведь тоже не способны воочию лицезреть. Но, тем не менее, упомянутые физические феномены как раз-таки полагаются строго научными, хоть и являют собой, конечно же, «совокупные метафоры», которые оформились согласно показателям расчётов и измерений. То есть дело не в том, что нечто мы можем увидеть, потрогать, услышать и так далее, а нечто нет. Как говорил Дени Дидро: «Если кто хочет, чтобы я поверил в Бога, пусть сделает так, чтобы я мог коснуться его рукой». Гравитацию ведь тоже невозможно ни потрогать, ни «положить в карман», ни «найти на дороге», но она так или иначе влияет на всё, что мы можем себе вообразить – по крайней мере, в контексте актуальной нам парадигмы научного знания. И здесь встаёт вполне логичный вопрос: почему мы считаем ту же гравитацию валидным научным понятием, а вот душу, самость или чувствительность к контексту – нет?
Пытаясь ответить на этот вопрос, мы достаточно быстро придём к тому, что разница между «душой и гравитацией» совсем в ином, а именно – в «формализованности» данных понятий. Формализация – это процесс и результат представления некоего феномена в формальном виде, то есть в виде формальной системы. Сами формальные системы традиционно определяются через семейства множеств: множества аксиом, множества правильно построенных высказываний, множества правил вывода и так далее – важна именно строгость определения. То есть, можно ответить на вопрос о разнице между душой и гравитацией указав, что гравитация формализуема, а душа – нет.
Далее, раз нам представляется целесообразным формирование «качественного» доказательства того, что человек «лучше» ИИ, так как человек обладает метафизическими свойствами, а ИИ ими не обладает, то нам придётся формализовать метафизические свойства. Только таким путём мы сможем сформировать строгую основу для осуществления дальнейшего доказательства. Само собой разумеется, что такие философские феномены, как душа и сознание, уже не одну тысячу лет «существуют» в неформализованном виде. И это так не просто потому, что «у философов руки не дошли», а потому что «формализация души» не представляет собой тривиальную задачу, которую можно решить однозначным и непротиворечивым образом.
Но всё же, в качестве мысленного эксперимента, представим, что мы способны описать метафизические феномены в формальном виде. То есть, по сути, допустим, что «трудная проблема сознания» уже решена. Далее, для формализации воспользуемся понятийным аппаратом теории множеств. Итак, сформируем множество «Х» – множество метафизических феноменов, в которое включены душа, сознание и прочие. Каждый элемент «у» множества «Х» также будет представлять собой множество, в которое включены атрибуты конкретного феномена, то есть некие элементы «z», такие, что элемент «z» принадлежит множеству «у». При таком подходе каждый элемент «z» должен будет представлять собой нечто, что можно чётко и строго определить, как принадлежащее какому-либо «y». Элементы множества «Х» могут иметь пересечения, но не должно быть такого элемента «y», который равен хотя бы одному элементу множества «Х», кроме самого себя. Это позволит нам чётко отличать метафизические феномены друг от друга и не спутать их. То есть, например, феномен «душа», условно, обладает атрибутами, которые могут быть представлены в виде множества – {1, 4, 8, 12, 87}, а феномен «дух» представлен иным множеством – {1, 2, 8, 12, 33, 44, 222}. Мы видим, что они пересекаются между собой, но они не строго равны, и, соответственно, мы сможем установить различие для формирования утверждения о том, что «Вот такое-то следствие обусловлено душой, а такое-то – духом».
В этом строго формализованном виде у нас имеется вполне осязаемая возможность доказать превосходство человека над ИИ, оставаясь при этом в рамках сугубо научного дискурса.
Однако посмотрим на один нюанс. Мы формализовали метафизические феномены и представили их в виде множества множеств, каждое из которых также обладает некими строго определёнными атрибутами. Мы можем формализовать заявляемые качественные отличия человека от ИИ иным способом – например, представив их алгоритмически или в виде графиков функций в n-мерном пространстве. Но суть от этого не изменится, так как формализация в любом случае предполагает формирование (для представления тех или иных феноменов) именно количественных отображений. Наш пример формализации помог нам сформировать основу для того, чтобы строго и формально доказать, что человек «лучше», чем ИИ, так как он, в отличие от ИИ, обладает метафизическими феноменами. Следовательно, теперь мы можем на языке формальной логики сформулировать доказательство. То есть, мы можем транспилировать метафизические феномены на язык логики и математики. А это значит, что оцифровать их – перевести в бинарный код – мы теперь тоже можем. И, как последующий логический шаг, – «объяснить» их ИИ мы, разумеется, уже также способны. В конечном итоге получается, что мы способны обучить ИИ реализации метафизических феноменов, если они представлены в формальном виде.
Это автоматически переводит «качественные» доказательства превосходства человека в «количественные». А как мы помним, количественные критерии различия между человеком и ИИ реализуют тенденцию к сокращению с каждой выпущенной интеллектуальной моделью нового поколения [8].
Вот теперь резюмируем. На основе вышесказанного мы утверждаем, что даже в том случае, если бы мы могли чётко определить такие понятия как душа, сознание и прочие из категории метафизических феноменов, то это бы означало, что систему ИИ мы также можем научить реализовывать эти самые феномены. А значит, строгое формальное доказательство превосходства человека над ИИ, только лишь будучи сформулированным, сразу же утрачивает свою актуальность. И происходит так потому, что формализация предполагает количественное отображение тех или иных феноменов, а тому, что мы можем выразить количественно, мы также можем обучить ИИ.
Модель гипотетической ситуации примерно следующая. Представим высказывание «Y»: «Человек лучше, чем ИИ, потому что «Х»». Далее происходит формализация «Х», то есть представление его в виде совокупности количественных параметров для того, чтобы научно доказать истинность высказывания «Y». На основе формализации «Х» происходит, соответственно, обучение ИИ реализации «Х». После чего «Х», как аргумент превосходства человека над ИИ, утрачивает силу, следовательно, высказывание «Y» становится ложным.
Таким образом, научное доказательство превосходства человека над ИИ не было бы целесообразным даже в том случае, если было бы оно вообще было возможным, так как любая формализация подразумевает переход в область количественного – область, в которой ИИ не перестаёт совершенствоваться, раз за разом превосходя человека. Иными словами, мы не способны строго доказать, что мы «лучше» ИИ, даже в том случае, если сможем «формализовать душу» и/или решить «трудную проблему сознания». А как мы уже показали выше, неформальные попытки доказательств принципиально не являются корректными по причине того, что они не могут считаться научными, и представляют собой трансфер дискурса в религиозную сферу.
Конечно же, интуитивно представляется, что человек обладает сознанием, а ИИ не обладает. И это корректно, но, опять же, только лишь с интуитивной точки зрения.
Итак, мы утверждаем, что ИИ на данный момент не обладает сознанием только и только потому, что мы сами не имеем чёткого представления о том, что же это такое.