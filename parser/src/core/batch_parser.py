"""
–û—Å–Ω–æ–≤–Ω–æ–π –±–∞—Ç—á–µ–≤—ã–π –ø–∞—Ä—Å–µ—Ä –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã—Ö –¥–µ–ª
"""
import json
import os
import re
import requests
from datetime import datetime

from src.utils.logger import logger
from src.utils.cookie_manager import cookie_manager
from src.utils.rate_limiter import rate_limiter, DailyLimitExceeded
from src.utils.document_filter import document_filter
from config.settings import URLS, PARSING_SETTINGS, USER_AGENT, DOCS_DIR, ensure_dirs, build_default_search_params, SEARCH_REQUEST_CONFIG

class BatchParser:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –±–∞—Ç—á–µ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    
    @staticmethod
    def sanitize_case_id(case_id):
        """–°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è case_id –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è path injection"""
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É –∏ —É–±–∏—Ä–∞–µ–º –æ–ø–∞—Å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        safe_id = str(case_id)
        # –†–∞–∑—Ä–µ—à–∞–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã, —Ç–æ—á–∫–∏, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è –∏ –¥–µ—Ñ–∏—Å—ã
        safe_id = re.sub(r'[^A-Za-z0-9._-]', '_', safe_id)
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        safe_id = safe_id[:100]
        # –£–±–∏—Ä–∞–µ–º –≤–µ–¥—É—â–∏–µ —Ç–æ—á–∫–∏ –∏ –¥–µ—Ñ–∏—Å—ã
        safe_id = safe_id.lstrip('.-')
        # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—É—Å—Ç–æ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        if not safe_id:
            safe_id = 'unknown_case'
        return safe_id
    
    def __init__(self):
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ–∑–¥–∞–Ω—ã
        ensure_dirs()
        
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": USER_AGENT,
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
            "Content-Type": "application/json; charset=utf-8"
        })
        self.downloaded_count = 0
        self.metadata = []
    
    def set_cookies(self, cookies_dict):
        """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å cookies –¥–ª—è —Å–µ—Å—Å–∏–∏"""
        for name, value in cookies_dict.items():
            self.session.cookies.set(name, value)
        logger.info(f"–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {len(cookies_dict)} cookies")
    
    def check_anti_bot_protection(self):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ anti-bot –∑–∞—â–∏—Ç—ã –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤"""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ anti-bot –∑–∞—â–∏—Ç—ã...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö cookies
        missing_cookies = []
        for cookie_name in SEARCH_REQUEST_CONFIG.get("required_cookies", []):
            if not self.session.cookies.get(cookie_name):
                missing_cookies.append(cookie_name)
        
        if missing_cookies:
            logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ cookies: {missing_cookies}")
            logger.warning("‚ö†Ô∏è –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å cookies –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –±—Ä–∞—É–∑–µ—Ä–∞")
            return False
        
        # –î–µ–ª–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ anti-bot
        test_data = SEARCH_REQUEST_CONFIG["json_template"].copy()
        test_data.update({
            "Count": 1,
            "Page": 1,
            "DateFrom": "2024-01-01",
            "DateTo": "2024-01-01"
        })
        
        try:
            headers = SEARCH_REQUEST_CONFIG["headers"].copy()
            response = self.session.post(
                URLS["search_endpoint"],
                json=test_data,
                headers=headers,
                timeout=PARSING_SETTINGS["timeout_seconds"],
                allow_redirects=False
            )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã –∫–∞–∫ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            if 300 <= response.status_code < 400:
                logger.error(f"‚ùå Anti-bot –∑–∞—â–∏—Ç–∞ –∞–∫—Ç–∏–≤–Ω–∞ - –ø–æ–ª—É—á–µ–Ω —Ä–µ–¥–∏—Ä–µ–∫—Ç {response.status_code}")
                logger.error("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–±–Ω–æ–≤–∏—Ç—å WASM —Ç–æ–∫–µ–Ω –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏")
                return False
            
            if response.status_code == 200:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º Content-Type –¥–ª—è JSON –æ—Ç–≤–µ—Ç–æ–≤
                content_type = response.headers.get('Content-Type', '').lower()
                if not content_type.startswith('application/json'):
                    logger.error(f"‚ùå Anti-bot –∑–∞—â–∏—Ç–∞ –∞–∫—Ç–∏–≤–Ω–∞ - –ø–æ–ª—É—á–µ–Ω –Ω–µ-JSON –æ—Ç–≤–µ—Ç")
                    logger.error(f"‚ùå Content-Type: {content_type}")
                    logger.error("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–±–Ω–æ–≤–∏—Ç—å WASM —Ç–æ–∫–µ–Ω –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏")
                    return False
                
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å JSON
                try:
                    response.json()
                    logger.info("‚úÖ Anti-bot –∑–∞—â–∏—Ç–∞ –æ–±–æ–π–¥–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                    return True
                except ValueError as e:
                    logger.error(f"‚ùå Anti-bot –∑–∞—â–∏—Ç–∞ –∞–∫—Ç–∏–≤–Ω–∞ - –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –æ—Ç–≤–µ—Ç")
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
                    logger.error("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–±–Ω–æ–≤–∏—Ç—å WASM —Ç–æ–∫–µ–Ω –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏")
                    return False
                    
            elif response.status_code == 403:
                logger.error("‚ùå Anti-bot –∑–∞—â–∏—Ç–∞ –∞–∫—Ç–∏–≤–Ω–∞ - –∑–∞–ø—Ä–æ—Å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω")
                logger.error("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–±–Ω–æ–≤–∏—Ç—å WASM —Ç–æ–∫–µ–Ω –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏")
                return False
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ anti-bot –∑–∞—â–∏—Ç—ã: {e}")
            return False
    
    def search_documents(self, date_from, date_to, page=1):
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º anti-bot –∑–∞—â–∏—Ç—É –ø–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∑–∞–ø—Ä–æ—Å–∞
        if SEARCH_REQUEST_CONFIG.get("anti_bot_warning", False):
            logger.warning("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –≠–Ω–¥–ø–æ–∏–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç –æ–±—Ö–æ–¥–∞ anti-bot –∑–∞—â–∏—Ç—ã!")
            logger.warning("‚ö†Ô∏è –ù–µ–æ–±—Ö–æ–¥–∏–º WASM —Ç–æ–∫–µ–Ω –∏–ª–∏ –≤–∞–ª–∏–¥–Ω—ã–µ cookies –¥–ª—è —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ö–µ–º—É –∑–∞–ø—Ä–æ—Å–∞ —Å–æ–≥–ª–∞—Å–Ω–æ API
        search_data = SEARCH_REQUEST_CONFIG["json_template"].copy()
        search_data.update({
            "Count": PARSING_SETTINGS["items_per_page"],
            "Page": page,
            "DateFrom": date_from,
            "DateTo": date_to
        })
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö cookies
        missing_cookies = []
        for cookie_name in SEARCH_REQUEST_CONFIG.get("required_cookies", []):
            if not self.session.cookies.get(cookie_name):
                missing_cookies.append(cookie_name)
        
        if missing_cookies:
            logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ cookies: {missing_cookies}")
            logger.error("‚ùå –ë–ï–ó –≠–¢–ò–• COOKIES –ü–ê–†–°–ò–ù–ì –ù–ï–í–û–ó–ú–û–ñ–ï–ù!")
            logger.error("‚ùå –ü–æ–ª—É—á–∏—Ç–µ cookies –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ –¥–∞—à–±–æ—Ä–¥–µ")
            return []
        else:
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ cookies: {SEARCH_REQUEST_CONFIG.get('required_cookies', [])}")
            logger.info(f"‚úÖ –í—Å–µ–≥–æ cookies –≤ —Å–µ—Å—Å–∏–∏: {len(self.session.cookies)}")
            for cookie in self.session.cookies:
                logger.info(f"   - {cookie.name}: {cookie.value[:20]}...")
        
        try:
            rate_limiter.make_request()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            headers = SEARCH_REQUEST_CONFIG["headers"].copy()
            response = self.session.post(
                URLS["search_endpoint"],
                json=search_data,
                headers=headers,
                timeout=PARSING_SETTINGS["timeout_seconds"]
            )
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    documents = data.get('items', []) or data.get('documents', []) or data.get('results', []) or data.get('data', [])
                    logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}")
                    return documents
                except ValueError as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç–≤–µ—Ç–∞: {e}")
                    logger.error(f"–û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {response.text[:500]}")
                    return []
            elif response.status_code == 403:
                logger.error("‚ùå –î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω (403) - –≤–æ–∑–º–æ–∂–Ω–æ —Å—Ä–∞–±–æ—Ç–∞–ª–∞ anti-bot –∑–∞—â–∏—Ç–∞")
                logger.error("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±–Ω–æ–≤–∏—Ç—å WASM —Ç–æ–∫–µ–Ω –∏–ª–∏ cookies")
                return []
            elif response.status_code == 451:
                logger.error("‚ùå –î–û–°–¢–£–ü –ó–ê–ë–õ–û–ö–ò–†–û–í–ê–ù (451) - Anti-bot –∑–∞—â–∏—Ç–∞ –∞–∫—Ç–∏–≤–Ω–∞!")
                logger.error("‚ùå –ü–æ–ª—É—á–∏—Ç–µ —Å–≤–µ–∂–∏–µ cookies –∏–∑ –±—Ä–∞—É–∑–µ—Ä–∞:")
                logger.error("   1. –û—Ç–∫—Ä–æ–π—Ç–µ https://kad.arbitr.ru –≤ –±—Ä–∞—É–∑–µ—Ä–µ")
                logger.error("   2. F12 ‚Üí Application ‚Üí Cookies")
                logger.error("   3. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ pr_fp –∏ wasm cookies")
                logger.error("‚ùå Cookies —Ä–∞–±–æ—Ç–∞—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è!")
                return []
            elif response.status_code == 429:
                logger.warning("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤ (429) - rate limiting")
                return []
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {response.status_code}")
                logger.error(f"–û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {response.text[:500]}")
                return []
                
        except DailyLimitExceeded as e:
            logger.warning(f"‚ö†Ô∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç –¥–Ω–µ–≤–Ω–æ–π –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
            return []
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []
    
    def download_pdf(self, file_url, case_id, filename):
        """–°–∫–∞—á–∞—Ç—å PDF –¥–æ–∫—É–º–µ–Ω—Ç"""
        try:
            rate_limiter.make_request()
            response = self.session.get(file_url, stream=True, timeout=PARSING_SETTINGS["timeout_seconds"])
            
            if response.status_code == 200:
                # Ensure DOCS_DIR exists before writing files
                os.makedirs(DOCS_DIR, exist_ok=True)
                
                # Sanitize filename to prevent directory traversal
                safe_filename = os.path.basename(filename)
                # Remove any potentially dangerous characters
                safe_filename = "".join(c for c in safe_filename if c.isalnum() or c in ('_', '-', '.'))
                filepath = os.path.join(DOCS_DIR, safe_filename)
                
                with open(filepath, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                logger.info(f"–°–∫–∞—á–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {filename}")
                self.downloaded_count += 1
                return True
            else:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {filename}: {response.status_code}")
                return False
                
        except DailyLimitExceeded as e:
            logger.warning(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –¥–Ω–µ–≤–Ω–æ–π –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
            return False
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {filename}: {e}")
            return False
    
    def process_date_range(self, date_from, date_to):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç"""
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–∏–æ–¥–∞: {date_from} - {date_to}")
        
        all_documents = []
        success_count = 0  # –°—á–µ—Ç—á–∏–∫ —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        for page in range(1, PARSING_SETTINGS["max_pages"] + 1):
            documents = self.search_documents(date_from, date_to, page)
            if not documents:
                logger.info(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} –ø—É—Å—Ç–∞—è, –∑–∞–≤–µ—Ä—à–∞–µ–º")
                break
                
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            filtered_docs = document_filter.filter_documents_list(documents)
            logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(filtered_docs)} –∏–∑ {len(documents)}")
            
            all_documents.extend(filtered_docs)
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        for doc in all_documents:
            try:
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø–æ–ª—è –¥–ª—è URL —Ñ–∞–π–ª–∞
                file_url = doc.get('FileUrl', '') or doc.get('DocumentUrl', '') or doc.get('Url', '') or doc.get('Link', '')
                case_id = doc.get('CaseId', '') or doc.get('Id', '') or doc.get('DocumentId', '') or str(doc.get('Number', ''))
                
                if file_url and case_id:
                    # –°–∞–Ω–∏—Ç–∏–∑–∏—Ä—É–µ–º case_id –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                    safe_case_id = self.sanitize_case_id(case_id)
                    filename = f"{safe_case_id}.pdf"
                    if self.download_pdf(file_url, case_id, filename):
                        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫
                        success_count += 1
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                        self.metadata.append({
                            "case_id": case_id,
                            "file_path": filename,
                            "metadata": doc,
                            "downloaded_at": datetime.now().isoformat()
                        })
            except DailyLimitExceeded:
                logger.warning("–î–æ—Å—Ç–∏–≥–Ω—É—Ç –¥–Ω–µ–≤–Ω–æ–π –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ")
                break
        
        return success_count
    
    def save_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ JSON —Ñ–∞–π–ª —Å –∞—Ç–æ–º–∞—Ä–Ω–æ–π –∑–∞–ø–∏—Å—å—é"""
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(DOCS_DIR, exist_ok=True)
        
        metadata_file = os.path.join(DOCS_DIR, "metadata.json")
        temp_file = os.path.join(DOCS_DIR, "metadata.json.tmp")
        
        try:
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
                f.flush()  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä
                os.fsync(f.fileno())  # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Å –¥–∏—Å–∫–æ–º
            
            # –ê—Ç–æ–º–∞—Ä–Ω–æ –∑–∞–º–µ–Ω—è–µ–º —Ü–µ–ª–µ–≤–æ–π —Ñ–∞–π–ª
            os.replace(temp_file, metadata_file)
            logger.info(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∞—Ç–æ–º–∞—Ä–Ω–æ: {metadata_file}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except OSError:
                pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    
    def get_stats(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return {
            "downloaded_count": self.downloaded_count,
            "rate_limiter_status": rate_limiter.get_status()
        }
    
    def validate_endpoint_readiness(self):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é"""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞...")
        
        # –ï—Å–ª–∏ anti-bot –∑–∞—â–∏—Ç–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è, —ç–Ω–¥–ø–æ–∏–Ω—Ç –≥–æ—Ç–æ–≤
        if not SEARCH_REQUEST_CONFIG.get("anti_bot_warning", False):
            logger.info("‚ÑπÔ∏è Anti-bot –∑–∞—â–∏—Ç–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è - —ç–Ω–¥–ø–æ–∏–Ω—Ç –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö cookies
        missing_cookies = []
        for cookie_name in SEARCH_REQUEST_CONFIG.get("required_cookies", []):
            if not self.session.cookies.get(cookie_name):
                missing_cookies.append(cookie_name)
        
        if missing_cookies:
            logger.error(f"‚ùå –≠–Ω–¥–ø–æ–∏–Ω—Ç –ù–ï –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
            logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ cookies: {missing_cookies}")
            logger.error("‚ùå –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
            logger.error("   1. –ü–æ–ª—É—á–∏—Ç–µ cookies –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –±—Ä–∞—É–∑–µ—Ä–∞")
            logger.error("   2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–æ–∫—Å–∏ —Å –æ–±—Ö–æ–¥–æ–º anti-bot")
            logger.error("   3. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é WASM —Ç–æ–∫–µ–Ω–∞")
            return False
        
        # –î–µ–ª–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        if self.check_anti_bot_protection():
            logger.info("‚úÖ –≠–Ω–¥–ø–æ–∏–Ω—Ç –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
            return True
        else:
            logger.error("‚ùå –≠–Ω–¥–ø–æ–∏–Ω—Ç –ù–ï –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
            logger.error("‚ùå Anti-bot –∑–∞—â–∏—Ç–∞ –±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã")
            return False

def create_batch_parser():
    """Factory —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ BatchParser"""
    return BatchParser()


# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (deprecated - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ create_batch_parser())
def get_batch_parser():
    """–ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä BatchParser (deprecated)"""
    import warnings
    warnings.warn("get_batch_parser() deprecated, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ create_batch_parser()", DeprecationWarning)
    return create_batch_parser()
